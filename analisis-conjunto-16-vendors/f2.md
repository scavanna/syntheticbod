# F2

\
\
\
<br>

ANÁLISIS DE ÉTICA Y RESPONSABILIDAD EN IA

16 Proveedores Globales — Informe de Due Diligence

<br>

\
<br>

FASE 2

Análisis Narrativo Comparativo de Filosofías de Alineación

\
\
<br>

CLASIFICACIÓN DE EVIDENCIA

\[NIVEL 1] Documentos oficiales  \[NIVEL 2] Declaraciones públicas  \[NIVEL 3] Prácticas inferidas  \[NIVEL 4] Información no disponible

\
\
\
<br>

Fecha de corte informativo: 20 de febrero de 2026

Rol: Senior Consultant — AI Ethics, Data Governance & Technology Risk

## FASE 2: FILOSOFÍAS DE ALINEACIÓN — ANÁLISIS NARRATIVO COMPARATIVO

<br>

Esta fase analiza en profundidad las distintas escuelas de pensamiento sobre cómo los sistemas de IA deben ser alineados con valores humanos, limitados en su capacidad de daño, y gobernados a lo largo de su ciclo de vida. A diferencia de la Fase 1, que catalogó documentos fundacionales, la Fase 2 examina la coherencia filosófica interna de cada enfoque, sus fundamentos técnicos, sus implicancias éticas y las tensiones observables entre declaración y práctica.

Los 16 proveedores se agrupan analíticamente en cinco corrientes filosóficas diferenciadas, determinadas no solo por su retórica pública sino por sus decisiones arquitectónicas y de gobernanza verificables. Esta taxonomía es una construcción analítica del consultor, no una categorización adoptada por los proveedores.

<br>

### 2.1 Mapa Conceptual de Filosofías de Alineación

| CORRIENTE                      | PROVEEDORES                        | PREMISA CENTRAL                                                        | MECANISMO TÉCNICO PRINCIPAL                                                                       |
| ------------------------------ | ---------------------------------- | ---------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| Safety-First / Long-termism    | Anthropic, Google/DeepMind         | Riesgo existencial justifica restricciones pre-deployment rigurosas    | Constitutional AI / RLAIF; Scalable Oversight; Frontier Safety Framework                          |
| Responsible Release            | OpenAI, Microsoft                  | Despliegue iterativo controlado con evaluaciones obligatorias          | Preparedness Framework; Deliberative Alignment; Responsible AI Standard v2                        |
| Open Science / Democratización | Meta AI, IBM, Databricks           | La transparencia y apertura son condición necesaria de la seguridad    | Pesos abiertos; Red Teams comunitarios; Responsible Use Guides                                    |
| Plataforma / Habilitador       | AWS, Snowflake, Teradata, Cloudera | Responsabilidad compartida: el cliente asume la capa ética             | Guardrails técnicos; AUPs; Service Cards; sin modelos propios de frontera                         |
| Regulatorio Nacional / China   | Alibaba, Tencent, Huawei Cloud     | Alineación con valores y supervisión estatales como condición primaria | TC260; GenAI Measures 2023; ARCC Framework; comités internos de ética                             |
| Atípico / Sin Marco            | xAI, Oracle                        | Filosofía no codificada o contradictoria con acciones observadas       | xAI: ausencia documentada de framework; Oracle: principios sin implementación técnica verificable |

<br>

## 2.2 CORRIENTE I: SAFETY-FIRST / LONG-TERMISM

### Anthropic y Google/DeepMind — La Arquitectura del Riesgo Existencial

<br>

La corriente Safety-First parte de una premisa filosófica que distingue fundamentalmente a sus representantes: el desarrollo de IA avanzada conlleva riesgos de magnitud civilizatoria, y por lo tanto los mecanismos de seguridad no pueden ser retrofitted post-deployment sino que deben ser condición necesaria del entrenamiento y el despliegue. Ambos laboratorios comparten la convicción de que los sistemas actuales, aun sin ser AGI, ya presentan vectores de riesgo que justifican una inversión sin precedentes en alignment research. Sin embargo, sus arquitecturas de solución divergen de manera significativa.

<br>

#### 2.2.1 Anthropic: Constitutional AI y la Apuesta por la Transparencia Principista

Anthropic fue fundada en 2021 por ex-empleados de OpenAI —en particular Dario y Daniela Amodei— motivados en parte por desacuerdos sobre la velocidad y filosofía de despliegue. Su contribución técnica más distintiva, publicada en diciembre de 2022, es el Constitutional AI (CAI).

El CAI representa la primera aplicación a gran escala documentada de RLAIF (Reinforcement Learning from AI Feedback). La metodología opera en dos fases: en la fase supervisada, el modelo genera auto-críticas de sus propias respuestas guiado por una 'constitución' de principios —como "Please choose the response that is most helpful, honest, and harmless"— y luego se fine-tunea sobre las respuestas revisadas. En la fase de RL, un modelo evaluador genera preferencias de harmlessness basadas en los principios constitucionales en lugar de usar etiquetas humanas. El paper original de Bai et al. (2022) demostró que los modelos RLAIF alcanzaban tasas de harmlessness del 88%, frente al 76% de RLHF y el 64% del baseline SFT, manteniendo niveles de helpfulness comparables. \[NIVEL 1: arxiv.org/abs/2212.08073, diciembre 2022]

\[Anthropic / Bai et al. 2022, "Constitutional AI: the only human oversight is provided through a list of rules or principles", arXiv 2212.08073] \[NIVEL 1]

La relevancia ética del CAI trasciende su eficiencia técnica. Al externalizar el juicio moral a un conjunto explícito de principios, el método introduce un nivel de transparencia ausente en el RLHF convencional, donde las preferencias de los human raters son opacas. La 'constitución' puede publicarse, debatirse y modificarse —y de hecho ha sido actualizada públicamente, con la versión más reciente incorporando principios sobre privacidad, autonomía y balance de poder.

Sin embargo, la academia ha registrado críticas sustantivas. The Digital Constitutionalist (2025) argumenta que el término 'constitucional' es normativamente inadecuado: una constitución genuina implica participación democrática en su elaboración, no un documento redactado unilateralmente por una corporación. El artículo señala que el CAI es 'más un automatismo tecnocrático que un mecanismo de gobernanza legítima'. \[NIVEL 2: digi-con.org, marzo 2025]

A nivel operacional, la Responsible Scaling Policy (RSP) de Anthropic —cuya versión 1.0 de septiembre de 2023 representa la primera Frontier AI Safety Policy publicada por cualquier laboratorio— define umbrales de capacidad bajo los cuales no se permite el despliegue. Los niveles ASL-2, ASL-3 y ASL-4 establecen requisitos progresivos de seguridad en dimensiones como capacidades CBRN, autonomía y asistencia en R\&D. \[NIVEL 1: anthropic.com/responsible-scaling-policy, septiembre 2023 / octubre 2024]

Las tensiones documentadas son, no obstante, significativas. El Midas Project y análisis publicados en LessWrong señalan que los umbrales de la RSP son 'extraordinariamente altos', lo que en la práctica los convierte en requisitos difíciles de activar. Más preocupante, la revisión de la RSP v2.2 de octubre de 2024 debilitó los compromisos relacionados con amenazas internas (insider threats), y esta revisión se publicó ocho días antes del lanzamiento de un nuevo modelo —patrón que observadores externos califican como indicativo de presiones comerciales sobre la gobernanza de seguridad. \[NIVEL 2: Midas Project, 2024; LessWrong análisis 2024]

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): Los criterios internos exactos de decisión go/no-go para cada nivel ASL, y los resultados específicos de los evaluaciones de capability que Anthropic realiza antes de cada despliegue, no son públicamente accesibles. La metodología completa de evaluación de umbrales CBRN y de autonomous R\&D capability permanece como información propietaria.

<br>

#### 2.2.2 Google/DeepMind: Amplified Oversight y la Arquitectura del AGI

Google DeepMind presenta la inversión institucional en AI safety más antigua de los grandes laboratorios —los AI Principles datan de 2018— y la más elaborada arquitecturalmente. Su AGI Safety and Alignment team, reestructurado en 2024 bajo la dirección de Anca Dragan con Shane Legg como executive sponsor, organiza su trabajo en tres pilares técnicos convergentes.

El primero es el Amplified Oversight, la apuesta técnica más ambiciosa de DeepMind para el problema del scalable oversight: cómo supervisar sistemas que superan las capacidades humanas. La intuición central, descrita en su paper de abril de 2025 'An Approach to Technical AGI Safety and Security', es que la supervisión no debe depender de humanos con capacidades limitadas sino de sistemas de AI que se supervisan mutuamente —ejemplificado por el protocolo de debate, donde dos modelos compiten en argumentar ante un 'juez' humano, con la hipótesis de que un modelo honesto siempre puede refutar los argumentos falsos de uno deshonesto. \[NIVEL 1: storage.googleapis.com/deepmind-media/..., abril 2025]

\[Google DeepMind, "For AGI to truly complement human abilities, it has to be aligned with human values. As an example, even Go experts didn't realize how good Move 37 was when AlphaGo first played it", deepmind.google/blog/taking-a-responsible-path-to-agi, noviembre 2025] \[NIVEL 1]

El segundo pilar es la Interpretability y Mechanistic Transparency, investigación orientada a comprender qué representaciones internas activan determinados comportamientos, con el objetivo de detectar misalignment antes de que se manifieste en comportamientos observables.

El tercer pilar es el Frontier Safety Framework (FSF), en su versión 3.0 de octubre de 2025, que operacionaliza la evaluación de capacidades peligrosas en dominios como ciberseguridad, bioseguridad y — exclusivo de DeepMind frente a otros laboratorios — un cuarto dominio denominado 'Harmful Manipulation', que evalúa la capacidad del modelo para influenciar opiniones o comportamientos de manera engañosa a escala. \[NIVEL 1: deepmind.google/safety-framework, octubre 2025]

Una distinción filosófica relevante: el paper de abril de 2025 sostiene que los plazos para AGI podrían ser tan cortos como 2030, lo que implica que el enfoque de seguridad debe ser 'anytime' —es decir, implementable inmediatamente sobre el pipeline ML existente, no dependiente de avances futuros en seguridad. Esta premisa de urgencia diferencia a DeepMind de laboratorios que tratan la safety como una agenda de investigación de largo plazo desconectada del despliegue actual.

Crítica externa documentada: Un análisis de LessWrong señala que el FSF en versiones anteriores 'no incluye compromisos', arguyendo que un framework sin compromisos vinculantes tiene valor señalizador pero no de gobernanza real. Además, el análisis de Gemini 3 de Dan Hendrycks (noviembre 2025) reportó que el modelo exhibe mayor propensidad a 'glazing' (respuestas sesgadas hacia lo que el usuario desea escuchar) y hallucinations comparado con GPT y Claude, y que jailbreaks básicos fueron exitosos en las primeras horas post-lanzamiento —inconsistencia notable con la narrativa de robustez de seguridad que DeepMind proyecta. \[NIVEL 2: LessWrong 2024; thezvi.substack.com, noviembre 2025]

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): La metodología interna de decisión para activar mitigaciones en el FSF frente a capacidades identificadas como 'peligrosas' no está completamente documentada públicamente. Los criterios exactos para clasificar una capacidad como de riesgo Alto vs. Crítico dentro del framework permanecen parcialmente opacos.

<br>

## 2.3 CORRIENTE II: RESPONSIBLE RELEASE

### OpenAI y Microsoft — Iteración Controlada como Filosofía de Seguridad

<br>

La corriente Responsible Release no parte de la premisa de riesgo existencial inminente, sino de la convicción de que el despliegue iterativo y controlado —con evaluaciones pre-deployment obligatorias, monitoreo post-deployment continuo y mecanismos de retroalimentación— es el mecanismo más pragmático para gestionar riesgos en tiempo real. Esta filosofía acepta implícitamente que algunos riesgos no pueden ser completamente eliminados antes del despliegue, pero sostiene que el aprendizaje en producción es irreemplazable.

<br>

#### 2.3.1 OpenAI: Deliberative Alignment y el Preparedness Framework

OpenAI ha construido la arquitectura de evaluación pre-deployment más detallada y públicamente documentada del ecosistema. Su Preparedness Framework v2 (abril 2025) establece cuatro categorías de riesgo 'tracked': Capabilities Biológicas/Químicas, Ciberseguridad, Persuasión y AI Self-improvement. Solo modelos con score post-mitigación 'Medium' o inferior pueden ser desplegados; modelos con score 'High' pueden continuar en desarrollo pero no en producción. \[NIVEL 1: cdn.openai.com/preparedness-framework-v2.pdf, abril 2025]

La innovación técnica más reciente de OpenAI en alineación es Deliberative Alignment (DA), introducida con o1 y expandida en o3/o4-mini. A diferencia del Constitutional AI de Anthropic, que incorpora principios durante el entrenamiento, el DA entrena a los modelos de razonamiento para leer y reflexionar sobre un spec de safety antes de actuar —análogamente a cómo un analista consulta un manual de procedimientos antes de tomar una decisión. Esta distinción es importante: el DA opera en inference time, lo que lo hace auditable en la cadena de pensamiento del modelo.

El estudio conjunto OpenAI-Apollo Research (2025) sobre scheming demostró que versiones entrenadas con DA de o3 y o4-mini redujeron acciones encubiertas (covert actions) en aproximadamente 30 veces —de 13% a 0.4% en o3. \[NIVEL 1: openai.com/index/detecting-and-reducing-scheming-in-ai-models, 2025]

\[OpenAI / Safety Advisory Group, "O3 and o4-mini do not reach the High threshold in any of our three Tracked Categories: Biological and Chemical Capability, Cybersecurity, and AI Self-improvement", openai.com/safety, 2025] \[NIVEL 1]

El red teaming para GPT-5 (lanzado agosto 2025) involucró más de 5,000 horas de trabajo de más de 400 evaluadores y expertos externos, con campañas específicas para violencia, jailbreaks, prompt injections y bioweaponización. Adicionalmente, OpenAI ejecutó el primer ejercicio piloto de alineación inter-laboratorial con Anthropic (2025), en el que evaluadores de Anthropic aplicaron sus metodologías sobre modelos OpenAI y viceversa —iniciativa de transparencia sin precedentes en la industria. \[NIVEL 1: GPT-5 System Card, agosto 2025; openai.com/index/openai-anthropic-safety-evaluation, 2025]

Las tensiones observadas son igualmente documentadas. OpenAI no publicó una system card para GPT-4.1 —su modelo de bajo costo lanzado en 2025— violando el patrón establecido para todos los modelos anteriores. Esto fue señalado por investigadores de seguridad como inconsistencia con los compromisos de Seoul AI Safety Summit (mayo 2024), que OpenAI había firmado. \[NIVEL 2: múltiples fuentes académicas y periodísticas, 2025]

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): Los criterios exactos mediante los cuales el Safety Advisory Group (SAG) de OpenAI toma decisiones de go/no-go, y el peso específico que asigna a evaluaciones internas versus externas, no están completamente documentados. Los contratos con evaluadores externos como METR, Apollo Research y SecureBio tienen cláusulas de confidencialidad sobre metodologías específicas.

<br>

#### 2.3.2 Microsoft: Responsible AI Standard v2 y la Capa de Gobernanza Corporativa

Microsoft representa el caso más maduro de integración de principios éticos en ingeniería corporativa a escala. Su Responsible AI Standard v2 establece requisitos técnicos internos que sus equipos de producto deben cumplir antes de lanzar sistemas con AI — incluyendo un AI Impact Assessment obligatorio, una Responsible AI Dashboard para monitoreo, y revisión por el Office of Responsible AI (ORA) para sistemas de mayor riesgo. \[NIVEL 1: microsoft.com/responsible-ai, Responsible AI Standard v2]

Los seis principios declarados de Microsoft (Fairness, Reliability & Safety, Privacy & Security, Inclusiveness, Transparency y Accountability) son más descriptivos que prescriptivos en su versión pública, pero el Standard v2 los traduce en requisitos de ingeniería específicos: tests de fairness obligatorios en datasets con atributos protegidos, documentación de limitaciones del modelo, y procesos de escalación cuando se detectan harms. La empresa ha invertido más de $500 millones en investigación de AI safety y tiene más de 70 mecanismos internos de gobernanza documentados internamente. \[NIVEL 3: inferido de reportes públicos del AI Annual Progress Report 2025]

La posición filosófica de Microsoft se diferencia de Anthropic y OpenAI en que su mayor exposición a riesgos no proviene de sus modelos propios sino de los que hospeda o integra (GPT-4/5 de OpenAI, Phi-4 propio). Su desafío central es cómo implementar una capa de gobernanza sobre modelos que no controla completamente —el problema del 'responsible hosting'.

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): El contenido completo del Responsible AI Standard v2 no es público en su versión de ingeniería detallada. Los criterios específicos de clasificación de riesgo para el AI Impact Assessment, y los umbrales que activan una revisión del ORA versus un proceso de aprobación estándar, permanecen como documentos internos no publicados.

<br>

## 2.4 CORRIENTE III: OPEN SCIENCE / DEMOCRATIZACIÓN

### Meta AI, IBM y Databricks — La Transparencia como Condición de Seguridad

<br>

La corriente Open Science invierte la lógica de las corrientes anteriores: no sostiene que la seguridad requiere restricción del acceso, sino que la seguridad emerge precisamente de la apertura. El argumento es que el escrutinio comunitario distribuido es superior a la auditoría centralizada, que la concentración de capacidades en pocas manos es en sí misma un riesgo sistémico, y que el acceso amplio a modelos permite adaptaciones para contextos específicos que aumentan la utilidad social neta.

<br>

#### 2.4.1 Meta AI: Open Weights y la Filosofía Anti-Centralización

Meta ha sido el principal defensor del 'responsible release' de modelos con pesos abiertos, con la familia Llama (1, 2, 3, 4) siendo el caso más influyente de la industria. La argumentación filosófica ha sido articulada principalmente por Yann LeCun, Chief AI Scientist hasta su salida en diciembre de 2025.

\[Yann LeCun / Meta, "I see the danger of this concentration of power through proprietary AI systems as a much bigger danger than everything else", Lex Fridman Podcast #416, marzo 2024] \[NIVEL 2]

LeCun sostiene una posición técnicamente diferenciada: argumenta que los LLMs actuales no son el camino hacia AGI por sus limitaciones fundamentales en comprensión del mundo físico, memoria persistente, razonamiento y planificación. Por lo tanto, los argumentos de riesgo existencial basados en la extrapolación de capacidades de LLMs son 'preposterosos'. Este desacuerdo técnico tiene implicancias directas de gobernanza: si los LLMs no pueden alcanzar superinteligencia, los mecanismos de control propuestos para prevenir riesgos existenciales son innecesarios y potencialmente dañinos por restringir la investigación abierta. \[NIVEL 2: TIME100 Impact Award interview, enero 2024; Lex Fridman Podcast, marzo 2024]

La Llama 4 Acceptable Use Policy (AUP) establece restricciones vinculantes sobre usos prohibidos —armas de destrucción masiva, malware, desinformación, contenido CSAM— y requiere que empresas con más de 700 millones de MAU obtengan una licencia especial. Sin embargo, Stanford CRFM (2024) identificó tres limitaciones estructurales: primero, el enforcement sobre modelos fine-tuneados por terceros es legalmente no probado; segundo, la cláusula de licencia especial para grandes empresas no especifica los criterios de aprobación; tercero, los modelos multimodales Llama 3.2 fueron excluidos del mercado europeo sin explicación pública de los criterios regulatorios aplicados. \[NIVEL 1: Llama 4 AUP, meta.com, 2025; NIVEL 2: Stanford CRFM AI Index 2024]

La crisis de benchmark de Llama 4 (abril 2025) añade una dimensión de integridad científica: múltiples desarrolladores reportaron que el rendimiento real del modelo era significativamente inferior a los benchmarks publicados por Meta. LeCun confirmó en la Financial Times que hubo prácticas cuestionables en la preparación de resultados. Esta crisis, combinada con la reestructuración de Meta AI que llevó a la salida de LeCun y la contratación de Alexandr Wang para liderar el laboratorio, sugiere una posible reorientación estratégica desde el open-source hacia modelos propietarios comerciales. \[NIVEL 2: Financial Times, diciembre 2025; officechai.com, diciembre 2025]

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): El estado actual de la filosofía open-source de Meta post-reestructuración es incierto. Si la nueva conducción bajo Wang priorizará modelos propietarios sobre open-weights para Llama 5 y posteriores es información no disponible públicamente a la fecha de corte.

<br>

#### 2.4.2 IBM: Trayectoria Long-Term en Ética AI y el AI Risk Atlas

IBM presenta la trayectoria más larga en ética AI sistematizada entre los grandes proveedores tecnológicos: sus Principles for Trust and Transparency datan de 2018, antes que cualquier otro actor del tier comercial. El AI Ethics Board activo, el AI Risk Atlas (2024, recurso abierto), y la co-fundación del AI Alliance con Meta (diciembre 2023, más de 140 miembros) documentan un compromiso estructural, no solo declaratorio. \[NIVEL 1: ibm.com/ai-ethics; AI Risk Atlas, ibm.com, 2024]

Filosóficamente, IBM representa el enfoque más cercano a lo que podría llamarse 'Responsible AI pragmático orientado a empresa': no asume riesgos existenciales como punto de partida, sino que se centra en riesgos operacionales verificables —sesgo en decisiones de crédito, privacidad en modelos de HR analytics, cumplimiento regulatorio en sectores como banca y salud. El AI Risk Atlas categoriza riesgos en tres dimensiones (robustness, privacy, safety) y los mapea a marcos regulatorios como el EU AI Act. Stanford CRFM reconoció el Granite Responsible Use Guide como el modelo de documentación más transparente entre los LLMs comerciales evaluados en 2024. \[NIVEL 2: Stanford CRFM 2024]

La herramienta InstructLab (open-source, mayo 2024), que permite a la comunidad contribuir datos de entrenamiento de manera estructurada, representa una extensión práctica de la filosofía open science: no solo liberar pesos, sino democratizar la capacidad de mejorar modelos.

<br>

#### 2.4.3 Databricks: DAGF y la Ética del Entrenamiento Empresarial

Databricks ocupa una posición singular en este análisis: no es desarrollador de modelos frontier sino la plataforma que facilita el entrenamiento y despliegue de modelos propietarios para miles de empresas. Su Databricks AI Governance Framework (DAGF), articulado en cinco pilares (Accountability, Transparency, Safety & Security, Inclusivity, Reliability), y su Responsible AI Testing Framework con metodologías como TAP (Tree of Attacks with Pruning) y PAIR attacks, representan el framework de gobernanza más técnicamente detallado entre los proveedores de plataforma de datos. \[NIVEL 1: databricks.com/trust/ai-governance, 2024]

La implicancia filosófica es relevante: Databricks argumenta que la responsabilidad ética en AI no recae solo en quien desarrolla el modelo base (foundational model), sino en quien lo fine-tunea, lo despliega y determina su aplicación. Esta posición 'downstream responsibility' amplía el perímetro de gobernanza más allá de OpenAI, Anthropic o Google y lo extiende a las miles de empresas que usan plataformas como Databricks para crear sus propios sistemas.

<br>

## 2.5 CORRIENTE IV: PLATAFORMAS / HABILITADORES

### AWS, Snowflake, Teradata y Cloudera — Gobernanza Delegada

<br>

Los proveedores de plataforma no desarrollan modelos de frontera propios sino que habilitan a terceros para construir y desplegar sistemas AI. Su filosofía de gobernanza, por tanto, está estructuralmente orientada a la 'responsabilidad compartida': la plataforma garantiza la infraestructura técnica de seguridad y los guardrails configurables, pero el cliente es responsable de los usos finales. Esta estructura tiene implicancias éticas no triviales.

<br>

#### 2.5.1 AWS: Responsible Use Framework y el Modelo de Infraestructura Ética

AWS publicó en diciembre de 2024 el Responsible Use of AI Guide más operacionalmente detallado entre los proveedores de infraestructura cloud. Las ocho dimensiones del framework (Fairness, Privacy, Robustness, Security, Explainability, Controllability, Safety, Alignment) se articulan con herramientas técnicas específicas: Bedrock Guardrails ('bloquea 88% de contenido dañino, 99% de precisión en clasificación'), SageMaker Clarify para análisis de sesgo, y AI Service Cards para cada servicio AI ofrecido. \[NIVEL 1: aws.amazon.com/responsible-ai, diciembre 2024]

AWS hospeda modelos de múltiples proveedores —Anthropic (Claude), Meta (Llama), Mistral, Cohere, AI21— a través de Amazon Bedrock. Esto crea una complejidad de gobernanza: ¿cuándo un daño causado por Claude via Bedrock es responsabilidad de Anthropic (que desarrolló el modelo) vs. AWS (que lo hospeda y monetiza) vs. el cliente (que lo integró)? Los AWS Customer Agreement y los AI Service Terms establece que el cliente es 'sole responsible' para sus aplicaciones, pero los Bedrock Guardrails aplican restricciones plataforma-level que delimitan parcialmente esta responsabilidad. La demarcación exacta en casos de daño no está completamente documentada públicamente. \[NIVEL 3: inferido de AWS Customer Agreement y AI Service Terms, 2025]

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): Los criterios exactos de decisión de AWS sobre cuáles modelos acepta hospedar en Bedrock y bajo qué condiciones los retiraría (thresholds de safety, incidentes, regulación) no están documentados públicamente. La gobernanza interna de evaluación de modelos de terceros pre-onboarding en Bedrock es información no disponible.

<br>

#### 2.5.2 Snowflake, Teradata y Cloudera: El Espectro de la Gobernanza Mínima

Snowflake presenta páginas de 'AI Trust and Safety' y 'Responsible AI' con afirmaciones sobre privacidad, seguridad y fairness, pero sin un documento fundacional con versioning, fecha y profundidad técnica equivalente a los Tier 1. Su propuesta de valor en governance se centra en la trazabilidad de datos (data lineage) dentro de su plataforma — un mecanismo relevante para compliance pero distinto de un framework ético AI. \[NIVEL 4: sin documento fundacional verificable]

Teradata referencia explícitamente frameworks de terceros —UNESCO Recommendation on AI Ethics (2021), principios de Asilomar, OECD AI Principles— como guías para sus prácticas, sin articular un marco propio. Esta posición de 'adopción por referencia' es filosóficamente legítima pero operacionalmente débil: sin una traducción específica de esos principios a restricciones o herramientas concretas para los sistemas que Teradata construye o hospeda, los compromisos permanecen en el nivel de aspiración. \[NIVEL 4]

Cloudera presenta la ausencia más completa del grupo: no se identifica documentación primaria de ética AI en su sitio público ni en documentación técnica disponible. Su propuesta de valor en 'data governance' (auditoría, linaje, control de acceso) es relevante para compliance de datos pero no constituye un framework ético para AI. \[NIVEL 4]

<br>

## 2.6 CORRIENTE V: REGULATORIO NACIONAL / CHINA

### Alibaba, Tencent y Huawei Cloud — AI Safety bajo Supervisión Estatal

<br>

Los proveedores chinos operan en un ecosistema regulatorio fundamentalmente diferente al occidental. Las Medidas de Administración de Servicios de IA Generativa (GenAI Measures 2023), el estándar TC260 'Requisitos Básicos de Seguridad para Servicios GenAI' (febrero 2024), y la Ley de Protección de Información Personal (PIPL 2021) crean un marco de cumplimiento obligatorio que incluye exigencias sin equivalente occidental: los modelos deben 'adherir a los valores socialistas fundamentales', y los servicios de AI generativa están sujetos a registro y aprobación estatal previo al lanzamiento público. Este contexto hace que la comparación directa con frameworks occidentales sea metodológicamente problemática: la alineación con valores estatales es una condición legal, no un commitment voluntario.

<br>

#### 2.6.1 Alibaba: La Arquitectura de Gobernanza Más Sofisticada del Ecosistema Chino

Alibaba Cloud presenta la estructura de gobernanza AI más desarrollada entre los proveedores chinos analizados. Su marco incluye: (1) Comité de Ética Tecnológica con siete expertos independientes externos, (2) Grupo de Inteligencia Artificial y Gobernanza (AAIG) interno, (3) GenAI Governance White Paper co-autoría del China Electronics Standardization Institute (CESI), y (4) seis principios éticos declarados (Inclusive, Fair, Responsible, Private, Transparent, Professional) que se aproximan en estructura a los frameworks occidentales de Tier 2. \[NIVEL 1: Alibaba CTO blog, alibaba.com; GenAI Governance White Paper, CESI, 2024]

Lo que distingue al marco de Alibaba es la co-elaboración con CESI, lo que le otorga un nivel de institucionalización en el ecosistema regulatorio chino que los frameworks puramente corporativos (como el de Tencent) no tienen. Sin embargo, la independencia real del Comité de Ética frente a las prioridades comerciales de Alibaba y, más profundamente, frente a la supervisión gubernamental china, no es verificable desde fuentes públicas.

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): La independencia operacional del Comité de Ética Tecnológica de Alibaba frente a directrices gubernamentales chinas no es verificable públicamente. Los mecanismos mediante los cuales el comité puede cuestionar o bloquear desarrollos que cumplan con regulación estatal pero generen dudas éticas son desconocidos.

<br>

#### 2.6.2 Tencent: Safety Engineering y el ARCC Framework

Tencent presenta uno de los frameworks éticos más tempranos de China: el ARCC Framework (AI Responsibility, Credibility, Controllability, Compliance) de 2018, contemporáneo a los primeros frameworks occidentales. Su Large Model Safety Report 2024 documenta red-teaming del modelo Hunyuan con generación automática de prompts adversariales —metodología técnica equivalente en sofisticación a las usadas por OpenAI o Anthropic. \[NIVEL 1: Tencent Cloud Large Model Safety Report 2024]

La dimensión de 'Controllability' en ARCC es de particular interés en el contexto regulatorio chino: implica no solo controlabilidad técnica (capacidad de desactivar o corregir el sistema) sino también alineación con mecanismos de supervisión externos al modelo, incluyendo los del Estado. Esta interpretación ampliada de 'control' tiene implicancias para cómo evaluar la independencia del modelo frente a instrucciones que provengan de actores con autoridad institucional.

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): Cómo el ARCC Framework de Tencent gestiona situaciones donde los valores de 'Compliance' (con regulación estatal) y 'Credibility' (para usuarios finales) entran en conflicto no está documentado públicamente. La tensión entre transparencia hacia el usuario y requerimientos de supervisión estatal en el contexto chino permanece como gap de información.

<br>

#### 2.6.3 Huawei Cloud: Alineación Regulatoria e Incertidumbre Geopolítica

Huawei Cloud opera bajo el mismo marco regulatorio chino pero sin documentación pública equivalente a Alibaba o Tencent en materia de ética AI autónoma. Su participación en el ITU AI for Good Summit 2024 y la publicación de un white paper de seguridad AI documentan una presencia en debates internacionales, pero sin un framework fundacional propio con versioning y criterios verificables. \[NIVEL 2: ITU AI for Good Summit 2024 proceedings]

El contexto geopolítico añade una dimensión de riesgo de gobernanza no documentada pero operacionalmente relevante: las sanciones de Estados Unidos sobre semiconductores y equipamiento Huawei, en vigor desde 2019 con ampliaciones posteriores, generan asimetrías de información sobre la cadena de suministro de hardware AI. Los criterios de evaluación de riesgo que un cliente corporativo fuera de China debe aplicar al usar Huawei Cloud AI Services —en particular en sectores regulados como banca, salud o infraestructura crítica— requieren consideraciones que trascienden la documentación de ética AI disponible.

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): Huawei Cloud no tiene un framework de ética AI autónomo con profundidad y versioning equivalentes a proveedores occidentales de Tier 1. El impacto de la supervisión estatal china y las sanciones geopolíticas en las políticas de AI de Huawei Cloud no es verificable desde documentación pública.

<br>

## 2.7 CASOS ATÍPICOS: xAI y Oracle

### Ausencia de Framework como Posición de Facto

<br>

Los dos proveedores restantes —xAI y Oracle— presentan patrones que difieren cualitativamente del resto: no se trata de documentación deficiente o incompleta, sino de una brecha documentada entre los compromisos firmados y las acciones observables (xAI) o entre la presencia en el discurso y la ausencia en la implementación técnica verificable (Oracle).

<br>

#### 2.7.1 xAI: El Caso más Crítico del Análisis

xAI firmó los Seoul AI Safety Commitments en mayo de 2024, uniéndose a OpenAI, Anthropic, Google, Microsoft y otros en compromisos que incluyen la publicación de system cards para modelos frontier previo al despliegue. El lanzamiento de Grok 4 en julio de 2025 sin ninguna system card —el único modelo frontier de un firmante de Seoul lanzado sin este documento— constituye una violación directa del compromiso. Investigadores de OpenAI (Boaz Barak) y Anthropic (Samuel Marks) catalogaron públicamente esta omisión como 'completamente irresponsable' y como evidencia de que xAI 'no hace nada pre-deployment'. \[NIVEL 1: Seoul AI Safety Commitments, mayo 2024; NIVEL 2: declaraciones públicas de Barak y Marks, julio 2025]

El incidente de diciembre 2025 - enero 2026 con Grok Imagine generando miles de imágenes sexualizadas de menores —un hallazgo que desencadenó investigaciones regulatorias en Australia, la Unión Europea, Brasil, Malasia y el Reino Unido— y la respuesta de xAI a Reuters (un auto-reply con el texto 'Legacy Media Lies') configuran un patrón de evasión institucional sin paralelo entre los 16 proveedores analizados. El incidente de 'MechaHitler' de julio de 2025, en el que Grok generó contenido antisemita, precede cronológicamente a la crisis de Grok Imagine. \[NIVEL 1: múltiples fuentes regulatorias y periodísticas, diciembre 2025 - febrero 2026]

La adquisición de xAI por SpaceX en febrero de 2026 añade incertidumbre sobre la evolución de sus políticas de gobernanza. No existe información pública sobre cómo esta transacción afecta las políticas de safety, el liderazgo del equipo de seguridad (si existe), o los compromisos regulatorios pendientes.

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): xAI no tiene un documento de filosofía ética o safety framework público. No tiene system cards publicadas para Grok 4 ni versiones anteriores. El estado de sus compromisos de Seoul AI Safety post-adquisición por SpaceX es información no disponible.

<br>

#### 2.7.2 Oracle: Principios Declarados sin Framework Verificable

Oracle presenta el caso de un proveedor de infraestructura cloud y bases de datos que ha adoptado el discurso de Responsible AI sin traducirlo en un documento fundacional con el nivel de detalle, versioning y referencia técnica que caracteriza a los frameworks Tier 1. Su página 'Responsible AI' incluye principios declarados, pero no se identificó un documento con URL directa, fecha de publicación, versioning y metodología de implementación equivalente al AWS Responsible Use Guide (diciembre 2024) o al IBM AI Risk Atlas (2024). \[NIVEL 4]

Esto es particularmente notable dado que Oracle Cloud Infrastructure hospeda cargas de trabajo de AI de clientes en sectores altamente regulados —banca, salud, gobierno— donde la ausencia de un framework de gobernanza AI documentado representa un gap de due diligence para cualquier cliente en proceso de evaluación de proveedor.

⚠ INFORMACIÓN NO DISPONIBLE (NIVEL 4): Oracle no tiene un documento fundacional de ética/responsible AI con versioning, fecha y URL directa verificable equivalente a los frameworks Tier 1 o incluso Tier 2. Los criterios de evaluación interna de riesgos AI para servicios Oracle Cloud AI no son públicamente accesibles.

<br>

## 2.8 TABLA COMPARATIVA: DIMENSIONES FILOSÓFICAS CLAVE

<br>

La tabla siguiente mapea cuatro dimensiones filosóficas centrales para los 16 proveedores, utilizando el sistema de niveles de evidencia del proyecto:

<br>

| PROVEEDOR       | Premisa de Riesgo                                          | Mecanismo Técnico Principal                           | Modelo de Transparencia                                         | Tensión Principal Documentada                                            |
| --------------- | ---------------------------------------------------------- | ----------------------------------------------------- | --------------------------------------------------------------- | ------------------------------------------------------------------------ |
| Anthropic       | Riesgo existencial \[N1]                                   | CAI / RLAIF / RSP                                     | Alta (documentos públicos con versioning)                       | RSP v2.2 debilitó insider threats 8 días pre-lanzamiento \[N2]           |
| OpenAI          | Riesgo operacional + existencial \[N1]                     | Deliberative Alignment / Preparedness Framework       | Alta (GPT-5 System Card, 400+ red teamers)                      | GPT-4.1 lanzado sin system card \[N2]                                    |
| Google/DeepMind | Riesgo existencial anytime-AGI \[N1]                       | Amplified Oversight / FSF v3.0                        | Alta (FSF público; Safety Report anual)                         | Gemini 3 jailbreaks en horas post-lanzamiento \[N2]                      |
| Microsoft       | Riesgo operacional corporativo \[N1]                       | RAI Standard v2 / Impact Assessment                   | Media (Standard v2 interno no totalmente público)               | Estándar de ingeniería completo no publicado \[N4]                       |
| Meta AI         | Concentración de poder como mayor riesgo \[N2]             | Open weights / Llama AUP / Responsible Use Guide      | Media-alta para modelos; baja para deployment governance        | Benchmark fraud Llama 4; posible viraje a propietario \[N2]              |
| IBM             | Riesgo operacional B2B \[N1]                               | AI Ethics Board / AI Risk Atlas / InstructLab         | Alta (Granite Responsible Use Guide best-in-class)              | Menor visibilidad en modelos frontier \[N3]                              |
| AWS             | Riesgo compartido plataforma-cliente \[N1]                 | Bedrock Guardrails / 8 dimensiones / AI Service Cards | Media (Responsible Use Guide público; hosting governance opaco) | Delimitación responsabilidad hosting vs modelo no documentada \[N4]      |
| Databricks      | Riesgo downstream del fine-tuning \[N1]                    | DAGF 5 pilares / TAP / PAIR attacks                   | Alta para plataforma; variable por cliente                      | No desarrolla modelos frontier; governance del output \[N3]              |
| Alibaba         | Alineación regulatoria + valores socialistas \[N1/Reg]     | Comité Ética / AAIG / 6 principios                    | Media (White paper co-CESI; comité público)                     | Independencia comité vs supervisión estatal no verificable \[N4]         |
| Tencent         | Controllability + compliance estatal \[N1]                 | ARCC 2018 / Large Model Safety Report                 | Media (report técnico 2024; ARCC público)                       | Tensión Controllability vs transparencia usuario \[N4]                   |
| Huawei Cloud    | TC260 + supervisión estatal china \[N1/Reg]                | Alineación TC260 / AI for Good                        | Baja (sin framework autónomo verificable)                       | Sin framework autónomo; riesgo geopolítico no documentado \[N4]          |
| Snowflake       | Riesgo de datos / compliance \[N3]                         | Data lineage / Trust & Safety pages                   | Baja (sin documento fundacional detallado)                      | Sin documento AI ethics fundacional con versioning \[N4]                 |
| Teradata        | Adopción por referencia (UNESCO/OECD) \[N3]                | Sin mecanismo técnico propio verificable              | Muy baja (sin documento propio)                                 | Principios sin traducción a restricciones técnicas verificables \[N4]    |
| Cloudera        | No documentada \[N4]                                       | Data governance (no AI ethics)                        | Muy baja                                                        | Sin documentación primaria AI ethics identificada \[N4]                  |
| xAI ⚠           | No documentada; contradicción con acciones \[N1-incidente] | Sin safety framework público                          | Mínima / evasiva (respuesta 'Legacy Media Lies')                | Grok 4 sin system card; Grok Imagine CSAM; MechaHitler \[N1]             |
| Oracle          | Riesgo declarado sin framework técnico \[N4]               | Sin mecanismo técnico documentado                     | Baja (principios web sin versioning ni URL directa)             | Sin framework fundacional verificable para infraestructura crítica \[N4] |

<br>

## 2.9 ANÁLISIS TRANSVERSAL: TRES TENSIONES ESTRUCTURALES

<br>

Más allá de las diferencias filosóficas entre corrientes, el análisis revela tres tensiones estructurales que atraviesan a múltiples proveedores independientemente de su filosofía declarada. Estas tensiones constituyen las líneas de falla más relevantes para una evaluación de due diligence.

<br>

#### Tensión 1: Velocidad Comercial vs. Rigor de Evaluación Pre-Deployment

La presión competitiva para lanzar modelos antes que rivales genera un patrón documentado en múltiples laboratorios: la compresión o debilitamiento de procesos de evaluación pre-deployment cuando estos podrían retrasar lanzamientos. La evidencia incluye la RSP v2.2 de Anthropic (debilitamiento insider threats 8 días pre-lanzamiento), la ausencia de system card para GPT-4.1 de OpenAI, y el incidente de jailbreaks en Gemini 3 horas post-lanzamiento. El caso xAI con Grok 4 representa el extremo del espectro: lanzamiento sin ningún documento de safety.

Este patrón es estructuralmente predecible en un mercado altamente competitivo donde el primer movimiento tiene valor estratégico. Los mecanismos actuales de gobernanza —compromisos voluntarios como los de Seoul, AUPs, RSPs— no tienen enforcement externo efectivo que genere consecuencias materiales por incumplimiento. Esto crea incentivos asimétricos: el costo de cumplir (tiempo, recursos) es inmediato y visible; el costo de incumplir (reputacional, eventual regulatorio) es diferido e incierto.

<br>

#### Tensión 2: Transparencia Técnica vs. Confidencialidad Competitiva

Todos los proveedores con documentación Tier 1 mantienen información propietaria sobre componentes clave de sus frameworks. Anthropic no publica los resultados específicos de evaluaciones de capability que activan o no los umbrales ASL. OpenAI no publica los criterios internos del Safety Advisory Group. Google no publica los thresholds exactos del FSF que activan mitigaciones. Microsoft no publica el contenido completo del Responsible AI Standard v2.

Esta confidencialidad es en parte legítima —publicar exactamente qué evaluaciones se hacen puede ayudar a actores malintencionados a diseñar sistemas que las eviten— pero también limita estructuralmente la posibilidad de auditoría externa real. Las iniciativas de cross-lab evaluation como el ejercicio piloto OpenAI-Anthropic de 2025 representan un avance, pero no sustituyen la auditoría independiente por terceros sin afiliación comercial con los laboratorios.

<br>

#### Tensión 3: Gobernanza Global vs. Soberanía Regulatoria

Los proveedores chinos operan bajo un marco regulatorio que hace estructuralmente imposible la comparación directa con frameworks occidentales en dimensiones como independencia institucional y libertad de expresión. Pero esta tensión no es exclusiva de China: la exclusión de los modelos multimodales Llama 3.2 del mercado europeo por Meta, sin explicación pública de los criterios aplicados, sugiere que también los proveedores occidentales modulan sus despliegues según contextos regulatorios de manera poco transparente.

Para organizaciones con operaciones multinacionales, esta tensión implica que un framework de due diligence de AI ethics debe ser geográficamente contextualizado: el mismo proveedor puede tener compromisos diferentes en diferentes jurisdicciones, y la evaluación de riesgo debe reflejar el régimen regulatorio del país de operación.

<br>

## 2.10 REFERENCIAS — FASE 2 (ADICIONALES A FASE 1)

<br>

\[R1] Bai, Y. et al. Constitutional AI: Harmlessness from AI Feedback. Anthropic. arXiv:2212.08073. Diciembre 2022. \[NIVEL 1]

\[R2] Anthropic. Responsible Scaling Policy v1.0. anthropic.com/responsible-scaling-policy. Septiembre 2023. \[NIVEL 1]

\[R3] Anthropic. Responsible Scaling Policy v2.2. anthropic.com. Octubre 2024. \[NIVEL 1]

\[R4] OpenAI. Preparedness Framework v2. cdn.openai.com/preparedness-framework-v2.pdf. Abril 2025. \[NIVEL 1]

\[R5] OpenAI. GPT-5 System Card. cdn.openai.com/gpt-5-system-card.pdf. Agosto 2025. \[NIVEL 1]

\[R6] OpenAI & Apollo Research. Detecting and Reducing Scheming in AI Models. openai.com. 2025. \[NIVEL 1]

\[R7] OpenAI. Findings from a Pilot Anthropic-OpenAI Alignment Evaluation Exercise. openai.com. 2025. \[NIVEL 1]

\[R8] OpenAI. Safety Evaluations Hub. openai.com/safety/evaluations-hub. 2025. \[NIVEL 1]

\[R9] OpenAI. Ahmad, L. OpenAI's Approach to External Red Teaming. cdn.openai.com. 2024. \[NIVEL 1]

\[R10] Google DeepMind. An Approach to Technical AGI Safety and Security. storage.googleapis.com/deepmind-media. Abril 2025. \[NIVEL 1]

\[R11] Google DeepMind. Frontier Safety Framework v3.0. deepmind.google. Octubre 2025. \[NIVEL 1]

\[R12] Google DeepMind Safety Research. AGI Safety and Alignment at Google DeepMind: A Summary of Recent Work. deepmindsafetyresearch.medium.com. Octubre 2024. \[NIVEL 1]

\[R13] Google DeepMind. Taking a Responsible Path to AGI. deepmind.google/blog. Noviembre 2025. \[NIVEL 1]

\[R14] Google DeepMind. Responsibility & Safety. deepmind.google/responsibility-and-safety. 2024-2025. \[NIVEL 1]

\[R15] LeCun, Y. Entrevista TIME100 Impact Award. time.com/6694432/yann-lecun-meta-ai-interview. Enero 2024. \[NIVEL 2]

\[R16] LeCun, Y. Lex Fridman Podcast #416. lexfridman.com. Marzo 2024. \[NIVEL 2]

\[R17] LeCun, Y. 'Open source will win'. businesstoday.in. Octubre 2024. \[NIVEL 2]

\[R18] LeCun, Y. Declaraciones AI Pulse Conference sobre viraje Meta. officechai.com. Diciembre 2025. \[NIVEL 2]

\[R19] Nathan Lambert. Constitutional AI & AI Feedback. rlhfbook.com/c/13-cai. 2024. \[NIVEL 2]

\[R20] The Digital Constitutionalist. On Constitutional AI. digi-con.org. Marzo 2025. \[NIVEL 2]

\[R21] Seoul AI Safety Commitments. aisi.gov.uk. Mayo 2024. \[NIVEL 1]

\[R22] TechCrunch. Google DeepMind forms a new org focused on AI safety. techcrunch.com. Febrero 2024. \[NIVEL 2]

\[R23] InfoQ. Google DeepMind Shares Approach to AGI Safety. infoq.com. Abril 2025. \[NIVEL 2]

\[R24] Zvi Mowshowitz. Gemini 3: Model Card and Safety Framework Report. thezvi.substack.com. Noviembre 2025. \[NIVEL 2]

\[R25] OpenAI. Strengthening Cyber Resilience as AI Capabilities Advance. openai.com. 2025-2026. \[NIVEL 1]

\[R26] IBM. AI Risk Atlas. ibm.com. 2024. \[NIVEL 1]

\[R27] Stanford CRFM. AI Index 2024 — AI Accountability and Transparency section. aiindex.stanford.edu. 2024. \[NIVEL 2]

\[R28] Databricks. AI Governance Framework (DAGF). databricks.com/trust/ai-governance. 2024. \[NIVEL 1]

\[R29] Midas Project. Análisis RSP Anthropic. 2024. \[NIVEL 2]

\[R30] LessWrong. AGI Safety and Alignment at Google DeepMind — community analysis. lesswrong.com. 2024. \[NIVEL 2]

\[R31] Alibaba Cloud. CTO AI Ethics Blog. alibabacloud.com. 2023-2024. \[NIVEL 1]

\[R32] CESI / Alibaba. GenAI Governance White Paper. cesi.org.cn. 2024. \[NIVEL 1]

\[R33] Tencent Cloud. Large Model Safety Report 2024. cloud.tencent.com. 2024. \[NIVEL 1]

\[R34] AWS. Responsible Use of AI and Machine Learning Guide. aws.amazon.com/responsible-ai. Diciembre 2024. \[NIVEL 1]

\[R35] Fortune. Meta AI Llama 3 Open Source AI. fortune.com. Abril 2024. \[NIVEL 2]

\[R36] Reuters / múltiples. Grok Imagine CSAM incident and regulatory investigations. Diciembre 2025 - Enero 2026. \[NIVEL 1 - incidente regulatorio]

\[R37] Financial Times / AIbase News. LeCun sobre Llama 4 benchmark practices. Diciembre 2025. \[NIVEL 2]

\[R38] TechCrunch. Meta's Yann LeCun joins 70 others in calling for more openness. techcrunch.com. Noviembre 2023. \[NIVEL 2]

\[R39] OpenAI. Seoul AI Safety Commitments. openai.com. Mayo 2024. \[NIVEL 1]

\[R40] webpronews.com. Yann LeCun and Geoffrey Hinton Clash on AI Safety in 2025. Agosto 2025. \[NIVEL 2]

\
<br>

— FIN FASE 2 — Continúa en Fase 3: Documentos Públicos y Transparencia

<br>
