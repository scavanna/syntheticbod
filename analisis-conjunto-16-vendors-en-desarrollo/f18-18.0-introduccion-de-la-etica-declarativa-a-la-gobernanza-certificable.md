---
icon: check
---

# F18 - 18.0 INTRODUCCI√ìN: DE LA √âTICA DECLARATIVA A LA GOBERNANZA CERTIFICABLE



**¬ß18.0 Introducci√≥n** ‚Äî La transici√≥n de √©tica declarativa a gobernanza certificable: tres razones convergentes que la hacen inevitable en 2026 (reguladores, stakeholders internos, frameworks maduros disponibles). Tabla comparativa de 6 dimensiones: pol√≠tica vs. sistema, accountability difusa vs. roles documentados, risk register gen√©rico vs. AI-specific, evidencia declarativa vs. trazable.

**¬ß18.1 ISO/IEC 42001:2023** ‚Äî Las 10 cl√°usulas detalladas con relevancia espec√≠fica para el CISO: desde Scope (qu√© significa ser AI deployer vs. AI provider) hasta la Cl√°usula 10 (PDCA que diferencia el AIMS de una implementaci√≥n puntual). Selecci√≥n de los 9 controles del Annex A m√°s relevantes para el ecosistema M365/Copilot, con descripci√≥n, implementaci√≥n concreta y nivel de madurez requerido. La certificaci√≥n ISO 42001 de Microsoft para M365 Copilot: qu√© cubre y qu√© no cubre.

**¬ß18.2 NIST AI RMF 1.0** ‚Äî Las 4 funciones GOVERN/MAP/MEASURE/MANAGE con implementaci√≥n espec√≠fica en contexto M365/Copilot/Azure AI Foundry. Las 7 caracter√≠sticas de IA trustworthy como arquitectura del programa de gobernanza, mapeadas a las fases del proyecto que las trabajan. El AI RMF como lenguaje com√∫n para comunicar el estado de gobernanza al Board.

**¬ß18.3 NIST AI 600-1 (Perfil GenAI)** ‚Äî Los 11 riesgos √∫nicos o exacerbados por la IA generativa seg√∫n el perfil publicado en julio 2024: confabulation, data privacy, data provenance, harmful bias, human-AI configuration, IP, prompt injection, CBRN, obscene content, information integrity, value chain. Para cada riesgo: descripci√≥n y acciones concretas en contexto M365.

**¬ß18.4 Mapping Cruzado** ‚Äî Tabla de 10 dominios de gobernanza mapeando simult√°neamente ISO 42001 cl√°usula/control, NIST AI RMF funci√≥n, EU AI Act art√≠culo, y GDPR/Ley 25.326 art√≠culo. La regla de oro: implementar un control una sola vez pero documentarlo como evidencia de m√∫ltiples frameworks.

**¬ß18.5 Certificaci√≥n Microsoft** ‚Äî An√°lisis comparativo preciso: 6 cosas que cubre la certificaci√≥n ISO 42001 de Microsoft para Copilot (sistema de gesti√≥n del proveedor), vs. 6 cosas que no cubre y son obligaciones del deployer (AIMS de la organizaci√≥n, risk assessments de casos de uso propios, DPIAs, IR plan interno). La analog√≠a del datacenter ISO 27001.

**¬ß18.6 AI Governance Committee** ‚Äî Composici√≥n de 8 roles con funci√≥n, aporte espec√≠fico y frecuencia de participaci√≥n, incluyendo el nuevo rol de AI Risk Manager. Agenda tipo de reuni√≥n mensual de 80 minutos estructurada en 7 slots: apertura, Risk Scorecard de IA, evaluaci√≥n de nuevos sistemas, alertas regulatorias, estado de red teaming, incidentes del per√≠odo, y pr√≥ximos pasos.

**¬ß18.7 Gap Analysis y Hoja de Ruta** ‚Äî Tabla de 10 requerimientos ISO 42001 comparando qu√© tiene una organizaci√≥n con ISO 27001 + GDPR, cu√°l es el gap espec√≠fico de IA, y qu√© acci√≥n cierra ese gap. Hoja de ruta en 3 horizontes: H1 (0-90 d√≠as: fundaciones ‚Äî AI Policy, AGC, inventario), H2 (90 d√≠as-12 meses: controles operacionales ‚Äî risk register, red teaming, DPIAs, Purview), H3 (12-24 meses: madurez y certificaci√≥n ‚Äî PDCA documentado, internal audit, SoA, certificaci√≥n opcional).

**Estado del proyecto:** Fases 1-18 ¬∑ 450 referencias ¬∑ 16 proveedores.



## 18.0 INTRODUCCI√ìN: DE LA √âTICA DECLARATIVA A LA GOBERNANZA CERTIFICABLE

<br>

Durante d√©cadas, la respuesta corporativa a los riesgos √©ticos de la tecnolog√≠a fue declarativa: publicar una pol√≠tica de uso aceptable, incluir cl√°usulas en contratos de proveedores, y mencionar 'valores' en el informe anual. La IA generativa ha vuelto esa aproximaci√≥n insostenible por tres razones convergentes.

Primera raz√≥n: los reguladores dejaron de aceptar declaraciones. El EU AI Act impone fines de hasta EUR 35 millones o el 7% del revenue global por sistemas de IA prohibidos, y fines de EUR 15 millones o el 3% para sistemas de alto riesgo incumplidos. La SEC en Estados Unidos exige divulgaci√≥n de riesgos materiales de IA. Los reguladores argentinos est√°n adaptando la Ley 25.326. Una declaraci√≥n de principios no constituye evidencia de control.

Segunda raz√≥n: los stakeholders internos exigen accountability t√©cnico. El Board pregunta: '¬øc√≥mo s√© que nuestros 20,000 usuarios de Copilot no est√°n filtrando datos confidenciales?' El departamento legal pregunta: '¬øpodemos demostrar que seguimos GDPR Art. 25 en nuestros sistemas de IA?' El auditor interno pregunta: '¬øhay controles sobre qui√©n aprueba el deployment de agentes de IA?' Estas preguntas requieren sistemas de gesti√≥n, no declaraciones.

Tercera raz√≥n: los frameworks de gobernanza de IA maduros ya existen. ISO/IEC 42001:2023 ‚Äî el primer est√°ndar internacional de sistemas de gesti√≥n de IA ‚Äî provee un framework auditable, certificable, y compatible con ISO 27001. NIST AI RMF 1.0 provee el lenguaje com√∫n para gesti√≥n de riesgos de IA adoptado por reguladores, auditores y la industria. Microsoft ya tiene certificaci√≥n ISO 42001 para M365 Copilot. El CISO puede y debe utilizarlos.

<br>

| DIMENSI√ìN                | √âTICA DECLARATIVA (estado anterior)                                                    | GOBERNANZA CERTIFICABLE (estado objetivo)                                                                                                                                                                              |
| ------------------------ | -------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Base                     | Principios publicados en website, c√≥digo de √©tica corporativo, cl√°usulas en contratos. | Sistema de gesti√≥n auditable: pol√≠ticas operativas, controles t√©cnicos, evidencias documentadas, ciclo PDCA de mejora continua.                                                                                        |
| Accountability           | 'El equipo de IA es responsable de usar la IA √©ticamente.'                             | Roles documentados: AI Governance Committee, AI Risk Owner por sistema, DPO, CISO con funciones espec√≠ficas. Escalaci√≥n definida.                                                                                      |
| Gesti√≥n de riesgos       | Risk register gen√©rico de TI que puede o no mencionar 'riesgos de IA'.                 | AI-specific risk register (NIST AI RMF MAP), con categor√≠as: riesgos de sesgo, memorizaci√≥n, exfiltraci√≥n, excessive agency, misinformation. Probabilidad e impacto por sistema.                                       |
| Evidencia para auditor√≠a | 'Contamos con una pol√≠tica de IA responsable aprobada por el Board en 2024.'           | Evidencia trazable: test results de red teaming (F13), DPIA completados (F15), sensitivity label coverage %, DLP policy effectiveness, incident log de IA, conformity assessment report.                               |
| Cumplimiento regulatorio | 'Cumplimos con GDPR y las regulaciones aplicables.'                                    | Compliance Manager con templates EU AI Act, GDPR, ISO 42001. Mapping regulaci√≥n ‚Üí control ‚Üí evidencia. Assessment score por framework. Gap plan con fechas y responsables.                                             |
| Mejora continua          | 'Revisamos nuestras pol√≠ticas anualmente.'                                             | PDCA estructurado: Plan (risk assessment, objectives), Do (implementar controles), Check (internal audit, management review), Act (corrective actions, update policies). Ciclo semestral para sistemas de alto riesgo. |

<br>

## 18.1 ISO/IEC 42001:2023 ‚Äî EL SISTEMA DE GESTI√ìN DE IA

<br>

ISO/IEC 42001:2023, publicado en diciembre de 2023, es el primer est√°ndar internacional para Sistemas de Gesti√≥n de Inteligencia Artificial (AIMS por sus siglas en ingl√©s: AI Management System). El est√°ndar sigue la estructura de alto nivel (HLS) compartida con ISO 27001, ISO 9001, ISO 14001 e ISO 27701, lo que significa que las organizaciones ya certificadas en ISO 27001 tienen una ventaja estructural significativa: el 60-70% de los procesos de gesti√≥n son an√°logos.

<br>

### 18.1.1 Estructura del Est√°ndar: 10 Cl√°usulas y 38 Controles

| #  | CL√ÅUSULA                    | CONTENIDO Y RELEVANCIA PARA EL CISO                                                                                                                                                                                                                                                                                                                                         |
| -- | --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1  | Scope (Alcance)             | Define que el est√°ndar aplica a organizaciones que desarrollan, proveen o usan sistemas de IA. Para el CISO con M365 E5: el scope m√≠nimo es 'uso de sistemas de IA' ‚Äî todos los usuarios de Copilot est√°n en scope. Si la organizaci√≥n desarrolla agentes en Copilot Studio o apps en Azure AI Foundry, el scope ampl√≠a a 'desarrollo'.                                     |
| 2  | Normative References        | Referencias normativas: ISO/IEC 22989:2022 (terminolog√≠a de IA) y ISO/IEC 23894:2023 (gesti√≥n de riesgos de IA). El CISO debe conocer la terminolog√≠a est√°ndar para comunicar con auditores externos.                                                                                                                                                                       |
| 3  | Terms & Definitions         | Glosario de 30+ t√©rminos. Definiciones clave para el CISO: AI system (incluye Copilot y sus agentes), AI provider (Microsoft), AI deployer (la organizaci√≥n), AI system impact assessment (similar al DPIA de IA de la F15).                                                                                                                                                |
| 4  | Context of the Organization | La organizaci√≥n debe entender: factores externos e internos que afectan su AIMS, stakeholders (empleados, clientes, reguladores, el proveedor de IA), scope del AIMS, el contexto de los sistemas de IA (qu√© pueden hacer, qu√© riesgos presentan). Output clave: inventario de sistemas de IA con su contexto.                                                              |
| 5  | Leadership                  | La alta direcci√≥n debe: demostrar compromiso con el AIMS, establecer la AI Policy (el documento de gobernanza de IA de la organizaci√≥n), asignar roles y responsabilidades. Para el CISO: ISO 42001 requiere expl√≠citamente el respaldo del CEO/Board ‚Äî no puede ser un programa solo de TI. El AI Governance Committee (ver ¬ß18.6) es la materializaci√≥n de este cl√°usula. |
| 6  | Planning                    | Obligatorio: (1) AI risk assessment ‚Äî identificar riesgos y oportunidades de los sistemas de IA en scope, (2) AI system impact assessment ‚Äî evaluar el impacto de cada sistema en stakeholders, (3) objectives and plans ‚Äî establecer objetivos medibles del AIMS y planes para alcanzarlos. NIST AI RMF MAP function es el complemento t√©cnico de este cl√°usula.           |
| 7  | Support                     | Recursos, competencias, awareness, comunicaci√≥n, documentaci√≥n. Para el CISO: plan de capacitaci√≥n en AI governance para el AI Governance Committee, programa de awareness para todos los usuarios de Copilot, documentaci√≥n controlada del AIMS (policies, procedures, records).                                                                                           |
| 8  | Operation                   | La cl√°usula m√°s extensa. Requiere: (1) AI operational planning, (2) AI risk treatment ‚Äî implementar controles del Annex A, (3) AI system impact assessment ‚Äî evaluar cada sistema antes de deployment, (4) gesti√≥n de proveedores de IA (Microsoft como proveedor primario), (5) data management para sistemas de IA. Los 38 controles del Annex A son la gu√≠a operacional. |
| 9  | Performance Evaluation      | Monitoreo y medici√≥n del AIMS: KPIs del programa, internal audit del AIMS, management review. Para el CISO: el Risk Scorecard de IA (ver F13 y F15) es el instrumento principal de medici√≥n. Management review semestral con el AI Governance Committee.                                                                                                                    |
| 10 | Improvement                 | PDCA completo: cuando el AIMS no funciona (hallazgos de auditor√≠a, incidentes, desviaciones de objetivos), se requiere: an√°lisis de causa ra√≠z, corrective action, verificaci√≥n de efectividad. El ciclo continuo diferencia el AIMS de una implementaci√≥n puntual.                                                                                                         |

<br>

### 18.1.2 Los 38 Controles del Annex A ‚Äî Selecci√≥n de los m√°s relevantes

El Annex A de ISO 42001 contiene 38 controles organizados en 9 dominios ‚Äî el equivalente del Annex A de ISO 27001 pero espec√≠fico para IA. A diferencia de ISO 27001, el Annex A de ISO 42001 no es formalmente obligatorio: la organizaci√≥n selecciona los controles aplicables en la Statement of Applicability (SoA) seg√∫n el scope y riesgos identificados. A continuaci√≥n los controles de mayor relevancia para el CISO con ecosistema Microsoft:

| CONTROL                             | DESCRIPCI√ìN                                                                                                                                 | IMPLEMENTACI√ìN EN M365/COPILOT                                                                                                                                                                                     | MADUREZ REQUERIDA                                   |
| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------- |
| A.5.2 ‚Äî AI Policy                   | Pol√≠tica de IA que establece los objetivos, alcance y compromisos de la organizaci√≥n con el uso responsable de IA.                          | Documento aprobado por el Board que define: cu√°les sistemas de IA est√°n autorizados, criterios de aprobaci√≥n, responsabilidades del AI Governance Committee, compromisos de transparencia y privacidad.            | B√°sica ‚Äî implementable en 60-90 d√≠as.               |
| A.5.4 ‚Äî AI Roles                    | Definici√≥n clara de roles: AI provider, AI deployer, AI user, y sus responsabilidades espec√≠ficas en cada sistema.                          | Para Copilot: Microsoft = AI provider, la organizaci√≥n = AI deployer, los 20K empleados = AI users. Para agentes Copilot Studio: la organizaci√≥n es simult√°neamente AI provider y deployer.                        | B√°sica ‚Äî complementa gobernanza existente.          |
| A.6.1 ‚Äî AI Risk Assessment          | Proceso documentado de evaluaci√≥n de riesgos de los sistemas de IA: probabilidad, impacto, riesgo inherente, controles, riesgo residual.    | El AI-specific risk register que complementa el risk register de TI. Usar las 10 categor√≠as OWASP LLM Top 10 (F13) como taxonom√≠a de riesgos t√©cnicos. Incluir riesgos de sesgo, privacidad, y misinformation.     | Media ‚Äî requiere 3-6 meses para sistematizar.       |
| A.6.2 ‚Äî AI System Impact Assessment | Evaluaci√≥n del impacto de cada sistema de IA en los stakeholders antes del deployment y peri√≥dicamente.                                     | El DPIA de IA documentado en F15 es la implementaci√≥n de este control para sistemas que procesan datos personales. Para sistemas sin datos personales: evaluaci√≥n de impacto en empleados, clientes, y decisiones. | Media ‚Äî integrar con proceso DPIA existente.        |
| A.8.3 ‚Äî Data for AI                 | Controles sobre la calidad, integridad, y adecuaci√≥n de los datos usados para entrenar, validar, y operar sistemas de IA.                   | Para RAG de Copilot: auditor√≠a de calidad del corpus SharePoint, exclusi√≥n de datos desactualizados, control de acceso al vector database. Para fine-tuning propio: AIBOM (AI Bill of Materials).                  | Media-Alta ‚Äî requiere gobernanza de datos.          |
| A.8.4 ‚Äî AI System Lifecycle         | Controles sobre el ciclo de vida completo del sistema de IA: dise√±o, entrenamiento, testing, deployment, operaci√≥n, retiro.                 | El AI deployment gate documentado en F7 (Post-Implementation Governance) y F13 (Red Teaming) es la implementaci√≥n de este control. El pipeline SDLC IA de F13 con sus gates de aprobaci√≥n.                         | Alta ‚Äî requiere madurez de procesos DevOps/AI.      |
| A.9.3 ‚Äî AI Incident Management      | Proceso para detectar, reportar, investigar, y remediar incidentes relacionados con sistemas de IA.                                         | El AI Incident Response Plan de la F7. Integraci√≥n con Microsoft Sentinel y Defender XDR para detecci√≥n t√©cnica de incidentes de IA. Proceso de notificaci√≥n regulatoria cuando aplica EU AI Act.                  | Alta ‚Äî requiere madurez de SOC para IA.             |
| A.9.7 ‚Äî Transparency of AI          | Comunicaci√≥n transparente a los usuarios sobre el uso de IA en los sistemas que los afectan.                                                | Notificaci√≥n a empleados del uso de Copilot y qu√© datos procesa. Para chatbots externos: disclosure de que el usuario interact√∫a con IA. Modelo Card para sistemas de IA propios desplegados internamente.         | Media ‚Äî pol√≠tica y comunicaci√≥n organizacional.     |
| A.10.3 ‚Äî Human Oversight            | Mecanismos para que los seres humanos puedan supervisar, revisar, y si es necesario anular las decisiones o acciones de los sistemas de IA. | Human-in-the-loop para acciones de alto impacto en agentes Copilot Studio (documentado en F13). Kill switch para agentes. Procesos de revisi√≥n humana de outputs de IA que afectan decisiones sobre personas.      | Alta ‚Äî cr√≠tico para sistemas de alto riesgo.        |
| A.10.5 ‚Äî AI System Accuracy         | Evaluaci√≥n de la precisi√≥n, exactitud, y confiabilidad del sistema de IA a lo largo del tiempo.                                             | Para sistemas propietarios: m√©tricas de performance en producci√≥n (accuracy, drift detection). Para Copilot: evaluaci√≥n de misinformation risk (OWASP LLM09, F13). Monitoreo de behavioral drift post-update.      | Alta ‚Äî requiere MLOps maduro para sistemas propios. |

<br>

üèÜ Microsoft ISO 42001 para M365 Copilot: Microsoft obtuvo certificaci√≥n ISO/IEC 42001 para Microsoft 365 Copilot y Microsoft 365 Copilot Chat ‚Äî auditada por un tercero independiente. La certificaci√≥n confirma que Microsoft aplic√≥ el framework necesario para gestionar riesgos y oportunidades asociados al desarrollo, deployment y operaci√≥n de M365 Copilot. El CISO puede usar esta certificaci√≥n en su propio compliance assessment. El certificado y audit report est√°n disponibles en el Microsoft Service Trust Portal. Importante: la certificaci√≥n de Microsoft cubre al proveedor ‚Äî la organizaci√≥n como deployer tiene sus propias obligaciones bajo ISO 42001 que requieren implementaci√≥n interna independiente.

<br>

## 18.2 NIST AI RMF 1.0 ‚Äî GOVERN, MAP, MEASURE, MANAGE

<br>

El NIST AI Risk Management Framework (AI RMF 1.0), publicado en enero de 2023, es el framework voluntario de gesti√≥n de riesgos de IA del Instituto Nacional de Est√°ndares y Tecnolog√≠a de Estados Unidos. Aunque voluntario, es el lenguaje com√∫n de facto para AI governance en el contexto norteamericano y es adoptado por reguladores, auditores, y la industria globalmente como benchmark de 'IA responsable'. En 2025-2026, el AI RMF ha evolucionado de gu√≠a voluntaria a benchmark de compliance esperado para Fortune 500 y organizaciones con operaciones en EE.UU.

El AI RMF se organiza en cuatro funciones ‚Äî GOVERN, MAP, MEASURE, MANAGE ‚Äî con la misma l√≥gica que el NIST Cybersecurity Framework (CSF) que el CISO ya conoce. La analog√≠a es √∫til: as√≠ como el CSF gestiona riesgos de ciberseguridad, el AI RMF gestiona los riesgos espec√≠ficos de los sistemas de IA.

<br>

### 18.2.1 Las 4 Funciones del NIST AI RMF

| FN | FUNCI√ìN | QU√â ESTABLECE                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | IMPLEMENTACI√ìN EN CONTEXTO M365/COPILOT/AZURE AI                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| -- | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| G  | GOVERN  | La funci√≥n transversal: establece las estructuras, procesos, y cultura organizacional para gestionar riesgos de IA. Aplica a todas las etapas del ciclo de vida y a todos los sistemas. Si GOVERN es d√©bil, MAP/MEASURE/MANAGE son ineficaces. Requiere: AI Policy, roles documentados (AI Governance Committee), integraci√≥n con enterprise risk management, gesti√≥n de terceros (proveedores de IA como Microsoft/OpenAI), entrenamiento y awareness.                             | AI Governance Committee activo con representaci√≥n CISO, DPO, Legal, Negocio (ver ¬ß18.6). AI Policy aprobada por Board. Inventario de todos los sistemas de IA en la organizaci√≥n con sus responsables. Contratos con Microsoft revisados contra checklist F6 (enterprise contracts). Programa de capacitaci√≥n en AI risk para el equipo de gobierno. GOVERN es la base que habilita las otras 3 funciones.                                                                                |
| M‚ÇÅ | MAP     | Contextualizar el riesgo de cada sistema de IA espec√≠fico: qui√©n lo usa, qu√© hace, qui√©n puede ser afectado, qu√© puede salir mal. MAP es la fase de descubrimiento y clasificaci√≥n. Sin MAP, los controles son gen√©ricos e inefectivos. Incluye: categorizaci√≥n del sistema de IA, identificaci√≥n de stakeholders y posibles impactos negativos, documentaci√≥n de capacidades y limitaciones, identificaci√≥n de riesgos por categor√≠a. El AI Inventory es el output central de MAP. | Para cada sistema en scope (Copilot, agentes Copilot Studio, apps Azure AI Foundry): completar ficha t√©cnica con: prop√≥sito y casos de uso, datos a los que tiene acceso, herramientas disponibles (para agentes), usuarios y sus caracter√≠sticas, impactos positivos y riesgos negativos por categor√≠a (OWASP LLM Top 10, sesgo, misinformation, privacidad). MAP 5.1: documentar likelihood y magnitude de impactos negativos. Threat model de F13 es la implementaci√≥n t√©cnica de MAP. |
| M‚ÇÇ | MEASURE | Cuantificar y analizar los riesgos identificados en MAP: evaluaciones t√©cnicas, testing, benchmarking, m√©tricas de trustworthiness. MEASURE responde: ¬øcu√°n grande es realmente este riesgo? ¬øest√°n funcionando los controles? MEASURE 2.11 requiere expl√≠citamente evaluaci√≥n de fairness y bias. El AI RMF llama a esta actividad TEVV: Testing, Evaluation, Verification, and Validation.                                                                                        | Red Teaming de F13 (PyRIT, m√©tricas ASR por categor√≠a OWASP): la implementaci√≥n t√©cnica de MEASURE. Evaluaci√≥n de sesgo con Azure Responsible AI Dashboard y Fairlearn antes del deployment de modelos propios. Privacy risk measurement con Purview DSPM for AI (F15). MEASURE 4.1: m√©tricas de performance en producci√≥n ‚Äî accuracy, drift detection, behavioral monitoring. El Risk Scorecard de IA (F13 ¬ß13.7) es el output central de MEASURE.                                       |
| M‚ÇÉ | MANAGE  | Actuar sobre los riesgos identificados y medidos: implementar controles, planes de respuesta a incidentes, comunicaci√≥n a stakeholders, y mejora continua. MANAGE 4.1 requiere: post-deployment monitoring, mecanismos de appeal y override, decommissioning, y change management. MANAGE incluye la gesti√≥n de incidentes de IA y la comunicaci√≥n de riesgos residuales a los stakeholders.                                                                                        | Controles t√©cnicos de F13 (guardrails, least privilege, human-in-the-loop) y F15 (DLP, sensitivity labels, DPIA) son la implementaci√≥n de MANAGE. AI Incident Response Plan de F7. Proceso de comunicaci√≥n de riesgos residuales al AI Governance Committee (Risk Scorecard mensual). Decommissioning checklist: c√≥mo retirar un sistema de IA (eliminaci√≥n del corpus RAG, archivado de logs, notificaci√≥n a usuarios).                                                                  |

<br>

### 18.2.2 Las 7 Caracter√≠sticas de IA Trustworthy seg√∫n NIST AI RMF

El AI RMF 1.0 define que un sistema de IA trustworthy debe poseer 7 caracter√≠sticas. Estas caracter√≠sticas son la arquitectura del programa de AI governance ‚Äî cada control implementado sirve una o m√°s de estas caracter√≠sticas. Para el CISO, son el vocabulario para comunicar el estado de gobernanza de IA al Board.

| # | CARACTER√çSTICA                 | C√ìMO SE MIDE / CONTROLA                                                                                                                                                                                        | FASE DEL PROYECTO QUE LO TRABAJA                                                                 |
| - | ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |
| 1 | Valid & Reliable               | El sistema hace lo que se declara que hace, de forma consistente, en el rango de condiciones esperadas. Para LLMs: evaluaci√≥n de accuracy en tareas declaradas, testing de edge cases, benchmarking peri√≥dico. | F13 (Red Teaming ‚Äî MEASURE), F16 (Explainability)                                                |
| 2 | Safe                           | El sistema no genera outputs que causan da√±o f√≠sico, psicol√≥gico, o financiero a los usuarios o terceros. Para Copilot: guardrails de contenido de Azure AI Content Safety, DLP policies de Purview.           | F7 (Post-Implementation Governance), F13 (Red Teaming)                                           |
| 3 | Secure & Resilient             | El sistema es resistente a ataques adversariales y mantiene su funci√≥n bajo condiciones adversas. Para LLMs: resistencia a prompt injection, jailbreaking, y ataques de exfiltraci√≥n.                          | F10 (CISO AI Defense), F13 (Red Teaming ‚Äî todos los tipos de ataque)                             |
| 4 | Accountable & Transparent      | Existe accountability clara sobre qui√©n es responsable del sistema y sus outputs. Los usuarios y afectados tienen informaci√≥n sobre el funcionamiento del sistema.                                             | F3 (Transparency), F7 (Governance), F15 (DPIA, Audit Log Purview), F18 (AI Governance Committee) |
| 5 | Explainable & Interpretable    | Los outputs del sistema pueden ser explicados en t√©rminos comprensibles para los usuarios y auditores. Para decisiones que afectan personas: se puede dar raz√≥n de por qu√© el sistema dio ese output.          | F16 (Explainability ‚Äî LIME, SHAP, Azure RAI Dashboard)                                           |
| 6 | Privacy-Enhanced               | El sistema protege la privacidad de los individuos cuyos datos procesa, consistente con regulaciones aplicables y las expectativas de los usuarios.                                                            | F15 (Privacy Engineering), GDPR Art. 25 implementado                                             |
| 7 | Fair with Harmful Bias Managed | El sistema no genera outputs que discriminan grupos protegidos. Los sesgos en los datos de entrenamiento o en el dise√±o son identificados, evaluados, y mitigados.                                             | F16 (Bias Evaluation ‚Äî Fairlearn, Azure RAI Dashboard), F4 (Ethical Dilemmas)                    |

<br>

## 18.3 NIST AI 600-1 ‚Äî EL PERFIL PARA IA GENERATIVA

<br>

NIST AI 600-1, publicado en julio de 2024, es el perfil del AI RMF espec√≠ficamente dise√±ado para IA Generativa ‚Äî el m√°s relevante para el CISO que gestiona M365 Copilot, agentes, y aplicaciones LLM. El documento fue desarrollado en respuesta al Executive Order 14110 del presidente Biden sobre IA segura y confiable, y representa la extensi√≥n del AI RMF al contexto espec√≠fico de los LLMs, sistemas RAG, y agentes aut√≥nomos.

<br>

| RIESGO √öNICO O EXACERBADO POR GAI    | DESCRIPCI√ìN SEG√öN AI 600-1                                                                                                                                                | ACCIONES SUGERIDAS EN CONTEXTO M365                                                                                                                                                                                                                                       |
| ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| CBRN Information                     | GAI puede generar informaci√≥n detallada sobre armas qu√≠micas, biol√≥gicas, radiol√≥gicas, nucleares que facilitar√≠a da√±os catastr√≥ficos.                                    | Guardrails de Azure AI Content Safety para categor√≠as CBRN. Pol√≠tica de uso aceptable expl√≠cita. Monitoreo de prompts que buscan informaci√≥n de este tipo.                                                                                                                |
| Confabulation (Hallucination)        | GAI genera informaci√≥n plausible pero factualmente incorrecta presentada con alto grado de confianza. Especialmente peligroso en contextos legales, m√©dicos, financieros. | Human-in-the-loop para outputs de alto impacto. Citaci√≥n de fuentes verificables cuando el sistema RAG las tiene disponibles. Entrenamiento de usuarios sobre limitaciones. MEASURE: evaluaci√≥n peri√≥dica de tasa de hallucination.                                       |
| Data Privacy                         | GAI puede exponer datos personales de individuos presentes en los datos de entrenamiento, o inferir informaci√≥n privada de las interacciones del usuario.                 | Implementaci√≥n completa de F15 (Privacy Engineering): DP en training, DLP en Copilot, DPIA para sistemas de alto riesgo, Data Minimization en RAG.                                                                                                                        |
| Data Provenance                      | La dificultad para trazar el origen y confiabilidad de los datos que entrenaron el modelo o que el RAG usa como contexto.                                                 | AIBOM (AI Bill of Materials) para modelos propios. Para el corpus RAG: metadatos de fuente y fecha de cada documento. Purview Information Protection para labels de clasificaci√≥n de datos fuente.                                                                        |
| Harmful Bias & Homogenization        | GAI amplifica sesgos en los datos de entrenamiento. La homogenizaci√≥n de perspectivas cuando muchos usuarios usan el mismo modelo reduce la diversidad de pensamiento.    | Azure Responsible AI Dashboard: evaluaci√≥n de fairness por grupos demogr√°ficos para modelos propios. Para Copilot: awareness de que el modelo tiene perspectivas inherentes y puede reflejar sesgos.                                                                      |
| Human-AI Configuration               | Excesiva confianza de los usuarios en los outputs de GAI ('automation bias'). Insuficiente entrenamiento sobre cu√°ndo y c√≥mo aplicar juicio humano.                       | Programa de capacitaci√≥n (F18 ¬ß18.6): educar a los 20K usuarios en qu√© verificar manualmente, en qu√© contextos no delegar a Copilot. KPI: reducci√≥n de incidentes de misinformation reportados.                                                                           |
| Information Integrity                | GAI puede ser usado para generar y diseminar desinformaci√≥n a escala (deepfakes, fake news, synthetic content).                                                           | Pol√≠tica de uso aceptable que proh√≠be el uso de Copilot para generar contenido que pueda inducir a error. C2PA (Content Authenticity Initiative) para contenido generado con IA cuando se publica externamente.                                                           |
| Intellectual Property                | GAI puede reproducir material con copyright de los datos de entrenamiento, exponiendo a la organizaci√≥n a reclamos de IP.                                                 | Microsoft Copilot Copyright Commitment (ver F6 an√°lisis de contratos): Microsoft indemniza a clientes enterprise por reclamos de copyright derivados de Copilot cuando se siguen las gu√≠as de uso. Pol√≠tica interna: no solicitar a Copilot reproducir obras espec√≠ficas. |
| Prompt Injection                     | Ataques que manipulan los inputs del GAI para modificar su comportamiento, filtrar informaci√≥n, o ejecutar acciones no autorizadas.                                       | Implementaci√≥n completa de F13: controles de prompt injection, red teaming peri√≥dico, monitoring de intentos de ataque en producci√≥n (Microsoft Defender for Cloud).                                                                                                      |
| Obscene, Degrading & Harmful Content | GAI puede generar contenido sexualmente expl√≠cito, violento, o da√±ino, especialmente cuando los guardrails son eludidos.                                                  | Azure AI Content Safety: filtros de contenido configurados. Red teaming peri√≥dico para verificar que los filtros no son bypasseados (FuzzyAI, PyRIT).                                                                                                                     |
| Value Chain & Component Integration  | Los sistemas GAI se construyen sobre componentes de terceros: modelos base, plugins, APIs. Cada componente agrega riesgos al sistema final.                               | Supply chain security para IA: due diligence de proveedores (F5, F6), AIBOM, auditor√≠a de plugins de Copilot Studio antes de deployment, verificaci√≥n de integridad de modelos.                                                                                           |

<br>

## 18.4 MAPPING CRUZADO: ISO 42001 √ó NIST AI RMF √ó EU AI ACT √ó GDPR

<br>

El CISO que gestiona un ecosistema de IA enterprise opera bajo m√∫ltiples marcos de referencia simult√°neamente: ISO 42001 para el sistema de gesti√≥n, NIST AI RMF para el lenguaje de riesgo, EU AI Act para las obligaciones legales (si opera con ciudadanos de la UE), y GDPR para los datos personales. El reto es evitar la duplicaci√≥n: el mismo control puede satisfacer requerimientos de m√∫ltiples frameworks. Esta secci√≥n mapea los dominios clave para maximizar la eficiencia del programa.

<br>

| DOMINIO DE GOBERNANZA                        | ISO 42001 CL√ÅUSULA / CONTROL                                    | NIST AI RMF FUNCI√ìN                    | EU AI ACT ART√çCULO                                                                     | GDPR / LEY 25.326 ART√çCULO                                                                            |
| -------------------------------------------- | --------------------------------------------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| AI Policy y Gobernanza                       | Cl√°usula 5 (Leadership), Cl√°usula 6 (Planning), A.5.2 AI Policy | GOVERN 1.1-1.4, GOVERN 2.1-2.2         | Art. 17 (Governance Quality Management ‚Äî alto riesgo), Art. 9 (Risk Management System) | No directo (complementario a GDPR Art. 24 ‚Äî Responsabilidad del Responsable)                          |
| Inventario y Clasificaci√≥n de Sistemas de IA | Cl√°usula 4.1 (Context), A.5.4 AI Roles                          | MAP 1.1-1.6 (AI System Categorization) | Art. 6 (Clasificaci√≥n riesgo), Art. 51 (Base de datos EU de IA alto riesgo)            | No directo                                                                                            |
| Gesti√≥n de Riesgos de IA                     | Cl√°usula 6.1-6.2, A.6.1 AI Risk Assessment                      | MAP 5.1-5.2, MEASURE 2.1-2.6           | Art. 9 (Risk Management System ‚Äî alto riesgo), Art. 14 (Human Oversight)               | Art. 25 GDPR (PbD), Art. 35 (DPIA ‚Äî si datos personales)                                              |
| Testing, Evaluaci√≥n y Validaci√≥n             | Cl√°usula 8.1-8.4, A.8.4 AI System Lifecycle                     | MEASURE 2.5-2.9 (TEVV)                 | Art. 9(6) (Testing relevante y representativo), Art. 15 (Logging ‚Äî alto riesgo)        | No directo (complementario Art. 25 GDPR)                                                              |
| Privacidad y Datos Personales                | Cl√°usula 8.3 A.8.3 (Data for AI)                                | GOVERN 4.1, MEASURE 2.3 (Data Quality) | Art. 10 (Data Governance ‚Äî alto riesgo)                                                | GDPR Art. 5 (principios), Art. 25 (PbD), Art. 35 (DPIA), Art. 17 (olvido) / Ley 25.326 Art. 4, 16, 43 |
| Transparencia y Explicabilidad               | A.9.7 Transparency, A.10.5 AI Accuracy                          | MEASURE 2.10-2.11, GOVERN 5.1-5.2      | Art. 13 (Transparency), Art. 50 (Obligaciones de transparencia GenAI)                  | GDPR Art. 13-14 (info al interesado), Art. 22 (decisiones automatizadas)                              |
| Supervisi√≥n Humana                           | A.10.3 Human Oversight                                          | MANAGE 4.1 (Appeal/Override)           | Art. 14 (Human Oversight ‚Äî alto riesgo)                                                | GDPR Art. 22 (derecho a no ser sujeto de decisi√≥n autom√°tica exclusiva)                               |
| Incidentes de IA                             | A.9.3 AI Incident Management                                    | MANAGE 2.1-2.4 (Incident Response)     | Art. 73 (Notificaci√≥n incidentes graves ‚Äî sistemas de alto riesgo)                     | GDPR Art. 33-34 (notificaci√≥n de brechas de datos personales)                                         |
| Gesti√≥n de Proveedores de IA                 | Cl√°usula 8.6 (AI Providers/Deployers)                           | GOVERN 6.1-6.2 (Third-party Oversight) | Art. 25-27 (Obligaciones del proveedor vs. deployer)                                   | GDPR Art. 28 (contratos con encargados de tratamiento)                                                |
| Mejora Continua                              | Cl√°usula 10 (Improvement), PDCA completo                        | GOVERN 1.4 (Continuous Improvement)    | Art. 9 (Continuous monitoring ‚Äî alto riesgo)                                           | No directo (principio general GDPR)                                                                   |

<br>

üí° La regla de oro del mapping cruzado: Implementar un control una sola vez pero documentarlo como evidencia de m√∫ltiples frameworks. Ejemplo: el DPIA de IA (F15) satisface simult√°neamente ISO 42001 A.6.2 (AI System Impact Assessment), NIST AI RMF MAP 5.1 (Impact Documentation), EU AI Act Art. 9 (Risk Management), y GDPR Art. 35 (DPIA). El AI Governance Committee al aprobarlo est√° dando evidencia para todos los frameworks. Un programa de gobernanza bien dise√±ado no duplica esfuerzo ‚Äî aplica eficiencia cruzada en todos los controles.

<br>

## 18.5 ISO 42001 Y M365 COPILOT: LA CERTIFICACI√ìN DE MICROSOFT

<br>

Microsoft obtuvo certificaci√≥n ISO/IEC 42001 para Microsoft 365 Copilot y Microsoft 365 Copilot Chat ‚Äî validada por un auditor independiente. Esta es una de las certificaciones de IA m√°s significativas del mercado y tiene implicaciones directas para el CISO que despliega Copilot a 20,000+ usuarios. Comprender qu√© cubre y qu√© no cubre la certificaci√≥n de Microsoft es esencial para calibrar correctamente el programa propio de gobernanza.

<br>

| QU√â CUBRE LA CERTIFICACI√ìN ISO 42001 DE MICROSOFT                                                                                                                   | QU√â NO CUBRE (OBLIGACIONES DEL DEPLOYER)                                                                                                                                      |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| La aplicaci√≥n del AI Management System de Microsoft para el desarrollo, deployment y operaci√≥n de M365 Copilot, incluyendo el Responsible AI Standard de Microsoft. | El AI Management System de la organizaci√≥n como deployer: pol√≠ticas internas, roles del AI Governance Committee, AI risk register propio.                                     |
| Los procesos de Microsoft para evaluaci√≥n de riesgos de Copilot, testing adversarial, evaluaci√≥n de bias, y mejora continua del sistema a nivel del proveedor.      | Las evaluaciones de riesgo espec√≠ficas de los casos de uso de la organizaci√≥n: c√≥mo la organizaci√≥n usa Copilot, qu√© datos expone, qu√© agentes construye sobre la plataforma. |
| Los controles t√©cnicos de Microsoft: guardrails de contenido, Zero Data Retention, separaci√≥n de datos por tenant, audit logging.                                   | La configuraci√≥n correcta de esos controles por la organizaci√≥n: sensitiviy labels configurados, DLP policies activas, corpus RAG auditado, permisos de SharePoint correctos. |
| La transparencia de Microsoft sobre las capacidades y limitaciones de Copilot: model cards, responsible AI impact assessments publicados.                           | Los DPIAs espec√≠ficos de la organizaci√≥n para los casos de uso de Copilot que involucran datos personales de empleados/clientes.                                              |
| El sistema de gesti√≥n de Microsoft para incidentes relacionados con M365 Copilot y la notificaci√≥n a clientes seg√∫n los contratos.                                  | El AI Incident Response Plan interno de la organizaci√≥n: c√≥mo responde cuando un empleado usa Copilot de forma que genera un incidente de privacidad o seguridad.             |
| La gesti√≥n de la supply chain de IA de Microsoft: qu√© modelos base usa, c√≥mo valida los componentes, qu√© proveedores participan en M365 Copilot.                    | La auditor√≠a y aprobaci√≥n de los plugins y conectores de terceros que la organizaci√≥n habilita en Copilot Studio para sus agentes.                                            |

<br>

La conclusi√≥n pr√°ctica: la certificaci√≥n ISO 42001 de Microsoft es una ventaja significativa para el programa de gobernanza del CISO ‚Äî reduce el esfuerzo de due diligence sobre el proveedor (Fase 5 del proyecto) y provee evidencia para los auditores. Pero no sustituye el AIMS de la organizaci√≥n deployer. Es equivalente a que el datacenter de Microsoft tenga ISO 27001: protege la capa del proveedor, pero la organizaci√≥n debe igualmente implementar sus propios controles sobre lo que hace con esa plataforma.

<br>

## 18.6 AI GOVERNANCE COMMITTEE: ESTRUCTURA, ROLES Y OPERACI√ìN

<br>

El AI Governance Committee (AGC) es el √≥rgano de gobernanza central del AIMS. ISO 42001 Cl√°usula 5 requiere que la alta direcci√≥n demuestre liderazgo en el AIMS; el AI Governance Committee es la estructura organizacional que materializa ese requerimiento. Para el CISO, el AGC es el foro donde convergen los riesgos t√©cnicos de IA con las decisiones de negocio y las obligaciones legales ‚Äî el nexo entre el Red Teaming de F13, los DPIAs de F15, y la estrategia de IA del negocio.

<br>

### 18.6.1 Composici√≥n del AI Governance Committee

| ROL                              | FUNCI√ìN EN EL AGC                                                                                                                                                    | APORTE ESPEC√çFICO                                                                                                                                              | FRECUENCIA DE PARTICIPACI√ìN                                                                               |
| -------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |
| CEO / C-Level Sponsor            | Patrocinador ejecutivo del AIMS. Aprueba la AI Policy y los objetivos del AIMS. Representaci√≥n del Board.                                                            | Seniority que asegura que el programa tiene recursos y que las decisiones de riesgo residual son tomadas al nivel correcto.                                    | Reuni√≥n de aprobaci√≥n de AI Policy. Revisi√≥n anual del estado del programa (Management Review ISO 42001). |
| CISO                             | Co-chair del AGC. Responsable de la integraci√≥n del AIMS con el programa de seguridad. Gesti√≥n de riesgos t√©cnicos de IA.                                            | Resultados de red teaming, security posture de sistemas de IA, incidents de seguridad relacionados con IA, integration con Defender XDR y Sentinel.            | Reuni√≥n mensual del AGC. Emergency meetings en incidentes de alto impacto.                                |
| DPO (Data Protection Officer)    | Responsable de los aspectos de privacidad del AIMS. Review de DPIAs. Liaison con autoridades supervisoras.                                                           | Status de DPIAs, brechas de compliance GDPR/Ley 25.326, consultas regulatorias, derechos del interesado pendientes sobre datos en sistemas de IA.              | Reuni√≥n mensual. Review de DPIAs ad hoc.                                                                  |
| CTO / Head of Architecture       | Perspectiva t√©cnica: arquitectura de los sistemas de IA, evaluaci√≥n de nuevos modelos y herramientas, viabilidad de controles t√©cnicos.                              | Roadmap tecnol√≥gico de IA, evaluaci√≥n de viabilidad de controles propuestos, technical debt de gobernanza, alignment con Azure AI Foundry roadmap.             | Reuni√≥n mensual. Gate reviews de nuevos deployments.                                                      |
| Legal / Compliance               | An√°lisis regulatorio: EU AI Act, GDPR, Ley 25.326, sector-espec√≠ficos. Revisi√≥n de contratos con proveedores de IA.                                                  | Alertas regulatorias, review de contratos IA (F6), posici√≥n legal ante incidentes, compliance mapping regulaci√≥n ‚Üí control.                                    | Reuni√≥n mensual. Ad hoc para cambios regulatorios significativos.                                         |
| Negocio / Unidades de Valor      | Representaci√≥n de las unidades que usan o construyen sistemas de IA. Contexto de casos de uso, impacto de controles sobre la productividad.                          | Use cases nuevos que requieren evaluaci√≥n de riesgo (MAP), feedback sobre efectividad de controles desde perspectiva usuario, business risk prioritization.    | Reuni√≥n mensual o bimensual seg√∫n carga de trabajo del AGC.                                               |
| RRHH                             | Impacto de sistemas de IA en empleados: Copilot an√°lisis de rendimiento, automatizaci√≥n de procesos HR, implicaciones laborales.                                     | Evaluaci√≥n de impacto en empleados (A.6.2 ISO 42001), comunicaci√≥n a empleados sobre uso de IA, pol√≠tica de uso aceptable desde perspectiva laboral.           | Trimestral o cuando hay cambios que afectan a los empleados.                                              |
| AI Risk Manager (nuevo rol 2025) | Rol operacional dedicado: coordina el AIMS en el d√≠a a d√≠a, mantiene el inventario de sistemas de IA, gestiona el risk register de IA, prepara reportes para el AGC. | Operaci√≥n diaria del AIMS: mantener AI inventory, coordinar red teamings, hacer seguimiento de remediation plans, preparar Risk Scorecard mensual para el AGC. | Full-time dedicado. Reporta directamente al CISO.                                                         |

<br>

### 18.6.2 Agenda Tipo del AGC Mensual

| SLOT   | AGENDA ITEM                                            | CONTENIDO                                                                                                                                                                                                                                                                                  | RESPONSABLE           |
| ------ | ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------- |
| 5 min  | Apertura y aprobaci√≥n de agenda                        | Aprobaci√≥n del acta anterior. Ajustes a la agenda si hay urgencias.                                                                                                                                                                                                                        | AGC Chair (CISO)      |
| 15 min | Risk Scorecard de IA del mes                           | Dashboard: ASR por sistema (F13 m√©tricas), estado de DPIAs (F15), coverage de Sensitivity Labels, incidentes de IA reportados, status de remediation items previos.                                                                                                                        | AI Risk Manager       |
| 20 min | Evaluaci√≥n de nuevos sistemas o cambios significativos | Para cada nuevo agente de Copilot Studio, app Azure AI Foundry, o cambio mayor en sistemas existentes: presentaci√≥n de AI System Assessment (scope, datos, herramientas, riesgos identificados, controles propuestos). Decisi√≥n: aprobar para piloto / aprobar con condiciones / rechazar. | AI Risk Manager + CTO |
| 15 min | Alertas regulatorias y de compliance                   | Novedades del EU AI Act, GDPR, Ley 25.326, nuevas gu√≠as de autoridades supervisoras. Impacto en el programa. Acciones requeridas.                                                                                                                                                          | Legal / Compliance    |
| 10 min | Estado de Red Teaming y controles de seguridad         | Resultados de ejercicios de red teaming completados o planificados. Nuevos vectores de ataque publicados en MITRE ATLAS u OWASP. Estado de remediaciones pendientes.                                                                                                                       | CISO                  |
| 10 min | Incidentes de IA del per√≠odo                           | Review de incidentes relacionados con sistemas de IA: prompts con PII detectados por DLP, intentos de jailbreaking, misinformation reportada por usuarios, otros. Post-mortems y lecciones aprendidas.                                                                                     | CISO + DPO            |
| 5 min  | AOB (Any Other Business) y pr√≥ximos pasos              | Items adicionales. Asignaci√≥n de action items con responsables y fechas. Confirmaci√≥n de pr√≥xima reuni√≥n.                                                                                                                                                                                  | AGC Chair             |

<br>

## 18.7 AIMS EN LA PR√ÅCTICA: GAP ANALYSIS Y HOJA DE RUTA PARA EL CISO (2026-2027)

<br>

La implementaci√≥n de un AIMS bajo ISO 42001 y NIST AI RMF no comienza desde cero ‚Äî el CISO con un programa de seguridad maduro (ISO 27001, GDPR compliance, SOC) ya tiene el 50-60% de la infraestructura de gesti√≥n que el AIMS requiere. El gap analysis identifica espec√≠ficamente qu√© falta para los requerimientos espec√≠ficos de IA.

<br>

### 18.7.1 Gap Analysis R√°pido: Estado de Madurez AIMS vs. ISO 42001

| REQUERIMIENTO ISO 42001              | QU√â TIENE UNA ORG CON ISO 27001 + GDPR                                      | GAP ESPEC√çFICO DE IA                                                                                                                                       | ACCI√ìN PARA CERRAR EL GAP                                                                                                                                                                                                           |
| ------------------------------------ | --------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| AI Policy (A.5.2)                    | Pol√≠tica de Information Security (ISMS), Pol√≠tica de Privacidad.            | No hay una AI Policy espec√≠fica que defina gobernanza de sistemas de IA.                                                                                   | Redactar AI Policy: scope (sistemas de IA autorizados), principios, roles del AGC, criterios de aprobaci√≥n de nuevos sistemas. Aprobaci√≥n por Board.                                                                                |
| AI Roles (A.5.4)                     | RACI de seguridad, roles de DPO, roles de IT.                               | No hay roles espec√≠ficos para AI provider/deployer/user. No hay AI Risk Manager.                                                                           | Definir roles ISO 42001: la organizaci√≥n como AI deployer, Microsoft como AI provider, roles internos del AGC. Crear rol AI Risk Manager si el volumen lo justifica.                                                                |
| AI Risk Assessment (A.6.1)           | Risk register de TI gen√©rico con categor√≠as de riesgo convencionales.       | El risk register no incluye categor√≠as espec√≠ficas de IA: memorizaci√≥n, prompt injection, excessive agency, misinformation, sesgo.                         | Extender el risk register con categor√≠as AI: usar OWASP LLM Top 10 y NIST AI 600-1 como taxonom√≠a. Completar para cada sistema en scope.                                                                                            |
| AI System Impact Assessment (A.6.2)  | DPIA para sistemas que procesan datos personales.                           | El DPIA cubre privacidad pero no todos los impactos de IA: impacto en empleados, riesgo de misinformation, riesgo de sesgo que afecta grupos.              | Ampliar el scope del DPIA o crear un AI System Impact Assessment paralelo que cubra impactos no-privacidad: employees impact, fairness, misinformation risk.                                                                        |
| AI Inventory (MAP NIST)              | IT Asset Inventory con todos los sistemas de TI.                            | El AI inventory requiere campos espec√≠ficos: modelo base, capacidades, herramientas de agente, corpus de datos, last red team date.                        | Crear AI inventory espec√≠fico como extensi√≥n del IT asset inventory. Completarlo con los campos AI-espec√≠ficos. Mantenimiento: el AI Risk Manager actualiza con cada nuevo deployment.                                              |
| Data for AI (A.8.3)                  | Data governance program, clasificaci√≥n de datos.                            | La gobernanza de datos no cubre espec√≠ficamente el corpus RAG, embeddings, datos de fine-tuning.                                                           | Extender el programa de gobernanza de datos para cubrir: corpus RAG (qu√© entra, qu√© se excluye, pol√≠tica de retenci√≥n), fine-tuning datasets (AIBOM), vector database access control.                                               |
| TEVV - Testing (MEASURE NIST)        | Vulnerability assessment, penetration testing para sistemas convencionales. | No hay red teaming espec√≠fico para IA. El pen testing convencional no cubre prompt injection, jailbreaking, o ataques RAG.                                 | Implementar F13 (Red Teaming de IA): PyRIT, proceso de 6 fases, plan trimestral de testing. Integrar resultados como evidencia de MEASURE en el AIMS.                                                                               |
| AI Incident Management (A.9.3)       | IR plan gen√©rico de TI.                                                     | El IR plan no incluye categor√≠as espec√≠ficas de incidentes de IA: prompt injection exitosa, exfiltraci√≥n v√≠a LLM, misinformation cr√≠tica, sesgo reportado. | Ampliar el AI Incident Response Plan de F7 para incluir categor√≠as AI-espec√≠ficas. Integrar alertas de Microsoft Defender for Cloud AI detection. Definir threshold de notificaci√≥n regulatoria para incidentes de IA.              |
| Human Oversight (A.10.3)             | No hay equivalente directo en ISO 27001.                                    | Sistemas de IA no tienen mecanismos formales de override humano. Agentes de Copilot Studio pueden ejecutar acciones sin aprobaci√≥n humana.                 | Implementar human-in-the-loop para acciones de alto impacto (F13 ¬ß13.4). Kill switch documentado para todos los agentes. Policy: qu√© tipos de decisiones nunca pueden ser tomadas exclusivamente por un sistema de IA.              |
| Mejora Continua - PDCA (Cl√°usula 10) | PDCA en el ISMS de ISO 27001.                                               | No hay ciclo PDCA documentado espec√≠fico para el AIMS de IA.                                                                                               | Establecer ciclo PDCA del AIMS: Plan (risk assessment semestral), Do (implementar controles del plan), Check (internal audit anual del AIMS, management review semestral en el AGC), Act (corrective actions, update de AI Policy). |

<br>

### 18.7.2 Hoja de Ruta de Implementaci√≥n del AIMS ‚Äî 3 Horizontes

| HORIZONTE                                        | OBJETIVO Y HITOS PRINCIPALES                                                                                                                                                                                                                                                                        | ACCIONES ESPEC√çFICAS Y CRITERIOS DE √âXITO                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| ------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| H1 ‚Äî Fundaciones(0-90 d√≠as)                      | Establecer las bases del AIMS: AI Policy aprobada, AI Governance Committee constituido y operativo, AI Inventory completo, gap analysis documentado. Al final de H1 la organizaci√≥n tiene visibilidad y estructura ‚Äî a√∫n no tiene todos los controles.                                              | Acci√≥n 1: Redactar AI Policy y obtener aprobaci√≥n del Board. Acci√≥n 2: Constituir el AGC con los roles definidos en ¬ß18.6. Acci√≥n 3: Completar AI Inventory para todos los sistemas de IA en scope con campos: sistema, proveedor, modelo base, datos accesibles, herramientas de agente, prop√≥sito, usuarios, last risk assessment date. Acci√≥n 4: Ejecutar gap analysis ISO 42001 (tabla ¬ß18.7.1). Acci√≥n 5: Priorizar los 5 gaps m√°s cr√≠ticos. Criterio de √©xito: AI Policy firmada, AGC activo (primera reuni√≥n celebrada), AI Inventory completo, Gap Analysis Report aprobado por el AGC.                                                                                                                                                                                                                          |
| H2 ‚Äî Controles Operacionales(90 d√≠as - 12 meses) | Cerrar los gaps de mayor riesgo: AI risk register completo, TEVV/red teaming sistem√°tico, DPIAs completados, human oversight mechanisms implementados. Al final de H2 la organizaci√≥n tiene el AIMS operacional para los sistemas de mayor riesgo.                                                  | Acci√≥n 1: Completar AI risk register con categor√≠as OWASP LLM Top 10 para todos los sistemas del inventario. Acci√≥n 2: Ejecutar primer ciclo completo de red teaming (F13): baseline para M365 Copilot y agentes Copilot Studio. Acci√≥n 3: Completar DPIAs para todos los sistemas que procesan datos personales de alto riesgo. Acci√≥n 4: Implementar human-in-the-loop para agentes con write access. Acci√≥n 5: Configurar Purview DSPM for AI, DLP en Copilot, Sensitivity Labels >90% coverage. Acci√≥n 6: Establecer programa de capacitaci√≥n para usuarios Copilot. Criterio de √©xito: AI risk register activo, 2 ciclos de red teaming completados, DPIAs firmados por DPO, controles Purview operativos, >70% de usuarios Copilot capacitados.                                                                    |
| H3 ‚Äî Madurez y Certificaci√≥n(12-24 meses)        | Alcanzar la madurez del AIMS: ciclo PDCA completo documentado, internal audit del AIMS, management review semestral en el AGC. Evaluaci√≥n de certificaci√≥n ISO 42001 si los auditores de reguladores o clientes lo requieren. Al final de H3 el CISO puede demostrar gobernanza certificable de IA. | Acci√≥n 1: Completar primer internal audit del AIMS ‚Äî evaluaci√≥n clause-by-clause de ISO 42001. Acci√≥n 2: Primer management review formal del AGC: estado del programa, risk scorecard anual, objetivos del a√±o siguiente, corrective actions. Acci√≥n 3: Statement of Applicability (SoA) completado: para cada uno de los 38 controles del Annex A, declarar si aplica o no, justificaci√≥n, y estado de implementaci√≥n. Acci√≥n 4: Si se busca certificaci√≥n: contratar conformity assessment body (CAB) acreditado para pre-assessment. Certificaci√≥n formal si el mercado o los reguladores lo requieren. Criterio de √©xito: AIMS documentado auditables, internal audit completado, SoA aprobado, management review documentado. Si certificaci√≥n: audit report de tercero disponible en Service Trust Portal interno. |

<br>

üìä El ROI de la certificaci√≥n ISO 42001: En 2024, el n√∫mero de organizaciones con certificaci√≥n ISO aument√≥ un 20% globalmente vs. 2023. ISO 42001 en particular tiene tasa de adopci√≥n acelerada conforme el EU AI Act escala sus obligaciones. Para el CISO con operaciones en el mercado financiero o de servicios de Argentina y LATAM: la certificaci√≥n ISO 42001 es una ventaja diferencial en licitaciones, RFPs, y auditor√≠as de clientes enterprise. El retorno no es solo regulatorio ‚Äî es competitivo. El costo estimado de implementaci√≥n + certificaci√≥n: 200-400 horas de personal interno + 30,000-60,000 USD honorarios de auditor externo. El ROI comparado con el costo de un incidente de gobernanza de IA mal manejado es sistem√°ticamente positivo.

<br>

## 18.8 REFERENCIAS (R421‚ÄìR450)

<br>

ISO/IEC 42001 ‚Äî Est√°ndar y Documentaci√≥n Oficial:

R421. ISO, 'ISO/IEC 42001:2023 ‚Äî Artificial Intelligence Management Systems', diciembre 2023. Primer est√°ndar internacional de sistemas de gesti√≥n de IA. Estructura HLS compatible con ISO 27001. 10 cl√°usulas, 38 controles en Annex A. Plan-Do-Check-Act. Publicado por ISO y la Comisi√≥n Electrot√©cnica Internacional. iso.org/standard/42001

R422. ISO, 'ISO/IEC 42005:2025 ‚Äî AI Systems Impact Assessment', 2025. Companion standard de ISO 42001. Gu√≠a espec√≠fica para AI System Impact Assessment (Control A.6.2). Complemento directo al DPIA de GDPR para impactos no-privacidad de sistemas de IA.

R423. ISO, 'ISO/IEC 23894:2023 ‚Äî Artificial Intelligence ‚Äî Guidance on Risk Management', 2023. Gesti√≥n de riesgos espec√≠ficos de IA. Referencia normativa de ISO 42001 (Clause 2). Marco de riesgo IA compatible con ISO 31000.

R424. EY, 'ISO 42001: Paving the Way for Ethical AI', diciembre 2025. 38 controles del Annex A. Comparable a ISO 27001 en estructura. Requiere: AI risk assessment, impact evaluation, lifecycle management, 3rd-party supplier oversight. PDCA methodology.

R425. KPMG Switzerland, 'ISO/IEC 42001: A New Standard for AI Governance', agosto 2025. Beneficios: stakeholder trust, enhanced AI governance. 20% crecimiento en certificaciones ISO en 2024 vs. 2023. Certificaci√≥n v√°lida 3 a√±os con auditor√≠as anuales de vigilancia.

R426. Lasso Security, 'ISO/IEC 42001:2023 Features, Types & Best Practices', octubre 2025. Certificaci√≥n full ISO 42001: CAB (Conformity Assessment Body) acreditado, audit de todas las cl√°usulas y anexos. Diferencia con SOC2, ISO 27001. Pre-certification assessment recomendado antes del audit formal.

R427. Prompt Security, 'Understanding the ISO/IEC 42001 for AI Management Systems', septiembre 2025. ISO 42001 facilita compliance EU AI Act. Vanta CISO prediction 2024: enterprises focalizar√°n AI governance en 2025. Aligns con ISO 27001, ISO 27701, NIST AI RMF.

R428. Mindgard, 'ISO/IEC 42001 Explained: AI Management System Standard Guide', octubre 2025. ISO 42001 primera norma auditable para AIMS. Cierra brecha entre pol√≠ticas impl√≠citas y sistema de gesti√≥n certificable. Discovered Azure AI Content Safety vulnerabilities (Feb 2024) como ejemplo del valor del standard.

R429. ISACA, 'ISO/IEC 42001 and EU AI Act: A Practical Pairing for AI Governance', diciembre 2025. EU AI Act = el reglamento; ISO 42001 = el sistema operativo que hace el compliance repetible y auditable. AI Act: GPAI obligations desde agosto 2025, enforcement agosto 2026. Fines: EUR 35M o 7% revenue. Traceability: 'Article 43 requirement ‚Üí Test Y ‚Üí Evidence Z'.

R430. RSI Security, 'AI Management System Implementation Guide ‚Äî ISO 42001', noviembre 2025. Roadmap 6 fases. Gap analysis clause-by-clause vs. ISO 27001 existente. Statement of Applicability para los 38 controles. Integraci√≥n: ISO 27001 + NIST AI RMF + GDPR reduce redundancias.

<br>

NIST AI RMF ‚Äî Framework y Documentaci√≥n Oficial:

R431. NIST, 'AI 100-1: Artificial Intelligence Risk Management Framework (AI RMF 1.0)', enero 2023. nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf. Cuatro funciones: GOVERN, MAP, MEASURE, MANAGE. Siete caracter√≠sticas de IA trustworthy. Reconoce AI como socio-t√©cnico. Voluntario y sector-agn√≥stico. Appendices: AI actor tasks, AI risks vs. traditional software (data drift, emergent behavior, privacy amplification).

R432. NIST, 'AI 100-1 Playbook', actualizaci√≥n semi-anual. airc.nist.gov/airmf-resources/playbook. Suggested actions para cada subcategor√≠a de las 4 funciones. Templates de implementation. Voluntary. Semi-annual updates based on community feedback.

R433. NIST, 'AI 600-1: Artificial Intelligence Risk Management Framework ‚Äî Generative AI Profile', julio 2024. nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf. Perfil cross-sectoral del AI RMF espec√≠fico para GAI. 11 riesgos √∫nicos o exacerbados por GAI: confabulation, data privacy, data provenance, harmful bias, human-AI configuration, IP, prompt injection, CBRN information, obscene content, information integrity, value chain. Suggested actions para GOVERN/MAP/MEASURE/MANAGE por cada riesgo.

R434. AIGL Blog, 'NIST AI 100-1: Comprehensive Summary and Analysis', noviembre 2025. GOVERN 1.1 (legal requirements documented), MAP 5.1 (likelihood/magnitude of impacts), MEASURE 2.11 (fairness/bias evaluation), MANAGE 4.1 (post-deployment monitoring, appeal/override, decommissioning). Profiles: Current vs. Target para gap analysis. AI RMF natural benchmark para compliance regulatorio y auditor√≠as externas.

R435. Elevate Consult, 'NIST AI Risk Management Framework: A Builder's Roadmap', diciembre 2025. MANAGE function: resource allocation based on GOVERN definitions. Response/recovery plans para incidentes de IA. AI Inventory con fields: model purpose, data sources, risk exposure, integration points, deployment environments, human-in-the-loop expectations. EU AI Act ‚Üí perfil High-Risk del AI RMF.

R436. Nemko Digital, 'AI Risk Mitigation & NIST RMF Process: Understanding NIST 2025', 2025. Governance structure 2025: de AI ethics discussions informales a estructuras formales con executive sponsorship. AI lifecycle governance: concept, design, data acquisition, training, testing, deployment, monitoring, retirement. Profiles para benchmarking de madurez y conformity assessments de EU AI Act.

R437. RSI Security, 'Roadmap to Achieving NIST AI RMF', diciembre 2025. GOVERN function: documented roles, training & capability building. GOVERN 2.1 (roles communication), GOVERN 2.2 (AI risk training). Integration AI RMF con ISO 42001, SOC2 AI controls, EU AI Act, internal risk committees.

<br>

Microsoft ISO 42001 y Gobernanza de IA:

R438. Microsoft Learn, 'ISO/IEC 42001:2023 Artificial Intelligence Management System Standards', 2025. learn.microsoft.com/en-us/compliance/regulatory/offering-iso-42001. Microsoft certificado ISO 42001 para M365 Copilot y Copilot Chat. Tercero independiente valid√≥ el framework de gesti√≥n de riesgos. Disponible en Service Trust Portal. Organizaciones pueden usar la certificaci√≥n de Microsoft en su propio compliance assessment, manteniendo sus propias obligaciones como deployers.

R439. Microsoft, 'Responsible AI Standard v2', junio 2022 (actualizado). El est√°ndar interno de Microsoft que es auditado para ISO 42001. Seis principios: Fairness, Reliability & Safety, Privacy & Security, Inclusiveness, Transparency, Accountability. Proceso de review de impacto para nuevos sistemas de IA. Disponible p√∫blicamente en microsoft.com/responsible-ai.

R440. Microsoft Learn, 'Microsoft AI Principles and Approach', 2025. Commitment: responsible AI as core to Microsoft's AI strategy. Azure AI Content Safety, Azure Responsible AI Dashboard, Fairlearn, Presidio, PyRIT: el stack open-source de herramientas de IA responsable de Microsoft.

R441. Vanta, 'Jadee Hanson CISO Prediction 2025: AI Governance Focus', diciembre 2024. Predicci√≥n verificada en 2025: enterprises enfocaron fuertemente en AI governance con √©nfasis en √©tica y calidad de datos de entrenamiento. ISO 42001 como veh√≠culo de certificaci√≥n y diferenciaci√≥n competitiva.

<br>

Gobernanza de IA ‚Äî Frameworks Complementarios:

R442. OECD, 'OECD AI Principles', 2019 (actualizado 2024). Los 5 principios OECD: inclusive growth, human-centered values, transparency, robustness, accountability. Adoptados por 46 pa√≠ses incluyendo Argentina. Base de referencia del AI RMF de NIST (Appendix A). oecd.org/ai/principles.

R443. EU AI Office, 'General-Purpose AI Code of Practice ‚Äî Draft 3', octubre 2025. GPAI provider obligations desde agosto 2025. Transparency (Art. 53): copyright and training data summaries. Systemic risk (Art. 55): reporting, cybersecurity. ISO 42001 como 'operating system' para demostrar conformity. Fines GPAI: EUR 15M o 3% global revenue.

R444. Elevateconsult, 'EU AI Code of Practice: 2025 Guide + ISO 42001 Map', noviembre 2025. Mapping EU AI Code of Practice ‚Üí ISO 42001 clauses. Leadership & Policy: ISO clauses on Context, Leadership, Planning. Risk & Impact: risk registers + impact assessments + red team links. Lifecycle Controls: training ‚Üí eval ‚Üí deployment gates. Documentation & Evidence: policy set, model card, training data summary, evaluation reports.

R445. UNESCO, 'Recommendation on the Ethics of Artificial Intelligence', noviembre 2021. La recomendaci√≥n global (193 estados miembros) sobre √©tica de IA. Principios: proportionality, safety, fairness, sustainability, privacy, multi-stakeholder governance, transparency, responsibility. Referencia para organizaciones con operaciones en m√∫ltiples jurisdicciones incluyendo Argentina.

R446. ISACA, 'AI Risk Framework and Governance Guide 2025'. Integraci√≥n del AI governance con enterprise risk management (ERM). AI governance como extensi√≥n del GRC (Governance, Risk, Compliance) existente. ROI del AI governance: reducci√≥n de incidentes, menor costo de remediaci√≥n, ventaja competitiva en licitaciones.

R447. Elevate Consult, 'ISO 42001 + NIST AI RMF Integration Guide', 2025. Mapping ISO 42001 √ó NIST AI RMF: complementariedad sin duplicaci√≥n. ISO 42001 Cl√°usula 6 ‚Üî NIST AI RMF MAP function. ISO 42001 Cl√°usula 8 ‚Üî NIST MEASURE + MANAGE. ISO 42001 Cl√°usula 9-10 ‚Üî NIST GOVERN (continuous improvement).

R448. ISACA, 'AI Governance Committee Structure and Best Practices', 2025. Composici√≥n recomendada: CEO sponsor, CISO, DPO, CTO, Legal, Negocio, RRHH. Nuevo rol emergente: AI Risk Manager / Chief AI Officer. Frecuencia: mensual para comit√© operacional, semestral para management review ISO 42001.

R449. Gartner, 'AI Governance Survey 2025'. Solo 51% de organizaciones tiene un framework de gobernanza de datos establecido ‚Äî prerequisito del AIMS. El 95% de l√≠deres de seguridad reconoce que unificar data security, compliance y privacidad es prioridad y desaf√≠o. Vendor consolidation strategy en 75% de organizaciones.

R450. Argentina, 'Propuesta Marco Regulatorio Nacional de IA', 2025 (en consulta). Proceso de definici√≥n del marco regulatorio nacional de IA en Argentina. Influenciado por EU AI Act y OECD AI Principles. Implicaciones para organizaciones con 20K+ empleados en Argentina que usan sistemas de IA para procesos que afectan a los trabajadores.

<br>

