---
icon: check
---

# F14 - 14.0 INTRODUCCI√ìN: POR QU√â LA CADENA DE SUMINISTRO DE IA ES EL NUEVO BLIND SPOT

14.0  Introducci√≥n: Por Qu√© la Cadena de Suministro de IA es el Nuevo Blind Spot

14.1  El AI Bill of Materials (AIBOM): Visibilidad como Prerequisito de Seguridad

14.2  Riesgos de Modelos Open Source: Hugging Face, Pickle Attacks y Namespace Hijacking

14.3  Shadow AI: El Riesgo Invisible Dentro de la Organizaci√≥n

14.4  MCP Server Security: El Nuevo Riesgo de Supply Chain Ag√©ntico

14.5  LLMjacking y Robo de Credenciales de IA

14.6  Evaluaci√≥n de Terceros en el Ecosistema M365: Plugins, Conectores y RAG

14.7  Plan de Acci√≥n para CISO: Programa de Supply Chain Security de IA

14.8  M√©tricas e Integraci√≥n con el AI Governance Framework

14.9  Referencias (R361‚ÄìR390)

<br>

## 14.0 INTRODUCCI√ìN: POR QU√â LA CADENA DE SUMINISTRO DE IA ES EL NUEVO BLIND SPOT

<br>

La cadena de suministro de software fue el blind spot cr√≠tico de la seguridad empresarial hasta que el incidente SolarWinds en 2020 demostr√≥ que un solo componente comprometido pod√≠a afectar a decenas de miles de organizaciones simult√°neamente. La industria respondi√≥ con SBOMs obligatorios (Executive Order 14028, 2021) y frameworks de seguridad de supply chain. La cadena de suministro de IA es el siguiente blind spot ‚Äî con una complejidad adicional: un modelo de IA comprometido no solo ejecuta c√≥digo malicioso, sino que manipula el razonamiento, sesga las decisiones, y exfiltra datos de formas que ninguna herramienta de seguridad tradicional puede detectar.

La escala del problema es concreta: Hugging Face, el mayor repositorio de modelos open source del mundo, hospeda casi 1.9 millones de modelos con uno nuevo publicado cada 7 segundos. Protect AI escane√≥ 4.47 millones de versiones √∫nicas de modelos y encontr√≥ 352,000 problemas de seguridad en 51,700 modelos. JFrog encontr√≥ 400 modelos con c√≥digo malicioso de un mill√≥n analizado. Y el 25% de las organizaciones no sabe qu√© servicios de IA o datasets tiene activos en su entorno (Wiz 2025). Sin visibilidad, no hay seguridad posible.

<br>

| DIMENSI√ìN                             | SUPPLY CHAIN SOFTWARE CL√ÅSICO                                                                             | SUPPLY CHAIN DE IA                                                                                                                                                                                | RIESGO DIFERENCIAL                                                                                                                                                                     |
| ------------------------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Componente vulnerable                 | Biblioteca con CVE conocido: detectable, parcheable. Comportamiento determin√≠stico.                       | Modelo con backdoor: activaci√≥n por triggers espec√≠ficos, comportamiento dormante hasta que se activa la condici√≥n.                                                                               | INVISIBLE: los backdoors en modelos de IA no son detectables por scanners de c√≥digo est√°tico ni CVE databases.                                                                         |
| Vector de ataque                      | Dependencia comprometida en package registry (npm, PyPI). Typosquatting de nombres de paquetes conocidos. | Modelo malicioso en Hugging Face. Model namespace hijacking: el namespace original se borra y un atacante lo reclama con un modelo trojanizado.                                                   | DIN√ÅMICO: el namespace de un modelo puede ser reclamado por un atacante en cualquier momento si el autor original borra su cuenta.                                                     |
| Detecci√≥n                             | SBOM + scanner de CVEs. Herramientas maduras (Snyk, OWASP Dependency Check, GitHub Dependabot).           | AIBOM + behavioral testing de modelos. Herramientas emergentes. Serialization scanning (Pickle/Safetensors). Sin CVE database equivalente para backdoors de IA.                                   | INMADURO: el ecosistema de seguridad de IA tiene 3-5 a√±os de retraso frente al de software tradicional.                                                                                |
| Impacto de compromiso                 | Ejecuci√≥n de c√≥digo malicioso, exfiltraci√≥n t√©cnica.                                                      | Decisiones sesgadas, exfiltraci√≥n de datos v√≠a outputs del modelo, backdoors que producen outputs espec√≠ficos ante triggers, credenciales de IA robadas (LLMjacking).                             | DIFUSO: el da√±o de un modelo comprometido puede ser dif√≠cil de distinguir de comportamiento normal del LLM, complicando la investigaci√≥n forense.                                      |
| Remediaci√≥n                           | Actualizar o reemplazar la librer√≠a. Proceso bien conocido.                                               | Reemplazar el modelo, retestear toda la aplicaci√≥n, revisar si datos de producci√≥n fueron exfiltrados durante el tiempo de exposici√≥n.                                                            | COSTOSO Y LENTO: a diferencia de un parche de librer√≠a, reemplazar un modelo puede requerir reentrenamiento, evaluaci√≥n de calidad, y migraci√≥n de aplicaciones.                       |
| Superficie de ataque nueva: Shadow AI | No hay equivalente directo.                                                                               | 49% de empleados usa herramientas de IA no aprobadas por IT. Shadow AI breaches cuestan en promedio USD 670,000 m√°s que incidentes tradicionales (Reco AI 2025). Promedio de detecci√≥n: 247 d√≠as. | MASIVA: la superficie de ataque del Shadow AI es inherentemente invisible para el equipo de seguridad porque los controles de visibilidad no est√°n desplegados donde el riesgo existe. |

<br>

La Fase 14 completa la arquitectura de seguridad del CISO ante la IA desde la perspectiva del supply chain: ¬øqu√© entra en nuestro ecosistema de IA (modelos, datos, plugins, MCP servers), qui√©n lo public√≥, est√° verificado, y c√≥mo lo gestionamos de forma continua? Es la capa de seguridad que empieza antes del primer prompt ‚Äî en el momento en que la organizaci√≥n decide qu√© componentes de IA usar√°.

<br>

## 14.1 EL AI BILL OF MATERIALS (AIBOM): VISIBILIDAD COMO PREREQUISITO

<br>

Un AI Bill of Materials (AIBOM) es el inventario estructurado de todos los componentes que constituyen un sistema de IA: modelos y sus versiones, datos de entrenamiento y su procedencia, dependencias de software, infraestructura de despliegue, y las relaciones entre todos estos elementos. Es el equivalente del SBOM para sistemas de IA ‚Äî con la diferencia cr√≠tica de que los sistemas de IA son din√°micos: se reentrenan, se fine-tunean, ingieren datos continuamente, y sus dependencias evolucionan. El AIBOM no es un documento est√°tico sino un artefacto vivo.

El EU AI Act (GPAI obligations, agosto 2025) requiere transparencia sobre datos de entrenamiento, medidas de mitigaci√≥n de riesgos y Safety and Security Model Reports para sistemas de IA de prop√≥sito general. Los reguladores pueden solicitar auditor√≠as de procedencia y las reclamaciones de secreto comercial sin documentaci√≥n no ser√°n suficientes. El AIBOM es la respuesta operacional a esta obligaci√≥n regulatoria.

<br>

### 14.1.1 Componentes del AIBOM: Qu√© Documentar

| COMPONENTE                                | QU√â INCLUYE                                                                                                                                                          | POR QU√â IMPORTA PARA SEGURIDAD                                                                                                                                                                                           | HERRAMIENTA                                                                                                           |
| ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- |
| Metadatos del Modelo                      | Nombre, versi√≥n, arquitectura (ej: transformer, 70B par√°metros), proveedor, fecha de creaci√≥n, URL de descarga, hash/checksum del archivo de pesos, licencia de uso. | El hash/checksum permite detectar si el modelo descargado fue alterado respecto al publicado por el autor. Sin hash, no hay integridad verificable.                                                                      | CycloneDX ML-BOM, SPDX AI profile, model registries (Azure AI Model Catalog, Hugging Face Model Card).                |
| Procedencia del Modelo (Model Provenance) | Qui√©n entren√≥ el modelo, cu√°ndo, con qu√© datos de entrenamiento, qu√© fine-tuning fue aplicado y cu√°ndo, qui√©n aprob√≥ cada versi√≥n para uso en producci√≥n.            | Sin procedencia documentada, no se puede determinar si el modelo fue comprometido durante el entrenamiento (data poisoning) o si las fuentes de datos tienen restricciones legales (copyright, PII).                     | MLflow experiment tracking, W\&B (Weights & Biases), Azure ML Model Registry, HuggingFace Model Cards.                |
| Datos de Entrenamiento                    | Fuentes de datos usadas, fechas de corte, licencias de los datasets, transformaciones de preprocesamiento, clasificaci√≥n de sensibilidad de los datos.               | Identifica exposici√≥n a: (1) data poisoning si una fuente de datos fue comprometida, (2) riesgos legales si los datos de entrenamiento incluyen contenido protegido por copyright o datos personales sin consentimiento. | Data lineage tools: Azure Purview, Apache Atlas, dbt, DVC (Data Version Control).                                     |
| Dependencias de Software                  | Frameworks de ML (PyTorch, TensorFlow, Keras), librer√≠as de inferencia (transformers, llama.cpp, ONNX), versiones exactas, CVEs conocidos.                           | Los frameworks de ML tienen CVEs propios que pueden ser explotados durante la carga del modelo. CVE-2025-1550 en Keras permite ejecuci√≥n de c√≥digo malicioso a trav√©s de custom layers.                                  | SBOM tools est√°ndar (Syft, Trivy) + scanners espec√≠ficos de ML (Protect AI Guardian, ClamAV 1.5 para modelos).        |
| Infraestructura de Despliegue             | Endpoints de inferencia (URLs, versiones de API), entornos de ejecuci√≥n (contenedores, GPUs), configuraciones de red y acceso, credenciales de servicio usadas.      | Un endpoint de inferencia sin autenticaci√≥n adecuada o con credenciales est√°ticas expuestas es un vector de LLMjacking. La infraestructura es parte del supply chain.                                                    | Azure AI Foundry deployment registry, Kubernetes SBOM tools, gesti√≥n de secretos (Azure Key Vault).                   |
| Integraciones y Conectores                | Plugins, conectores MCP, APIs externas llamadas por el sistema de IA, fuentes de datos RAG, webhooks.                                                                | Cada integraci√≥n es un punto de entrada para prompt injection indirecta y un vector de supply chain. Un conector comprometido puede manipular las respuestas del LLM.                                                    | Inventario de conectores de Copilot Studio, registro de MCP servers en uso, revisi√≥n de permisos de cada integraci√≥n. |
| Evaluaciones y Resultados de Testing      | Resultados de red teaming (ver Fase 13), benchmarks de calidad, evaluaciones de sesgo, testing de safety, fecha del √∫ltimo test.                                     | Sin registro de evaluaciones hist√≥ricas, no se puede detectar drift de comportamiento: si el modelo empieza a comportarse diferente tras una actualizaci√≥n, ¬øc√≥mo lo sabemos si no tenemos la baseline?                  | PyRIT scorecards, Azure AI Foundry evaluation runs, Promptfoo risk assessments.                                       |

<br>

### 14.1.2 Est√°ndares de AIBOM: CycloneDX ML-BOM y SPDX AI Profile

| EST√ÅNDAR                             | DESCRIPCI√ìN                                                                                                                                                                                                  | SOPORTE AIBOM                                                                                                                                                                                                   | ADOPCI√ìN EN MICROSOFT                                                                                                                                              |
| ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| CycloneDX v1.5+(OWASP gestiona)      | El est√°ndar SBOM m√°s adoptado para software. La versi√≥n 1.5 incorpora el Machine Learning BOM (ML-BOM) que extiende CycloneDX con campos espec√≠ficos para AI/ML: modelos, datasets, data lineage.            | Soporte nativo para: model architecture, training datasets con provenance, AI/ML dependencies, hyperparameters. Formato machine-readable (JSON/XML). Compatible con la mayor√≠a de herramientas SBOM existentes. | Azure DevOps puede generar CycloneDX SBOMs. Integraci√≥n planificada con Azure AI Foundry para AIBOM generation en los pipelines de ML (roadmap 2026).              |
| SPDX v3.0(Linux Foundation gestiona) | Est√°ndar abierto del sector para SBOM. La versi√≥n 3.0 a√±ade un AI Profile con elementos espec√≠ficos para sistemas de IA y ML: campos para datos de entrenamiento, modelos, y sus relaciones.                 | AI Profile incluye: campos para autonomy type, safety risk assessment, data preprocessing steps. Interoperable con CycloneDX. Gobierno conjunto SPDX-OWASP para estandarizaci√≥n.                                | SPDX es soportado como formato de exportaci√≥n en herramientas de compliance de GitHub Advanced Security.                                                           |
| Model Card(Google / Hugging Face)    | Documento semi-estructurado que describe un modelo: sus capacidades, limitaciones, datos de entrenamiento, evaluaciones de sesgo, casos de uso intended y usos fuera de scope.                               | Est√°ndar de facto para documentaci√≥n de modelos en la comunidad open source. Menos estructurado que CycloneDX/SPDX pero m√°s adoptado. Hugging Face lo requiere para modelos publicados.                         | Azure AI Studio genera Model Cards para modelos desplegados. Microsoft publica Model Cards para sus modelos propietarios (phi-4, Florence, etc).                   |
| Sigma / CLSigStore(Cosign para ML)   | Est√°ndar de firma criptogr√°fica para artefactos de software. La comunidad de ML est√° trabajando en la extensi√≥n a modelos: firmas digitales para verificar la integridad y procedencia de archivos de pesos. | Emergente: a√∫n no hay un est√°ndar de firma de modelos ampliamente adoptado. Cisco propone Model Artifact Trust Standard. CycloneDX soporta attestation fields.                                                  | Microsoft trabaja en firmado de modelos propios. Sin est√°ndar enterprise claro a√∫n ‚Äî √°rea en evoluci√≥n activa (2026-2027 probablemente el a√±o de estandarizaci√≥n). |

<br>

üìå Insight SD Times, enero 2026: Enterprise customers and regulators are moving beyond standard SOC 2 reports to demand Ingredient Transparency. Some vendor evaluations have stalled not because of firewall configurations, but because the vendor could not demonstrate the provenance of its training data. For the modern C-Suite, the AI BOM is becoming the standard Certificate of Analysis required to greenlight any AI-driven partnership.

<br>

## 14.2 RIESGOS DE MODELOS OPEN SOURCE: HUGGING FACE, PICKLE ATTACKS Y NAMESPACE HIJACKING

<br>

Hugging Face es para los modelos de IA lo que npm es para JavaScript o PyPI para Python: el repositorio de facto donde la comunidad comparte, descarga y usa modelos. Con casi 1.9 millones de modelos y uno nuevo publicado cada 7 segundos, es tambi√©n el vector de supply chain de mayor riesgo para organizaciones que desarrollan aplicaciones de IA propias o usan modelos open source en Azure AI Foundry. Los ataques documentados en 2025 demuestran que la apertura del ecosistema que lo hace valioso es exactamente lo que lo hace peligroso.

<br>

### 14.2.1 Taxonom√≠a de Ataques a Modelos Open Source

| TIPO DE ATAQUE                                        | MEC√ÅNICA T√âCNICA                                                                                                                                                                                                                                                                                                                                                                                        | INCIDENTE REAL DOCUMENTADO                                                                                                                                                                                                                                                                                                                              | DETECCI√ìN Y MITIGACI√ìN                                                                                                                                                                                                                                                                                                        |
| ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Pickle Deserialization (Malicious Pickling)           | El formato Pickle de Python permite ejecutar c√≥digo arbitrario durante la deserializaci√≥n del modelo (el momento en que PyTorch carga el archivo .pt o .pkl). Un atacante embebe un payload malicioso en el archivo Pickle que se ejecuta autom√°ticamente cuando el desarrollador carga el modelo con torch.load().                                                                                     | ReversingLabs (febrero 2025): t√©cnica 'NullifAI'. Dos modelos en Hugging Face con payload malicioso al inicio del stream Pickle, evadiendo Picklescan (el scanner de Hugging Face). Los modelos ten√≠an tag 'No issue'. El payload creaba nuevos procesos y ejecutaba comandos del sistema operativo. Hugging Face los elimin√≥ en 24h tras notificaci√≥n. | MIGRAR a formato Safetensors: dise√±ado sin capacidad de ejecuci√≥n de c√≥digo, solo datos. Usar Protect AI Guardian, ClamAV 1.5 (Cisco Foundation AI) para scanning pre-uso. NUNCA usar torch.load() con weights\_only=False en modelos de fuentes externas. Verificar hash SHA-256 del archivo antes de cargar.                |
| Model Namespace Hijacking (Reclamaci√≥n de Namespaces) | Un desarrollador o organizaci√≥n publica un modelo en Hugging Face bajo su namespace (Author/ModelName). Posteriormente, la cuenta del autor se elimina, liberando el namespace. Un atacante registra el mismo namespace y publica un modelo malicioso con el mismo nombre. Todos los sistemas que hacen pull del modelo por nombre (sin pin de hash) descargan autom√°ticamente la versi√≥n comprometida. | Palo Alto Networks Unit 42 (septiembre 2025): caso DentalAI / toothfAIry. La organizaci√≥n DentalAI borr√≥ su cuenta de Hugging Face. Un actor malicioso reclam√≥ el namespace y public√≥ un modelo comprometido bajo el mismo nombre. Pipelines que referenciaban el modelo por nombre (sin hash pin) lo descargaron autom√°ticamente.                      | Siempre referenciar modelos por hash/commit ID, no solo por nombre. Usar un Model Registry interno (Azure AI Foundry Model Catalog) que valide la integridad del modelo en el momento de importaci√≥n. Monitorear cambios en los modelos usados. Pol√≠tica: solo modelos de fuentes verificadas ingresados al registry interno. |
| Framework Vulnerabilities (CVEs en ML Frameworks)     | Los frameworks de ML (Keras, TensorFlow, PyTorch, ONNX) tienen CVEs propios que pueden ser explotados a trav√©s de modelos especialmente crafteados. Los mecanismos de extensibilidad de los frameworks (custom layers, plugins, callbacks) son vectores de ataque.                                                                                                                                      | CVE-2025-1550 en Keras: custom layers pueden ejecutar c√≥digo arbitrario a pesar de las features de seguridad. Protect AI Guardian alert√≥ a usuarios en Hugging Face antes de que la vulnerabilidad fuera divulgada p√∫blicamente.                                                                                                                        | Mantener frameworks de ML actualizados ‚Äî tratarlos como dependencias de software con pol√≠tica de patch. Incluir versiones de frameworks en el SBOM. Usar ambientes aislados (contenedores, sandboxing) para cargar y ejecutar modelos de fuentes externas.                                                                    |
| Data Poisoning en Training/Fine-tuning                | Un actor malicioso introduce datos envenenados en el pipeline de entrenamiento o fine-tuning: datos dise√±ados para crear backdoors que el modelo activa solo ante triggers espec√≠ficos, o para sesgar sistem√°ticamente las respuestas en una direcci√≥n deseada por el atacante.                                                                                                                         | Investigaci√≥n Trend Micro 2025: modelos con backdoors embedded como triggers estad√≠sticos son pr√°cticamente invisibles a an√°lisis est√°tico, SBOMs y revisi√≥n de c√≥digo. El comportamiento malicioso solo emerge bajo condiciones espec√≠ficas que el atacante controla.                                                                                  | Validaci√≥n diferencial de datos de entrenamiento (clustering + anomaly detection). Isolation de ambientes de fine-tuning. Shadow testing de modelos nuevos antes de promotion a producci√≥n. Behavioral testing post-entrenamiento con dataset de validaci√≥n adversarial.                                                      |
| Trojanized PyPI Packages (AI SDKs falsos)             | Atacantes publican paquetes en PyPI que se hacen pasar por SDKs leg√≠timos de proveedores de IA (Aliyun AI Labs, Alibaba Cloud AI Research). Los paquetes contienen malware oculto en el mismo formato Pickle, aprovechando que los desarrolladores conf√≠an en PyPI m√°s que en repositorios desconocidos.                                                                                                | ReversingLabs 2025: paquetes trojanizados en PyPI posando como SDKs para servicios de IA cloud, usando el mismo vector Pickle para ocultar el malware.                                                                                                                                                                                                  | Igual que software tradicional: SBOM + dependency scanning (Snyk, Trivy, GitHub Dependabot) para todos los paquetes Python en proyectos de IA. Pol√≠tica de dependencias aprobadas. Auditor√≠a de PyPI packages en proyectos de ML.                                                                                             |

<br>

### 14.2.2 Herramientas de Seguridad para Modelos Open Source

| HERRAMIENTA                    | PROVEEDOR                                 | CAPACIDADES                                                                                                                                                                                                                                            | APLICACI√ìN PARA CISO                                                                                                                                                                                                       |
| ------------------------------ | ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Protect AI Guardian            | Protect AI (partnership con Hugging Face) | Escanea modelos en Hugging Face: deserialization attacks, CVEs en frameworks, c√≥digo malicioso en extensiones. 4.47M modelos escaneados en 6 meses. Detecta: archive slip, Joblib code execution, TensorFlow backdoors, Llamafile malicious execution. | Integrar en el proceso de importaci√≥n de modelos externos al Azure AI Model Catalog. Cualquier modelo de Hugging Face debe pasar por Guardian antes de ser admitido al registro interno.                                   |
| ClamAV 1.5 + Cisco Cerberus    | Cisco Foundation AI                       | ClamAV 1.5 (open source): detecta malware en formatos de modelos .pt y .pkl. Cerberus: inspecci√≥n 24/7 en tiempo real de modelos subidos a Hugging Face, resultados en threat feeds estandarizados.                                                    | ClamAV es el √∫nico antivirus en VirusTotal que detecta malware en modelos de IA. Integrar en pipelines de CI/CD para escanear modelos antes de deployment.                                                                 |
| Azure AI Foundry Model Catalog | Microsoft                                 | Registry curado de modelos con vetting de proveedores. Incluye modelos de Microsoft, Meta, Mistral, OpenAI, y otros validados. Alternativa a consumir directamente de Hugging Face para organizaciones enterprise.                                     | Pol√≠tica: para cualquier modelo en aplicaciones de producci√≥n, usar solo modelos del Azure AI Model Catalog o modelos de Hugging Face que hayan pasado por escaneo Guardian/ClamAV y est√©n importados al registry interno. |
| VirusTotal para Modelos        | Google / VirusTotal                       | Permite upload y escaneo de archivos de modelos .pt, .pkl. ClamAV es el √∫nico motor AV actualmente con capacidad de detectar malware espec√≠fico de ML.                                                                                                 | Scan ad-hoc de modelos sospechosos. No como mecanismo principal (sin automatizaci√≥n), sino como herramienta de investigaci√≥n cuando surge una alerta.                                                                      |
| Picklescan + Safetensors       | Hugging Face / EleutherAI                 | Picklescan: scanner de c√≥digo de seguridad para archivos Pickle en Hugging Face (con las limitaciones demostradas por NullifAI). Safetensors: formato alternativo a Pickle que no permite ejecuci√≥n de c√≥digo ‚Äî solo almacena tensores.                | Requerir uso de Safetensors en todos los modelos propios desarrollados internamente. Para modelos externos, dar preferencia a versiones en Safetensors cuando disponibles.                                                 |

<br>

## 14.3 SHADOW AI: EL RIESGO INVISIBLE DENTRO DE LA ORGANIZACI√ìN

<br>

El Shadow AI es el conjunto de herramientas, modelos y sistemas de IA que los empleados adoptan sin el conocimiento ni la aprobaci√≥n del equipo de IT y seguridad. No es un fen√≥meno marginal: seg√∫n una encuesta de 2,000 empleados en EE.UU. y Reino Unido, el 49% usa herramientas de IA no aprobadas por sus empleadores, y m√°s de la mitad no entiende c√≥mo sus inputs son almacenados y analizados por esas herramientas. LayerX 2025 encontr√≥ que el 77% de empleados enterprise que usa IA ha pegado datos corporativos en un chatbot, y el 22% de esos casos inclu√≠a datos financieros o personales confidenciales.

<br>

### 14.3.1 El Ecosistema del Shadow AI y Sus Vectores de Riesgo

| TIPO DE SHADOW AI                      | DESCRIPCI√ìN                                                                                                                                                                                                                                     | DATO CUANTITATIVO                                                                                                                                                                                             | RIESGO PRINCIPAL                                                                                                                                                                                                                                                   |
| -------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Chatbots p√∫blicos no aprobados         | ChatGPT, Claude, Gemini, Copilot, DeepSeek accedidos directamente por empleados con datos corporativos. El caso Samsung (ingenieros que pegaron c√≥digo propietario en ChatGPT) fue el primero masivamente documentado.                          | 77% de empleados enterprise que usa IA ha pegado datos corporativos en un chatbot (LayerX 2025). Samsung bane√≥ herramientas de IA externas tras el incidente. JPMorgan y Goldman Sachs restringieron ChatGPT. | EXFILTRACI√ìN DE DATOS: el proveedor del chatbot puede usar los inputs para entrenamiento (dependiendo de los t√©rminos de servicio y configuraci√≥n ZDR). Datos de clientes, c√≥digo fuente, estrategias de M\&A, datos financieros no-p√∫blicos enviados sin control. |
| Plugins y extensiones de browser       | Extensiones de Chrome/Edge con IA que interceptan el contenido de p√°ginas web, emails, y documentos antes de que el usuario interact√∫e con ellos. Muchas solicitan permisos amplios (leer todos los datos de todos los sitios web).             | Gartner: 80% de los empleados en empresas con 10,000+ empleados usan al menos una extensi√≥n de browser sin aprobaci√≥n de IT.                                                                                  | INTERCEPTACI√ìN: las extensiones de IA pueden capturar credenciales, cookies de sesi√≥n, contenido de SaaS corporativos (Salesforce, ServiceNow, HubSpot) y enviarlo a servidores externos sin que el usuario o IT lo sepa.                                          |
| Herramientas de AI coding no aprobadas | Desarrolladores usando Cursor, Tabnine, Codeium, Continue u otras herramientas de AI coding que no forman parte del stack aprobado. El c√≥digo corporativo (incluidos secretos embebidos) se env√≠a a los modelos de los proveedores.             | Sonar Survey 2026: 60% de empleados en empresas grandes dicen que usan herramientas de IA no aprobadas si aumentan la productividad, aunque reconocen el riesgo de seguridad.                                 | C√ìDIGO SECRETO: los modelos de AI coding reciben fragmentos de c√≥digo que pueden incluir API keys, credenciales hardcodeadas, l√≥gica de negocio propietaria, y c√≥digo de sistemas clasificados como no para divulgaci√≥n.                                           |
| Agentes de IA no gestionados           | OpenClaw y similares: agentes de IA open source que empleados instalan y conectan a sus cuentas corporativas de Slack, Google Workspace, Microsoft 365 sin aprobaci√≥n de IT. Los agentes tienen permisos OAuth sobre los sistemas corporativos. | OpenClaw: 150,000 GitHub stars en enero 2026. Gartner lo describi√≥ como 'inaceptable liability de ciberseguridad' y recomend√≥ bloquearlo inmediatamente. 21,000 instancias expuestas.                         | AGENTE PRIVILEGIADO: un agente no gestionado conectado a M365 con OAuth tiene permisos sobre Email, Teams, SharePoint, OneDrive. Si el agente es comprometido (o tiene vulnerabilidades propias), el atacante hereda esos permisos.                                |
| Modelos locales no gestionados         | Desarrolladores descargando y ejecutando modelos de Hugging Face localmente (LLaMA 3, Mistral, DeepSeek) en sus laptops corporativas, a veces conectados a datos internos a trav√©s de frameworks como Ollama+LangChain.                         | 25% de organizaciones no saben qu√© servicios de IA o datasets tienen activos en su entorno (Wiz 2025).                                                                                                        | SUPPLY CHAIN + DATA: combina los riesgos de modelos open source sin vetting con el riesgo de que esos modelos accedan a datos corporativos en la laptop del desarrollador.                                                                                         |
| SaaS de IA con integraci√≥n OAuth       | Herramientas SaaS de IA que los empleados conectan a sus cuentas corporativas v√≠a OAuth: herramientas de summarizaci√≥n de emails, asistentes de calendario, herramientas de notas inteligentes. Cada una tiene acceso a datos corporativos.     | Reco AI 2025: Shadow AI breaches cuestan promedio USD 670,000 m√°s que incidentes tradicionales. Promedio de detecci√≥n: 247 d√≠as. Afectan principalmente PII de clientes (65%) e IP (40%).                     | ACCESO LATENTE: las integraciones OAuth de Shadow AI son la forma m√°s dif√≠cil de detectar. El acceso parece leg√≠timo (viene de un OAuth grant real), no es malware, y no genera alertas de seguridad tradicionales.                                                |

<br>

### 14.3.2 Controles para Shadow AI

| CAPA DE CONTROL              | CONTROLES ESPEC√çFICOS                                                                                                                                                                                                                                                                                       | IMPLEMENTACI√ìN EN MICROSOFT ECOSYSTEM                                                                                                                                                                                                                                                                     |
| ---------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Visibilidad y Descubrimiento | Descubrir qu√© herramientas de IA ya est√°n en uso (antes de prohibir o aprobar): auditar OAuth grants en M365, revisar logs de DLP por env√≠o de datos a dominios de IA, escanear el tr√°fico de red para identificar conexiones a APIs de IA.                                                                 | Microsoft Defender for Cloud Apps (MCAS): discovery de shadow IT incluyendo herramientas de IA. Azure AD sign-in logs: OAuth consents otorgados por usuarios. Microsoft Purview DLP: alertas cuando datos etiquetados se env√≠an a dominios externos de IA.                                                |
| Pol√≠tica y Clasificaci√≥n     | Implementar un registro de herramientas de IA aprobadas (AI Approved Tools Registry) con nivel de datos permitido para cada herramienta. Ejemplo: ChatGPT Plus con ZDR habilitado ‚Üí datos internos non-confidential OK; Microsoft 365 Copilot ‚Üí todos los datos OK; ChatGPT gratuito ‚Üí solo datos p√∫blicos. | Microsoft Entra Conditional Access: bloquear acceso OAuth a dominios de IA no aprobados. Intune: gestionar qu√© extensiones de browser se pueden instalar en dispositivos corporativos. MIP sensitivity labels: datos con etiqueta Confidential/Restricted no pueden pegarse en aplicaciones no aprobadas. |
| DLP para IA                  | Implementar pol√≠ticas de DLP espec√≠ficas para el flujo de datos hacia sistemas de IA: alertar o bloquear cuando datos clasificados como confidenciales se env√≠an a endpoints de IA no aprobados.                                                                                                            | Microsoft Purview DLP: pol√≠ticas para detectar env√≠o de PII, datos financieros, o datos clasificados a dominios externos de IA (.openai.com, .anthropic.com, .deepseek.com). Integraci√≥n con sensitivity labels para DLP automatizado basado en clasificaci√≥n.                                            |
| Gesti√≥n de OAuth Grants      | Auditar y revocar OAuth grants no autorizados a herramientas de IA externas. Implementar proceso de aprobaci√≥n para nuevas OAuth connections a aplicaciones de IA.                                                                                                                                          | Microsoft Entra ID: revisar Enterprise Applications para OAuth consents de usuarios. Reco o Obsidian Security: herramientas especializadas en Shadow AI discovery a trav√©s de an√°lisis de OAuth grants. Proceso: revisi√≥n mensual de nuevos OAuth grants, revocaci√≥n de no-aprobados.                     |
| Educaci√≥n y Habilitaci√≥n     | La prohibici√≥n sin alternativa fuerza al Shadow AI a la clandestinidad total. La estrategia m√°s efectiva es ofrecer alternativas aprobadas que cubran los casos de uso leg√≠timos: M365 Copilot aprobado para productividad, GitHub Copilot Enterprise para desarrollo.                                      | Programa de AI Champions: empleados certificados que ayudan a sus equipos a usar herramientas de IA aprobadas efectivamente. Pol√≠tica clara: qu√© est√° aprobado, con qu√© tipos de datos, y c√≥mo solicitarle aprobaci√≥n a IT para una herramienta nueva.                                                    |

<br>

## 14.4 MCP SERVER SECURITY: EL NUEVO RIESGO DE SUPPLY CHAIN AG√âNTICO

<br>

El Model Context Protocol (MCP), introducido por Anthropic en noviembre 2024, se ha convertido en el est√°ndar de facto para que los LLMs interact√∫en con fuentes de datos externas y herramientas. En menos de un a√±o, hay m√°s de 20,000 servidores MCP publicados en GitHub y el protocolo es soportado nativamente por Visual Studio Code, Claude Code CLI, Cursor, y otras herramientas de desarrollo. La adopci√≥n masiva precedi√≥ a la madurez de seguridad del ecosistema ‚Äî exactamente el patr√≥n que en el pasado llev√≥ a las crisis de supply chain m√°s graves.

El estudio de Astrix Security sobre 20,000 implementaciones de MCP servers en GitHub revela la realidad: el 88% requiere credenciales para funcionar, el 53% usa API keys est√°ticas o Personal Access Tokens de larga vida, y solo el 8.5% usa OAuth (el m√©todo moderno de delegaci√≥n segura). El 79% de las API keys se pasan a trav√©s de simples variables de entorno. El ecosistema MCP es, en t√©rminos de gesti√≥n de credenciales, similar al estado del cloud en 2013: funcional pero fundamentalmente inseguro.

<br>

### 14.4.1 Vectores de Riesgo de los MCP Servers

| VECTOR                                                 | MEC√ÅNICA                                                                                                                                                                                                                                                                                                                                                                                     | INCIDENTE/INVESTIGACI√ìN                                                                                                                                                                                                                                    | CONTROL                                                                                                                                                                                                                                                                                 |
| ------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Supply Chain: C√≥digo Malicioso en MCP Servers          | Los MCP servers se distribuyen principalmente a trav√©s de GitHub (repositorios privados y p√∫blicos) sin registros oficiales verificados ni est√°ndares de signing. Cualquier desarrollador puede publicar un MCP server. La instalaci√≥n equivale a ejecutar c√≥digo arbitrario: los instaladores autom√°ticos (mcp-installer) facilitan la instalaci√≥n de servidores sin inspecci√≥n del c√≥digo. | Investigaci√≥n Wiz: 'instalar un MCP server local es definitivamente ejecutar c√≥digo arbitrario en tu m√°quina'. Cursor IDE: investigadores demostraron c√≥mo un MCP server rogue puede inyectar c√≥digo malicioso en el browser integrado del IDE.            | Usar solo MCP servers de fuentes verificadas (Microsoft, proveedores conocidos). Revisar el c√≥digo fuente antes de instalar cualquier MCP server de terceros. Pol√≠tica: MCP servers requieren aprobaci√≥n del equipo de seguridad antes de uso en entornos con datos corporativos.       |
| Credenciales Est√°ticas Expuestas                       | 53% de MCP servers usan API keys o PATs de larga vida pasados como variables de entorno o embebidos en archivos de configuraci√≥n. Estas credenciales son de largo plazo (a veces sin expiraci√≥n), tienen scope amplio (t√≠picamente m√°s permisos de los necesarios), y se distribuyen con el c√≥digo.                                                                                          | Astrix Research 2025: an√°lisis de 20,000 implementaciones de MCP servers en GitHub. 88% requiere credenciales. 53% usa API keys est√°ticas. 79% las pasa v√≠a environment variables. Solo 8.5% usa OAuth.                                                    | Herramienta open source: Astrix MCP Secret Wrapper ‚Äî wrappea cualquier MCP server existente y reemplaza credenciales est√°ticas por pull din√°mico desde un vault (AWS Secrets Manager, Azure Key Vault). Principio: short-lived credentials, just-in-time access, least privilege scope. |
| Prompt Hijacking y Session ID Guessing                 | La comunicaci√≥n entre clientes MCP y servidores MCP remotos no siempre es segura. Los session IDs que identifican una sesi√≥n activa pueden ser predecibles o suficientemente cortos para ser adivinados por un atacante, que puede entonces secuestrar la sesi√≥n y acceder a los datos y herramientas del servidor.                                                                          | CSO Online (diciembre 2025): MCP servers remotos expuestos a 'prompt hijacking' por session IDs predecibles. LangSmith vulnerability (Noma Security): vulnerabilidad en herramienta de IA que permit√≠a robo de API keys y hijacking de respuestas del LLM. | Implementar TLS mutuo para comunicaciones cliente-servidor MCP. Session IDs cryptographically random de longitud suficiente. Autenticaci√≥n basada en OAuth 2.0 para servidores remotos. Validar el modelo de autorizaci√≥n de MCP antes de desplegarlo en producci√≥n.                    |
| Tool Poisoning (Descripci√≥n Maliciosa de Herramientas) | Un MCP server malicioso puede proporcionar al LLM descripciones de herramientas manipuladas que instigan al modelo a tomar acciones no intencionadas cuando las herramientas son llamadas ‚Äî incluso si el c√≥digo de las herramientas en s√≠ es benigno.                                                                                                                                       | Investigaci√≥n acad√©mica 2025: demostraci√≥n de c√≥mo un MCP server puede describir una herramienta de 'leer archivo' de forma que el LLM la usa para leer archivos de credenciales cuando el prompt del usuario es suficientemente ambiguo.                  | Tratar las descripciones de herramientas MCP como inputs no confiables. Validar que las acciones que el agente toma corresponden a la intenci√≥n del usuario. Monitorear los tool calls de los agentes para detectar patrones an√≥malos (ver Fase 10 ‚Äî Defender for AI).                  |
| Privilege Escalation entre Servidores MCP              | En arquitecturas con m√∫ltiples servidores MCP, un servidor de baja confianza puede intentar hacer llamadas a un servidor de alta confianza a trav√©s del LLM actuando como intermediario, efectuando una escalada de privilegios indirecta.                                                                                                                                                   | Caso ServiceNow (diciembre 2025): 'second-order' prompt injection entre agentes con distintos niveles de privilegio. Un agente de baja confianza manipulaba a un agente de alta confianza para exportar un case file completo a una URL externa.           | Arquitectura de confianza: cada servidor MCP tiene un nivel de confianza expl√≠cito. El LLM no puede pedir a un servidor de alta confianza que ejecute acciones iniciadas por un servidor de baja confianza sin validaci√≥n humana.                                                       |

<br>

## 14.5 LLMJACKING Y ROBO DE CREDENCIALES DE IA

<br>

LLMjacking es el nombre que la comunidad de seguridad dio al robo y abuso de credenciales de APIs de LLM (OpenAI, Anthropic, Azure OpenAI, Amazon Bedrock, Google Vertex AI). Los atacantes roban API keys corporativas y las usan para acceder a los modelos en nombre de la organizaci√≥n v√≠ctima ‚Äî generando facturas masivas y, m√°s cr√≠tico, usando los modelos para generar contenido que bypasea los guardrails √©ticos configurados para usuarios empresariales. Microsoft present√≥ una demanda civil en 2025 contra un grupo especializado en LLMjacking que usaba las credenciales robadas para construir servicios de pago para otros criminales.

<br>

| ASPECTO                                | DETALLE                                                                                                                                                                                                                                                                                                                    | IMPACTO CUANTIFICADO                                                                                                                                                                                                                                           | CONTROL                                                                                                                                                                    |
| -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Vectores de robo de API keys           | Los atacantes obtienen API keys a trav√©s de: (1) repositorios de c√≥digo p√∫blico con keys hardcodeadas (GitHub, GitLab), (2) credenciales en archivos .env versionados, (3) secrets en logs o variables de entorno expuestas, (4) compromiso de sistemas de CI/CD, (5) apps de terceros que almacenan las keys del usuario. | Wiz (junio 2025): escaneo de repositorios p√∫blicos encontr√≥ AI-related secret leaks masivos ‚Äî notebooks de Jupyter con keys hardcodeadas son el vector m√°s com√∫n.                                                                                              | Rotaci√≥n regular de API keys. Nunca almacenar en repos. Usar Azure Key Vault para gesti√≥n de secrets. GitHub secret scanning + push protection.                            |
| Escala del abuso                       | Un atacante con acceso a m√∫ltiples API keys roba capacidad de compute de m√∫ltiples organizaciones simult√°neamente. El acceso a modelos de √∫ltima generaci√≥n (GPT-4, Claude 3.5 Sonnet, Gemini Ultra) cuesta significativamente m√°s que modelos anteriores.                                                                 | Investigadores estiman costos potenciales de m√°s de USD 100,000 por d√≠a cuando se consultan modelos frontier con una API key robada. El costo se factura a la organizaci√≥n v√≠ctima.                                                                            | Billing alerts: configurar alertas de Azure para consumo inusual de Azure OpenAI. Dashboards de uso por usuario/aplicaci√≥n. Rate limiting por aplicaci√≥n.                  |
| El servicio criminal resultante        | El grupo demandado por Microsoft usaba las API keys robadas para construir un servicio de pago que ofrec√≠a a otros criminales acceso a modelos de IA configurados para generar contenido que normalmente violar√≠a los t√©rminos de servicio de los proveedores: malware, phishing, deepfakes, CSAM.                         | Los compradores del servicio criminal obten√≠an: (1) acceso al modelo sin pagar, (2) jailbreak de los guardrails del proveedor que el modelo enterprise tiene configurados, (3) ausencia de logging en la cuenta del comprador (el logging est√° en la v√≠ctima). | Monitorear anomal√≠as en los patterns de uso de Azure OpenAI. Keys por aplicaci√≥n (no una key maestra para todo). Scope m√≠nimo por key.                                     |
| Exposici√≥n de datos de la organizaci√≥n | Cuando un atacante usa la API key robada, el prompt y la respuesta se registran en los logs de la organizaci√≥n v√≠ctima (no del atacante). Si la key tiene acceso a modelos con datos de la organizaci√≥n (fine-tuning, RAG), el atacante puede potencialmente extraer esos datos.                                           | Menos documentado pero potencialmente m√°s grave que el costo financiero: el atacante puede usar la key para interrogar el sistema de IA de la organizaci√≥n y extraer informaci√≥n propietaria embebida en el contexto del modelo.                               | Auditor√≠a de logs de Azure OpenAI: revisar patrones de uso fuera del horario laboral, desde IPs inusuales, con prompts que no corresponden al uso normal de la aplicaci√≥n. |

<br>

## 14.6 EVALUACI√ìN DE TERCEROS EN EL ECOSISTEMA M365: PLUGINS, CONECTORES Y RAG

<br>

El ecosistema de Microsoft 365 Copilot y Copilot Studio tiene una arquitectura de extensibilidad que permite a terceros publicar plugins, conectores de Graph, agentes de voz y conectores de datos que ampl√≠an las capacidades del Copilot. Para el CISO, cada extensi√≥n de terceros es un componente de la cadena de suministro de IA que requiere evaluaci√≥n antes de que la organizaci√≥n lo adopte. La superficie de ataque del Copilot se expande con cada plugin activado.

<br>

| TIPO DE EXTENSI√ìN M365                                          | ACCESO QUE OTORGA                                                                                                                                               | RIESGO PRINCIPAL                                                                                                                                                                                                                                      | DUE DILIGENCE REQUERIDO                                                                                                                                                                                                                                                                                                                                                                                                                                |
| --------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Plugins de Copilot M365 (marketplace de Microsoft)              | Acceso a los datos del usuario en M365 seg√∫n permisos OAuth otorgados. Algunos plugins tienen capacidades de read/write.                                        | Supply chain: un plugin comprometido tiene acceso a todos los datos a los que el usuario tiene acceso, m√°s los datos que el Copilot ha procesado en la sesi√≥n.                                                                                        | (1) Verificar que el plugin est√° en el cat√°logo oficial aprobado de Microsoft. (2) Revisar los permisos OAuth que solicita ‚Äî ¬øson m√≠nimos necesarios? (3) Leer la pol√≠tica de privacidad del proveedor: ¬øc√≥mo maneja los datos del usuario? (4) Verificar certificaciones de seguridad del proveedor (SOC 2 Type II, ISO 27001). (5) Bloquear todos los plugins por defecto y aprobar solo los necesarios.                                             |
| Conectores de Copilot Studio (Graph connectors, Power Platform) | Acceso a fuentes de datos externas (Salesforce, ServiceNow, SAP, DBs propias) para el RAG del agente. Los datos de estas fuentes alimentan el contexto del LLM. | Data ingestion sin control: los datos de los sistemas conectados pasan a ser contexto del LLM. Si esos datos incluyen PII, secretos, o informaci√≥n confidencial, pasan a estar disponibles para el agente ‚Äî y potencialmente para exfiltraci√≥n.       | (1) Inventariar TODOS los conectores configurados en cada agente de Copilot Studio. (2) Para cada conector: ¬øqu√© datos exactos est√°n en scope? ¬øEst√°n clasificados con MIP labels? (3) Verificar que las credenciales del conector est√°n en Azure Key Vault (no hardcodeadas en el agente). (4) Auditar los logs de uso del conector: ¬øqui√©n est√° accediendo a los datos a trav√©s del agente?                                                          |
| MCP Servers para Claude Code / Copilot                          | Acceso a herramientas espec√≠ficas: filesystem, APIs, databases, servicios cloud. Potencialmente ilimitado dependiendo del server.                               | El m√°s peligroso: MCP servers de terceros con acceso a sistemas de producci√≥n. Un server comprometido o malicioso tiene acceso a todo lo que el desarrollador tiene.                                                                                  | (1) Policy: PROHIBIDOS los MCP servers de fuentes desconocidas en entornos con datos corporativos. (2) Lista blanca de MCP servers aprobados. (3) Code review del c√≥digo fuente de cualquier MCP server antes de aprobaci√≥n. (4) Gesti√≥n de credenciales: Azure Key Vault para todas las credenciales de MCP servers. (5) Monitoreo de acciones: logs completos de tool calls del agente.                                                              |
| RAG Data Sources (SharePoint, bases de datos, APIs)             | Los documentos y datos indexados en el RAG del agente se convierten en contexto disponible para el LLM y, potencialmente, para los usuarios del agente.         | Corpus poisoning: un documento malicioso en el corpus RAG puede contener instrucciones de prompt injection que se activan cuando son recuperadas. Data leakage: el RAG puede exponer documentos a usuarios que no tienen permiso directo sobre ellos. | (1) Aplicar el principio de least data: solo indexar los documentos estrictamente necesarios para el caso de uso del agente. (2) Verificar que los permisos del RAG respetan los permisos de SharePoint del usuario (Copilot for M365 hace esto nativamente, pero los agentes custom pueden no hacerlo). (3) Auditar el corpus del RAG peri√≥dicamente: ¬øhay documentos desactualizados, de clasificaci√≥n incorrecta, o que no deber√≠an estar en scope? |

<br>

## 14.7 PLAN DE ACCI√ìN PARA CISO: PROGRAMA DE SUPPLY CHAIN SECURITY DE IA

<br>

El programa de supply chain security de IA para una organizaci√≥n con 20,000+ licencias M365 E5 debe construirse en tres niveles simult√°neos: visibilidad (saber qu√© hay), control (gestionar qu√© entra), y monitoreo continuo (detectar cambios en lo que ya est√°). Los 12 pasos que siguen son acciones concretas ordenadas por prioridad y horizonte temporal.

<br>

| <p><br></p> | PRIORIDAD              | ACCI√ìN                                                                       | DETALLE DE IMPLEMENTACI√ìN                                                                                                                                                                                                                                                                                                                                                                                                                                                         | HORIZONTE         |
| ----------- | ---------------------- | ---------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------- |
| 1           | CR√çTICO ‚Äî VISIBILIDAD  | Inventario de todos los componentes de IA (AIBOM inicial)                    | Ejecutar un inventario completo: (1) Qu√© modelos est√°n en uso (incluyendo Azure AI Foundry deployments, modelos fine-tuned propios, y modelos locales en laptops de desarrolladores). (2) Qu√© conectores y plugins est√°n activados en Copilot Studio. (3) Qu√© MCP servers est√°n instalados en los entornos de desarrollo. (4) Qu√© herramientas de IA externas est√°n siendo accedidas (Shadow AI discovery v√≠a Defender for Cloud Apps). Resultado: AIBOM v1 en formato CycloneDX. | 30 d√≠as           |
| 2           | CR√çTICO ‚Äî SHADOW AI    | Desplegar Shadow AI Discovery y DLP para IA                                  | Activar Defender for Cloud Apps para discovery de aplicaciones de IA no aprobadas. Implementar pol√≠tica de DLP en Microsoft Purview para detectar env√≠o de datos clasificados (Confidential, Highly Confidential) a dominios de IA externos. Auditar OAuth consents en Azure AD: revocar consents a aplicaciones de IA no aprobadas. Reportar hallazgos al AI Governance Committee (ver Fase 7).                                                                                  | 30 d√≠as           |
| 3           | CR√çTICO ‚Äî CREDENCIALES | Auditar y proteger todas las API keys de IA                                  | Inventariar todas las API keys de servicios de IA (Azure OpenAI, OpenAI, Anthropic, Google Vertex) que la organizaci√≥n tiene. Verificar que NINGUNA est√° en repositorios de c√≥digo (GitHub secret scanning). Migrar todas al Azure Key Vault. Implementar rotaci√≥n autom√°tica con per√≠odos m√°ximos de 90 d√≠as. Activar billing alerts en Azure para detectar consumo an√≥malo de Azure OpenAI.                                                                                     | 30 d√≠as           |
| 4           | ALTO ‚Äî MCP             | Pol√≠tica de MCP Servers y lista blanca                                       | Emitir pol√≠tica expl√≠cita: (1) PROHIBIDOS los MCP servers de fuentes no verificadas en cualquier entorno con datos corporativos. (2) Lista blanca inicial: solo MCP servers de Microsoft y partners de confianza nivel Tier-1. (3) Proceso de aprobaci√≥n: cualquier nuevo MCP server requiere code review de seguridad + aprobaci√≥n del equipo de seguridad. (4) Usar Astrix MCP Secret Wrapper para todos los MCP servers aprobados que usen credenciales.                       | 30-60 d√≠as        |
| 5           | ALTO ‚Äî PLUGINS         | Governance de plugins y conectores de Copilot Studio                         | Inventariar todos los plugins activados en el tenant de M365. Deshabilitar todos los plugins no en la lista de aprobados. Establecer proceso formal de aprobaci√≥n de nuevos plugins: due diligence del proveedor (SOC 2, pol√≠tica de privacidad), revisi√≥n de permisos OAuth, aprobaci√≥n del CISO para plugins con acceso a datos de nivel Confidential o superior.                                                                                                               | 30-60 d√≠as        |
| 6           | ALTO ‚Äî MODELO REGISTRY | Implementar Azure AI Model Registry como punto √∫nico de entrada para modelos | Establecer como pol√≠tica: ning√∫n modelo de IA externo puede usarse en aplicaciones de producci√≥n sin haber sido importado al Azure AI Model Catalog interno. Proceso de importaci√≥n: (1) Escaneo con Protect AI Guardian o ClamAV 1.5. (2) Verificaci√≥n de hash SHA-256. (3) Documentaci√≥n en el AIBOM (procedencia, versi√≥n, licencia). (4) Aprobaci√≥n del equipo de seguridad. (5) Pin de versi√≥n espec√≠fica (no float al latest).                                              | 60 d√≠as           |
| 7           | ALTO ‚Äî SBOM+AIBOM      | Integrar generaci√≥n autom√°tica de AIBOM en pipelines de CI/CD                | Para todos los proyectos internos que desarrollan o consumen modelos de IA: integrar generaci√≥n de AIBOM (CycloneDX format) en el pipeline de CI/CD. El AIBOM se genera autom√°ticamente con cada build que incluye un modelo o dependencia de IA nueva. Los AIBOMs se almacenan en un repositorio central (Azure Artifacts o similar) y se actualizan con cada release.                                                                                                           | 60-90 d√≠as        |
| 8           | MEDIO ‚Äî RAG GOVERNANCE | Auditor√≠a y governance del corpus RAG de todos los agentes                   | Para cada agente de Copilot Studio que usa RAG: (1) Documentar exactamente qu√© fuentes de datos est√°n indexadas. (2) Verificar que los permisos de acceso del RAG corresponden a los permisos del usuario (no over-sharing). (3) Implementar data classification review: ¬øhay documentos Confidential en corpus p√∫blicos? (4) Proceso de re-auditor√≠a trimestral.                                                                                                                 | 60-90 d√≠as        |
| 9           | MEDIO ‚Äî SAFETENSORS    | Mandato de uso de Safetensors para modelos propios                           | Para todos los modelos fine-tuned propietarios desarrollados internamente: mandato de guardar y distribuir en formato Safetensors en lugar de Pickle. Safetensors no permite ejecuci√≥n de c√≥digo durante la deserializaci√≥n, eliminando la clase completa de ataques de malicious Pickling.                                                                                                                                                                                       | 90 d√≠as           |
| 10          | MEDIO ‚Äî MONITOREO      | Activar Defender for AI y logging completo de Azure OpenAI                   | Activar el logging completo de Azure OpenAI Service: todos los prompts y respuestas logueados en Azure Monitor/Log Analytics (con configuraci√≥n de retenci√≥n y acceso apropiados). Defender for Cloud / Defender for AI: activar threat detection para AI workloads. Configurar alertas para: consumo de tokens fuera del baseline, IPs inusuales accediendo a los endpoints, patrones de prompt injection conocidos.                                                             | 90 d√≠as           |
| 11          | MEDIO ‚Äî EDUCACI√ìN      | Programa de AI Security Awareness para desarrolladores                       | Training espec√≠fico para el equipo de desarrollo sobre supply chain security de IA: (1) Por qu√© no se debe usar torch.load() sin weights\_only=True. (2) C√≥mo verificar la integridad de un modelo descargado. (3) Por qu√© las API keys no van en el c√≥digo. (4) Qu√© herramientas de IA est√°n aprobadas y c√≥mo solicitarle aprobaci√≥n a IT para una nueva. Formato: m√≥dulo de 30 minutos, completi√≥n obligatoria, quiz de verificaci√≥n.                                           | 90 d√≠as           |
| 12          | CICLO ‚Äî CONTINUO       | Programa de revisi√≥n continua de AIBOM y monitoreo de supply chain           | Establecer proceso mensual de revisi√≥n del AIBOM: ¬øhay nuevos modelos en uso no registrados? ¬øAlg√∫n modelo cambi√≥ de versi√≥n inesperadamente (posible namespace hijacking)? ¬øHay nuevos CVEs en los frameworks de ML usados? Suscribirse a threat feeds de AI supply chain: Protect AI Vulnerability Database, Cisco Cerberus alerts. Revisar nuevas publicaciones de MITRE ATLAS sobre supply chain attacks.                                                                     | Mensual, continuo |

<br>

## 14.8 M√âTRICAS E INTEGRACI√ìN CON EL AI GOVERNANCE FRAMEWORK

<br>

El programa de supply chain security de IA se integra con el AI Governance Framework establecido en la Fase 7 del proyecto. Las m√©tricas de supply chain security son KPIs del AI Governance Committee y complementan el scorecard de red teaming de la Fase 13.

<br>

| M√âTRICA                           | F√ìRMULA                                                                                                                      | TARGET                                                                                                                              | FUENTE                                                                                     |
| --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| Cobertura de AIBOM                | # sistemas de IA con AIBOM actualizado en los √∫ltimos 90 d√≠as / total sistemas IA en producci√≥n √ó 100                        | >90% cobertura. 100% para sistemas de alto riesgo.                                                                                  | Azure AI Foundry registry + SBOM pipeline CI/CD.                                           |
| Shadow AI Incidents               | # herramientas de IA no aprobadas detectadas (nuevas por mes). Trend: ¬ødescendente o ascendente?                             | Tendencia descendente. <5 nuevas detecciones/mes indica que el programa de awareness est√° funcionando.                              | Defender for Cloud Apps discovery. DLP alerts.                                             |
| API Key Hygiene Score             | # API keys en repos de c√≥digo / total API keys √ó 100 (inverso). O: % de API keys con menos de 90 d√≠as de edad.               | 0 API keys en repos de c√≥digo. 100% de API keys con rotaci√≥n activa. 0 API keys sin expiraci√≥n configurada.                         | GitHub Secret Scanning. Azure Key Vault audit logs.                                        |
| Model Verification Rate           | # modelos en producci√≥n que pasaron por el proceso de verificaci√≥n (Guardian/ClamAV + hash + registry) / total modelos √ó 100 | >95%. El 5% residual debe estar documentado con justificaci√≥n (modelos propios de Microsoft que no requieren verificaci√≥n externa). | Azure AI Model Catalog. Registro de aprobaciones.                                          |
| MCP Server Compliance             | # MCP servers en uso de la lista blanca aprobada / total MCP servers detectados en el entorno √ó 100                          | >95%. Cualquier MCP server fuera de la lista debe generar alerta inmediata.                                                         | Inventario de MCP servers (GitHub Copilot logs, Claude Code logs, VSCode extension audit). |
| Time to Detect LLMjacking         | Tiempo desde que comienza el uso an√≥malo de una API key hasta que se detecta la anomal√≠a                                     | <24 horas para detecci√≥n autom√°tica v√≠a billing alert. <1 hora para consumo masivo (>10x baseline).                                 | Azure Monitor billing alerts. Azure OpenAI usage dashboard.                                |
| Connector/Plugin Security Reviews | # plugins y conectores nuevos aprobados que pasaron por el proceso de security review / total nuevos √ó 100                   | 100%. Ning√∫n plugin/conector se activa sin review de seguridad.                                                                     | Registro de aprobaciones del AI Governance Committee.                                      |

<br>

üîó Integraci√≥n con fases anteriores del proyecto: Supply Chain Security (F14) + Red Teaming (F13) + Gobernanza Post-Implementaci√≥n (F7) + ROI/TCO (F9) forman el ciclo completo de seguridad operacional de IA: F7 establece el framework de gobernanza; F9 justifica financieramente las inversiones; F13 testa los sistemas desplegados; F14 asegura que lo que entra al ecosistema es confiable antes de que llegue al testing. Es un ciclo, no una secuencia.

<br>

## 14.9 REFERENCIAS (R361‚ÄìR390)

<br>

AI Bill of Materials (AIBOM) y Est√°ndares:

R361. Manifest Cyber, 'AI Bill of Materials (AIBOM): Transparency for AI Supply Chains', 2025. AIBOM como artefacto vivo operacional (no documento est√°tico). Componentes: datasets, modelos, dependencias, entornos de deployment. Frameworks aplicables: EU AI Act, NIST AI RMF, DoD AI security directives. manifestcyber.com/aibom.

R362. Mend.io, 'Creating an AI Bill of Materials (AI BOM) for Secure GenAI', octubre 2025. Componentes AIBOM: model metadata (arquitectura, versioning, provenance), training datasets, software dependencies, deployment environment. Beneficios: data leakage prevention, adversarial risk detection, model tampering visibility.

R363. Noma Security, 'Securing AI Systems Through Transparency: The Critical Role of AI Bill of Materials (AIBOM)', julio 2025. La r√°pida integraci√≥n de IA ha superado el desarrollo de frameworks de governance. AIBOM: est√°ndar en evoluci√≥n basado en CycloneDX MLBOM extension y SPDX v3.0 AI profile. noma.security.

R364. Palo Alto Networks, 'What Is an AI-BOM (AI Bill of Materials)?', 2025. McKinsey 2025: 88% de organizaciones usan IA regularmente en al menos una funci√≥n de negocio (vs. 78% a√±o anterior). EU AI Act GPAI obligations: transparencia obligatoria sobre datos de entrenamiento. paloaltonetworks.com/cyberpedia.

R365. SD Times, 'From SBOM to AI BOM: Rethinking supply chain security for AI native software', enero 2026. EU AI Act GPAI obligations (agosto 2025): reguladores pueden solicitar provenance audits, las reclamaciones de trade secret no ser√°n suficientes. Wiz: 25% de organizaciones no saben qu√© servicios de IA o datasets tienen activos. sdtimes.com.

R366. Diginomica, 'How an AI Bill of Materials could build trust in enterprise AI', diciembre 2025. CycloneDX v1.5: Machine Learning BOM. SPDX v3.0: AI profile. AIBOMs se convertir√°n en est√°ndar para AI-focused security. Desaf√≠o: AI systems son din√°micos, requieren lifecycle tracking continuo.

R367. Sonatype, 'How AI Governance Reduces Risk in Software Supply Chain Security', diciembre 2025. AIBOM como prerequisito de compliance. Automatizaci√≥n esencial: CI/CD integration para AIBOM generation. Sin automatizaci√≥n, los registros quedan obsoletos r√°pidamente.

R368. OWASP CycloneDX Project. CycloneDX v1.5 Machine Learning BOM specification. OWASP AIBOM project: guidance para AI transparency, auditability, accountability. cyclonedx.org.

R369. SPDX v3.0 AI Profile. Linux Foundation. AI profile fields: autonomy type, safety risk assessment, data preprocessing steps. Interoperable con CycloneDX. spdx.dev.

<br>

Riesgos de Modelos Open Source y Hugging Face:

R370. Protect AI + Hugging Face, '4M Models Scanned: 6 Months In', abril 2025. 4.47 millones de versiones √∫nicas escaneadas. 352,000 problemas de seguridad en 51,700 modelos. M√≥dulos de detecci√≥n: PAIT-ARV-100 (archive slip), PAIT-JOBLIB-101 (Joblib code execution), PAIT-TF-200 (TensorFlow architectural backdoor), PAIT-LMAFL-300 (Llamafile malicious code). CVE-2025-1550 en Keras. huggingface.co/blog/pai-6-month.

R371. ReversingLabs, 'Malicious ML Models on Hugging Face Platform', febrero 2025. T√©cnica 'NullifAI': payload al inicio del stream Pickle evade Picklescan. Modelos en PyTorch format (Pickle comprimido) usando 7z en lugar de ZIP para evadir el scanner. Hugging Face tags 'No issue' en modelos maliciosos. reversinglabs.com.

R372. Dark Reading, 'Open Source AI Models: Big Risks for Malicious Code, Vulnerabilities', febrero 2025. La apertura del ecosistema facilita la publicaci√≥n de modelos maliciosos. Empresas no deben depender de las medidas de seguridad de repositorios p√∫blicos para su propia seguridad. darkreading.com.

R373. Trend Micro, 'Exploiting Trust in Open-Source AI: The Hidden Supply Chain Risk No One Is Watching', 2025. JFrog: 400 modelos con c√≥digo malicioso de un mill√≥n analizado. Backdoored models: triggers estad√≠sticos, pr√°cticamente invisibles a an√°lisis est√°tico. Real-world incidents: Hugging Face (tokens expuestos, Wiz Research 2023), model poisoning v√≠a unvetted training pipelines. trendmicro.com.

R374. Palo Alto Networks Unit 42, 'Model Namespace Reuse: An AI Supply-Chain Attack Exploiting Model Name Trust', septiembre 2025. Caso DentalAI/toothfAIry: namespace reclamado por actor malicioso. Alcanza a usuarios de Azure AI Foundry Model Catalog y Vertex AI que consumen modelos de Hugging Face. unit42.paloaltonetworks.com.

R375. Infosecurity Magazine, 'Malicious AI Models on Hugging Face Exploit Novel Attack Technique', febrero 2025. Pickle: serializaci√≥n sin capacidad de seguridad por dise√±o. Picklescan: blacklist-based, no escalable. Alternativa: Safetensors (EleutherAI). infosecurity-magazine.com.

R376. Cisco Foundation AI, 'Cisco Foundation AI Advances AI Supply Chain Security with Hugging Face', agosto 2025. ClamAV 1.5: detecci√≥n de malware en .pt y .pkl (milliseconds). Cerberus: inspecci√≥n 24/7 en tiempo real. Cisco Secure Access: configuraci√≥n de acceso granular a Hugging Face basada en riesgo. blogs.cisco.com.

<br>

Shadow AI:

R377. CSO Online, 'Top 5 real-world AI security threats revealed in 2025', diciembre 2025. Survey 2,000 empleados EE.UU. + UK: 49% usa herramientas de IA no aprobadas por el empleador. M√°s de la mitad no entiende c√≥mo sus inputs son almacenados y analizados. Samsung incident (c√≥digo propietario en ChatGPT). JPMorgan, Goldman Sachs restricciones ChatGPT. csoonline.com.

R378. LayerX, 'Enterprise AI Usage Report 2025'. 77% de empleados enterprise que usa IA ha pegado datos corporativos en un chatbot. 22% de esas instancias inclu√≠an datos confidenciales personales o financieros.

R379. Reco AI, 'AI & Cloud Security Breaches: 2025 Year in Review', diciembre 2025. Shadow AI breaches: promedio USD 670,000 m√°s caro que incidentes tradicionales. Tiempo promedio de detecci√≥n: 247 d√≠as (vs. 241 para incidentes tradicionales). Afectan PII de clientes (65%) e IP (40%). 97% de organizaciones con AI-related breaches carec√≠an de controles b√°sicos de acceso. reco.ai.

R380. Netwrix, '12 Critical Shadow AI Security Risks Your Organization Needs to Monitor in 2026', febrero 2026. Netwrix Cybersecurity Trends Report 2025: 37% de organizaciones ajustaron estrategias de seguridad por amenazas de IA. MCP servers y LangChain tools como nuevo vector de Shadow AI. netwrix.com.

R381. Sombrainc, 'LLM Security Risks in 2026: Prompt Injection, RAG, and Shadow AI', enero 2026. Samsung incident: ingenieros pegaron c√≥digo propietario en ChatGPT para debug. Wall Street restrictions. ServiceNow second-order prompt injection entre agentes. sombrainc.com.

<br>

MCP Server Security:

R382. Wiz, 'MCP and LLM Security Research Briefing', abril 2025. 20,000+ MCP servers p√∫blicos. Local MCP server = ejecutar c√≥digo arbitrario. Supply chain risk: builders independientes sin est√°ndares de desarrollo seguro. Sin pinning, signing, o package locking en la especificaci√≥n actual. wiz.io/blog.

R383. Astrix Security, 'State of MCP Server Security 2025: Research Report', 2025. 20,000 implementaciones analizadas en GitHub. 88% requieren credenciales. 53% usan static API keys o PATs. Solo 8.5% OAuth. 79% de API keys via environment variables. MCP Secret Wrapper (open source): alternativa a credenciales est√°ticas. astrix.security.

R384. Fortune, 'AI coding tools exploded in 2025. The first security exploits show what could go wrong', diciembre 2025. CrowdStrike: LangFlow AI explotado para deployment de malware. Prompt injection en herramientas de AI coding: Cursor, GitHub Copilot, Google Gemini. MCP como vector de prompt injection. fortune.com.

R385. CSO Online, 'OpenClaw integrates VirusTotal malware scanning', febrero 2026. OpenClaw: 150,000 GitHub stars, primer AI agent security crisis de 2026. 21,000 instancias expuestas. Gartner: 'inaceptable cybersecurity liability'. ClawHavoc: exploits en el skills marketplace. csoonline.com.

<br>

LLMjacking y Credenciales de IA:

R386. CSO Online, 'Top 5 real-world AI security threats revealed in 2025 ‚Äî LLMjacking section', diciembre 2025. LLMjacking: robo de credenciales API de LLMs para acceder en nombre de la organizaci√≥n v√≠ctima. Microsoft demanda civil contra grupo especializado en LLMjacking. Costos estimados: >USD 100,000/d√≠a con modelos frontier. csoonline.com.

R387. Wiz Research, 'Leaking AI Secrets in Public Code', junio 2025. Escaneo de repositorios p√∫blicos: AI-related secret leaks masivos. Jupyter notebooks con API keys hardcodeadas son el vector m√°s com√∫n. wiz.io.

R388. OpenSSF Blog / IDC / Canonical / Google Cloud, 'Securing AI: The Next Cybersecurity Battleground', agosto 2025. Survey enterprise practitioners: mayor desaf√≠o en AI security ‚Äî falta de gu√≠as maduras para el ciclo de vida de IA (48.2% respondentes). openssf.org.

<br>

Frameworks de Supply Chain Security de IA:

R389. Hodeitek, 'CISO Playbook: AI Supply Chain Security Strategies, Risks, and Controls for 2025', noviembre 2025. AI supply chain = board-level issue. Extend SBOM to AIBOM (Model BOM): training data sources, preprocessing steps, libraries, model architectures, weight versions, known dependencies vulnerabilities. Third-party dataset due diligence: provenance, licensing, contractual clauses. hodeitek.com.

R390. Trend Micro, 'State of AI Security Report 1H 2025'. Redis vector database: use-after-free vulnerability explotable (Wiz Research). NVIDIA Container Toolkit: External Initialization of Trusted Variables. CoSAI (Coalition for Secure AI): model signing, machine readable model cards, incident response for AI, zero trust for AI, MCP security. trendmicro.com.

<br>

