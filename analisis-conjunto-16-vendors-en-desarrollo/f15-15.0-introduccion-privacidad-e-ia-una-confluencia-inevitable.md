---
icon: check
---

# F15 - 15.0 INTRODUCCI√ìN: PRIVACIDAD E IA ‚Äî UNA CONFLUENCIA INEVITABLE





**¬ß15.0 Introducci√≥n** ‚Äî La tensi√≥n estructural entre IA y privacidad: tabla comparativa de 6 dimensiones mostrando los riesgos sin privacy engineering vs. los controles t√©cnicos que lo mitigan (training, contexto LLM, corpus RAG, fine-tuning, outputs, logs). El marco regulatorio que lo obliga: GDPR Art. 25, EU AI Act Art. 10, Reforma 25.326 Argentina.

**¬ß15.1 Privacy-by-Design para IA** ‚Äî Adaptaci√≥n de los 7 principios de Cavoukian (1996, elevados a ley en GDPR) al contexto espec√≠fico de LLMs, RAG y agentes. El principio proactivo aplicado al DPIA de IA antes del deployment; el de configuraci√≥n predeterminada aplicado a Zero Data Retention y contexto m√≠nimo en Copilot; el de funcionalidad total respondiendo al mito de que privacidad = peor modelo.

**¬ß15.2 T√©cnicas de Privacy Preservation** ‚Äî Differential Privacy con detail: el par√°metro epsilon y sus valores en producci√≥n enterprise (Œµ=8.65 alineado con ISO/IEC 27559 y NIST SP 800-226), DP-SGD, las librer√≠as Opacus y TF Privacy, el trade-off privacidad-utilidad con datos cuantitativos. Federated Learning: los 4 tipos (horizontal, vertical, cross-device, FL+DP combinado) con casos reales 2025 incluyendo Zurich+Orange (30% mejora) y KAIST. Homomorphic Encryption y SMPC: estado 2025, overhead, Azure Confidential Computing como alternativa pr√°ctica.

**¬ß15.3 Anonimizaci√≥n, Pseudonimizaci√≥n y Synthetic Data** ‚Äî El continuo legal-t√©cnico entre las tres t√©cnicas con sus implicaciones bajo GDPR/Ley 25.326. El problema de los quasi-identifiers (Sweeney 1997: c√≥digo postal + g√©nero + fecha de nacimiento re-identifica al 87%). Machine Unlearning como respuesta t√©cnica al derecho al olvido: reentrenamiento exacto (viable para modelos propios peque√±os), approximate unlearning (investigaci√≥n activa), y la postura pr√°ctica de compliance cuando el unlearning exacto no es t√©cnicamente posible. Microsoft Presidio como herramienta operacional para anonimizaci√≥n de PII en corpus.

**¬ß15.4 Data Minimization en LLM y RAG** ‚Äî Las 5 dimensiones de minimizaci√≥n t√©cnica: contexto del LLM (purging autom√°tico, filtros de PII, separaci√≥n por clasificaci√≥n), corpus RAG (pol√≠ticas de retenci√≥n, exclusi√≥n de categor√≠as, auditor√≠a de permisos SharePoint), logs de interacciones (m√°ximo 90 d√≠as para contenido completo), outputs del LLM (Purview DLP, sensitivity labels en respuestas), y embeddings en vector database (access control, no indexar datos ultra-sensibles, encriptaci√≥n at-rest).

**¬ß15.5 Microsoft Purview para Privacidad de IA** ‚Äî Mapa completo de 8 capacidades Purview con descripci√≥n, relevancia para privacidad de IA, y estado de disponibilidad: DSPM for AI, Sensitivity Labels (MIP), DLP para Copilot, Audit Log, Insider Risk Management for AI, Compliance Manager (con templates EU AI Act y GDPR), Data Lifecycle Management, y Data Risk Assessments. El Copilot Oversharing Problem: el 8.5% de prompts enterprise expone datos sensibles, y el l√≠mite de Purview (controla acceso, no inferencia sem√°ntica).

**¬ß15.6 DPIA para Sistemas de IA** ‚Äî Cu√°ndo es obligatorio (6 criterios GDPR con ejemplos en contexto M365). El proceso completo de 8 pasos adaptado a sistemas de IA con outputs y evidencias para cada paso: descripci√≥n, necesidad/proporcionalidad, riesgos espec√≠ficos de IA (memorizaci√≥n, inferencias indeseadas, sesgo, reidentificaci√≥n, derecho al olvido inviable), controles existentes, plan de mitigaci√≥n, consulta con DPO, revisi√≥n continua, documentaci√≥n.

**¬ß15.7 Plan para CISO** ‚Äî Seis acciones concretas para los primeros 90 d√≠as: activar DSPM for AI, completar etiquetado MIP, implementar DLP en Copilot, ejecutar DPIAs para casos de alto riesgo, configurar retenci√≥n de logs, e inventariar todos los sistemas de IA. Programa maduro para a√±o 1 y 2027: privacy review como gate de deployment, auditor√≠a anual, capacitaci√≥n de 20,000 usuarios Copilot, Presidio para corpus RAG, proceso de ejercicio de derechos GDPR, y PETs para modelos propios.



## 15.0 INTRODUCCI√ìN: PRIVACIDAD E IA ‚Äî UNA CONFLUENCIA INEVITABLE

<br>

La privacidad y la inteligencia artificial comparten una tensi√≥n estructural: la IA se nutre de datos, y los datos de mayor valor son a menudo los m√°s sensibles desde una perspectiva de privacidad. Un LLM entrenado con millones de documentos puede haber memorizado fragmentos de informaci√≥n personal. Un sistema RAG que indexa el corpus documental corporativo puede exponer datos que el usuario tiene permiso de acceder en contexto pero no de ver directamente. Un agente de Copilot Studio con acceso al calendario ejecutivo puede inferir informaci√≥n sensible de patrones de reuniones.

El privacy engineering es la disciplina que responde a esta tensi√≥n: no prohibir el uso de datos ‚Äî que har√≠a la IA in√∫til ‚Äî sino dise√±ar sistemas de IA que maximizan la utilidad preservando la privacidad como propiedad t√©cnica, no como declaraci√≥n de intenciones. Para el CISO de 2026, el privacy engineering de IA es el complemento t√©cnico de los marcos regulatorios analizados en la Fase 11 (LATAM) y del stack de gobernanza de la Fase 7.

<br>

| DIMENSI√ìN                          | SIN PRIVACY ENGINEERING                                                                                                                                                                            | CON PRIVACY ENGINEERING                                                                                                                                                    |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Datos de entrenamiento             | El modelo puede memorizar y reproducir datos personales del corpus de entrenamiento (membership inference attack exitoso).                                                                         | Differential Privacy en training: garant√≠a matem√°tica de que la contribuci√≥n de ning√∫n individuo es identificable en el modelo final.                                      |
| Contexto del LLM en sesi√≥n         | El LLM tiene acceso a todo el contexto de la sesi√≥n: emails anteriores, documentos abiertos, historial de conversaci√≥n ‚Äî puede combinarlos de formas no intuitivas.                                | Data Minimization: el contexto se construye solo con los datos estrictamente necesarios para la tarea. Purging autom√°tico del contexto entre sesiones.                     |
| Corpus RAG                         | El knowledge base RAG puede contener documentos de distintas clasificaciones. Una consulta puede recuperar documentos que el usuario puede acceder individualmente pero no deber√≠a ver combinados. | Segmentaci√≥n de RAG por clasificaci√≥n. Access control en vector database. Sensitivity labels que fluyen desde SharePoint/Purview al contexto del LLM.                      |
| Datos de entrenamiento fine-tuning | Si la empresa fine-tunea un modelo con datos propios (conversaciones de clientes, contratos), esos datos pueden ser extra√≠bles del modelo resultante.                                              | Federated Learning y Differential Privacy para fine-tuning. AIBOM que documenta qu√© datos se usaron. Derecho al olvido: c√≥mo eliminar un individuo del modelo.             |
| Outputs del sistema                | El LLM puede generar outputs que revelan PII de terceros, especialmente si proces√≥ documentos con datos de m√∫ltiples personas.                                                                     | PII detection en outputs (Microsoft Purview DLP en prompts y respuestas). Guardrails de output que sanitizan informaci√≥n personal antes de mostrarla al usuario.           |
| Logs y auditor√≠a                   | Los logs de interacciones con el LLM contienen el contenido completo de prompts y respuestas ‚Äî fuente de PII que muchas organizaciones retienen indefinidamente.                                   | Pol√≠tica de retenci√≥n de logs de IA (Purview Data Lifecycle Management). Anonimizaci√≥n de logs para analytics. Solo retener metadatos (no contenido) para an√°lisis de uso. |

<br>

üìã Marco regulatorio que obliga el privacy engineering: EU AI Act Art. 10 (Datos de entrenamiento): sistemas de alto riesgo deben garantizar que los datos tienen la calidad y representatividad adecuada, y que se aplican medidas de privacidad apropiadas. GDPR Art. 25 (Privacy-by-Design): la privacidad debe incorporarse en el dise√±o del sistema desde el inicio, no a√±adirse despu√©s. Argentina Ley 25.326 Reforma 2025: tratamiento automatizado de datos personales requiere evaluaciones de impacto. El penalty m√°ximo bajo GDPR: 4% revenue global o EUR 20M. El costo de privacidad-por-dise√±o es sistem√°ticamente menor que el costo de privacidad-por-remediaci√≥n.

<br>

## 15.1 PRIVACY-BY-DESIGN APLICADO A SISTEMAS DE IA

<br>

Privacy-by-Design (PbD), formulado por Ann Cavoukian en los a√±os 90 y elevado a est√°ndar en GDPR Art. 25, establece 7 principios fundacionales. Su aplicaci√≥n a sistemas de IA requiere una reinterpretaci√≥n: los principios fueron dise√±ados para sistemas de informaci√≥n convencionales. La naturaleza no determin√≠stica, la capacidad de inferencia y la memorizaci√≥n de los LLMs crean desaf√≠os que Cavoukian no anticip√≥. Esta secci√≥n adapta los 7 principios al contexto espec√≠fico de LLMs, RAG, agentes y modelos de ML empresariales.

<br>

| # | PRINCIPIO PbD                                                             | APLICACI√ìN CL√ÅSICA                                                                                                                       | ADAPTACI√ìN ESPEC√çFICA PARA IA                                                                                                                                                                                                                                                                                                                                                                                                                            |
| - | ------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1 | Proactivo, no Reactivo ‚Äî Preventivo, no Correctivo                        | Identificar y abordar riesgos de privacidad antes de que ocurran, no responder a violaciones despu√©s.                                    | Para IA: el DPIA (Data Protection Impact Assessment) de IA debe realizarse antes de entrenar el modelo o indexar el corpus RAG, no tras el deployment. Threat model de privacidad: ¬øqu√© inferencias indeseadas puede hacer el modelo? ¬øqu√© datos puede memorizar? La brecha entre 'dise√±ado para' y 'puede hacer' es el espacio de riesgo no anticipado.                                                                                                 |
| 2 | Privacidad como Configuraci√≥n Predeterminada                              | El sistema m√°s restrictivo de privacidad debe ser el comportamiento por defecto. Los usuarios deben optar por compartir m√°s, no menos.   | Para IA: Copilot con Zero Data Retention por defecto, no como opci√≥n opt-in. El contexto del LLM debe incluir el m√≠nimo de datos por defecto ‚Äî el usuario puede ampliar el contexto expl√≠citamente. Los agentes de Copilot Studio deben tener herramientas deshabilitadas por defecto y habilitarse solo cuando el usuario las solicita expl√≠citamente.                                                                                                  |
| 3 | Privacidad Incorporada al Dise√±o                                          | La privacidad no es un complemento sino un componente central de la arquitectura del sistema.                                            | Para IA: la separaci√≥n entre el corpus RAG y las interacciones de usuario no es una feature sino un requisito de dise√±o. La arquitectura de segmentaci√≥n por clasificaci√≥n de datos no se agrega despu√©s del deployment ‚Äî se dise√±a en el pipeline de indexaci√≥n. Differential Privacy en entrenamiento es una decisi√≥n de arquitectura, no un parche.                                                                                                   |
| 4 | Funcionalidad Total ‚Äî Suma Positiva, no Suma Cero                         | La privacidad y la funcionalidad no son mutuamente excluyentes. Es posible maximizar ambas.                                              | Para IA: Federated Learning como caso emblem√°tico ‚Äî modelo entrenado sobre datos de m√∫ltiples organizaciones sin que ninguna comparta sus datos con las otras. Synthetic Data que preserva las propiedades estad√≠sticas del dataset real sin exponer individuos reales. El mito de que privacidad = menor calidad del modelo es rebatido por la investigaci√≥n: los modelos con DP muestran degradaci√≥n modest cuando epsilon es calibrado correctamente. |
| 5 | Seguridad de Extremo a Extremo ‚Äî Protecci√≥n Durante Todo el Ciclo de Vida | Los datos deben estar protegidos desde su recolecci√≥n hasta su eliminaci√≥n.                                                              | Para IA: el ciclo de vida del dato en IA incluye etapas inexistentes en sistemas tradicionales: preprocesamiento para training, representaci√≥n como embedding, almacenamiento en vector database, uso como contexto en inferencia, aparici√≥n en outputs. Privacy engineering debe cubrir cada etapa. Cuando un individuo ejerce su derecho al olvido bajo GDPR, ¬øc√≥mo se elimina su dato de un modelo ya entrenado? (Machine Unlearning ‚Äî ver 15.3)      |
| 6 | Visibilidad y Transparencia                                               | El sistema debe ser transparente sobre c√≥mo procesa datos. Los usuarios deben poder verificar que la privacidad es real, no declarativa. | Para IA: el AI Transparency Report (Fase 3 del proyecto) es la expresi√≥n institucional de este principio. A nivel t√©cnico: model cards que documentan qu√© datos se usaron en entrenamiento, qu√© medidas de privacidad se aplicaron, qu√© evaluaciones de bias se realizaron. Para Copilot: el Audit Log de Microsoft Purview que permite verificar qu√© datos proces√≥ el LLM en cada interacci√≥n.                                                          |
| 7 | Respeto por la Privacidad del Usuario ‚Äî Centrado en el Usuario            | El sistema debe respetar los intereses del usuario y ofrecer herramientas de control efectivo.                                           | Para IA: el derecho de acceso bajo GDPR debe incluir la capacidad de saber qu√© datos propios proces√≥ el LLM. El derecho de rectificaci√≥n debe incluir la correcci√≥n de datos en el corpus RAG. El derecho al olvido debe incluir un mecanismo t√©cnico de eliminaci√≥n del modelo (Machine Unlearning). Ninguno de estos derechos tiene una implementaci√≥n trivial en sistemas de IA ‚Äî son el estado del arte de la investigaci√≥n en 2025-2026.            |

<br>

## 15.2 T√âCNICAS DE PRIVACY PRESERVATION: DP, FEDERATED LEARNING, HE, SMPC

<br>

El privacy engineering de IA opera con un conjunto de t√©cnicas que tienen fundamento matem√°tico ‚Äî no son pol√≠ticas o declaraciones de intenci√≥n, sino mecanismos t√©cnicos que proveen garant√≠as formales y cuantificables de privacidad. Esta secci√≥n documenta las cuatro t√©cnicas fundamentales con su aplicabilidad concreta para el CISO con ecosistema Microsoft.

<br>

### 15.2.1 Differential Privacy (DP) ‚Äî La T√©cnica con Mayor Adopci√≥n Enterprise

Differential Privacy es una definici√≥n matem√°tica de privacidad propuesta por Cynthia Dwork (Microsoft Research) en 2006. Su garant√≠a fundamental: el output de un algoritmo DP es pr√°cticamente indistinguible independientemente de si un individuo espec√≠fico estuvo o no en el dataset de entrenamiento. La privacidad est√° garantizada para cualquier atacante, con cualquier informaci√≥n auxiliar, incluso en el futuro.

| COMPONENTE DP                                        | DESCRIPCI√ìN APLICADA A IA ENTERPRISE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ---------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Par√°metro Epsilon (Œµ) ‚Äî El presupuesto de privacidad | El par√°metro epsilon cuantifica la privacidad: epsilon=0 es privacidad perfecta (modelo que no aprende nada). Epsilon grande significa menos privacidad. Valores t√≠picos en producci√≥n enterprise: Œµ=1 (alta privacidad, degradaci√≥n de utilidad del 5-10%), Œµ=8 (privacidad razonable, degradaci√≥n menor al 2%), Œµ=14 (compliance con ISO/IEC 27559 y NIST SP 800-226, degradaci√≥n m√≠nima). Un estudio de 2026 en credit risk models mostr√≥ que Œµ entre 5.74 y 14.13 es compatible con GDPR Privacy-by-Design, ISO/IEC 27559, y NIST SP 800-226. |
| Implementaci√≥n en Training (DP-SGD)                  | El algoritmo DP-SGD (Differentially Private Stochastic Gradient Descent) aplica DP durante el training: en cada paso de gradient descent, el gradiente de cada ejemplo se recorta a un norm m√°ximo (clipping) y luego se a√±ade ruido gaussiano calibrado para garantizar el presupuesto epsilon. TensorFlow Privacy (Google) y Opacus (PyTorch/Meta) son las librer√≠as open-source est√°ndar. Para modelos fine-tuned sobre Azure: el entrenamiento puede ejecutarse con Opacus integrado en el pipeline de Azure ML.                              |
| Implementaci√≥n en Queries e Inferencia (Local DP)    | En sistemas que no pueden aplicar DP en entrenamiento (modelos base propietarios), se aplica DP a nivel de queries y outputs: antes de reportar estad√≠sticas agregadas de uso del sistema, se a√±ade ruido calibrado. Los query results de analytics sobre logs del LLM se suministran con DP: el CISO puede saber el % de usuarios que usan Copilot para documentos financieros sin poder identificar a ning√∫n usuario espec√≠fico.                                                                                                                |
| Trade-off Privacidad-Utilidad                        | DP introduce degradaci√≥n en la calidad del modelo. El costo depende del tama√±o del dataset (m√°s datos = menor degradaci√≥n por DP), del valor de epsilon elegido, y de la tarea. Regla pr√°ctica: para datasets enterprise con >100,000 ejemplos, Œµ=8 introduce degradaci√≥n en accuracy de <2%. Para datasets peque√±os (<10,000 ejemplos), la degradaci√≥n puede ser significativa y requiere compensaci√≥n con t√©cnicas como transfer learning desde modelos pre-entrenados con DP.                                                                  |
| Adopci√≥n en Ecosistema Microsoft                     | Microsoft fue el inventor de la teor√≠a de Differential Privacy (Cynthia Dwork, MSR). Microsoft implementa DP en Windows Telemetry y en Azure Synapse Analytics para exportaci√≥n de estad√≠sticas. TensorFlow Privacy (desarrollado parcialmente en colaboraci√≥n con Microsoft) soporta Azure ML. Azure Data Factory tiene capacidades de data masking basadas en principios de DP para pipelines de datos. Microsoft Presidio: herramienta open-source de anonimizaci√≥n PII que combina detection (NLP) con t√©cnicas de sustituci√≥n configurables. |

<br>

### 15.2.2 Federated Learning ‚Äî Entrenar Modelos Sin Mover Datos

Federated Learning (FL) invierte el paradigma del machine learning tradicional: en lugar de traer los datos al modelo, lleva el modelo a los datos. Solo los updates del modelo ‚Äî nunca los datos crudos ‚Äî viajan a un servidor central. Propuesto por Google en 2017, es hoy la t√©cnica de privacy preservation con mayor adopci√≥n enterprise, especialmente en sectores con restricciones regulatorias estrictas sobre movimiento de datos.

| TIPO FL                        | ARQUITECTURA                                                                                                                                                                  | CASO DE USO ENTERPRISE                                                                                                                                                                                      | EJEMPLO REAL 2025                                                                                                                                                                                          |
| ------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Horizontal FL(Cross-Silo)      | M√∫ltiples organizaciones con mismas features pero distintos usuarios. Cada organizaci√≥n entrena localmente, comparte solo gradientes.                                         | Bancos colaborando en modelo de detecci√≥n de fraude sin compartir transacciones de clientes. Hospitales entrenando modelo diagn√≥stico sin compartir historias cl√≠nicas.                                     | Zurich Insurance + Orange Telecom: modelo de predicci√≥n FL sobre datos de Orange sin acceso a ellos. Resultado: 30% mejora en predicciones vs. modelo individual.                                          |
| Vertical FL                    | Organizaciones con distintas features pero los mismos usuarios. Combina features de distintas fuentes sin revelar cada fuente al otro.                                        | Banco + retailer + telco sobre mismos clientes: el banco tiene historial crediticio, el retailer tiene patrones de compra, la telco tiene uso de datos. FL vertical sin que ninguno vea los datos del otro. | Credit scoring con datos alternativos: banco + empresa de telecomunicaciones entrenan modelo conjunto. El banco mejora predicci√≥n de default sin acceder a datos de telco.                                 |
| Cross-Device FL                | Modelo entrenado en miles/millones de dispositivos edge. Solo los updates del modelo suben al servidor.                                                                       | Mejora de teclado predictivo (Google GBoard), personalizaci√≥n de modelos de voz, detecci√≥n de intrusiones en endpoints.                                                                                     | Google Keyboard usa FL para personalizaci√≥n de predicci√≥n sin enviar texto del usuario a Google. Apple usa FL para Siri improvements. Relevante para modelos de endpoint security de M365.                 |
| FL + DP Combinado(Recomendado) | FL protege contra el servidor central que ve datos crudos. DP a√±adida sobre los gradientes de FL protege contra membership inference attacks sobre los gradients compartidos. | El est√°ndar de facto para deployments enterprise con requisitos de compliance: FL + DP garantiza privacidad incluso si el servidor es comprometido o el atacante tiene acceso a los gradientes.             | Estudio Nature Scientific Reports 2026: FL+DP para credit risk en escenario bancario real. DP-FL con Œµ=8.65 alineado con GDPR, ISO/IEC 27559, NIST SP 800-226. Degradaci√≥n de accuracy: <3% vs. FL sin DP. |

<br>

### 15.2.3 Homomorphic Encryption y SMPC ‚Äî Computaci√≥n sobre Datos Cifrados

| T√âCNICA                               | C√ìMO FUNCIONA Y GARANT√çA                                                                                                                                                                                                                                                                                                                                                                                                 | ESTADO 2025-2026 Y APLICABILIDAD ENTERPRISE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Homomorphic Encryption (HE)           | Permite realizar operaciones matem√°ticas sobre datos cifrados sin necesidad de descifrarlos. El resultado cifrado, al descifrarse, es igual al resultado que se habr√≠a obtenido sobre los datos en plaintext. Garant√≠a: el servidor de ML nunca ve los datos en plaintext ‚Äî ni durante la inferencia ni durante el entrenamiento. Esquemas: CKKS (para operaciones de punto flotante, m√°s relevante para ML), BFV, BGWE. | 2025: operacionalmente viable para inferencia sobre modelos peque√±os. Para training, el overhead computacional es a√∫n 10x-1000x mayor que sin HE. Microsoft SEAL (librer√≠a open-source de HE) es el est√°ndar. Azure Confidential Computing integra enclaves hardware (Intel TDX) como alternativa m√°s pr√°ctica a HE pura para cargas de trabajo enterprise. Aplicaci√≥n m√°s real 2025: inferencia sobre modelos de clasificaci√≥n en contextos regulados (HIPAA) donde el servidor cloud no debe ver los datos del paciente. |
| Secure Multi-Party Computation (SMPC) | Permite que m√∫ltiples partes computen una funci√≥n conjunta sobre sus inputs combinados sin revelar sus inputs individuales a ninguna otra parte. Ejemplo cl√°sico: tres bancos computan el promedio de sus tasas de default sin que ninguno sepa las tasas individuales de los otros. PySyft (OpenMined) es la librer√≠a open-source de referencia para SMPC en ML.                                                        | 2025: madurez creciente pero overhead significativo para modelos grandes. Viable para agregaciones estad√≠sticas entre organizaciones (variante: Privacy Set Intersection para encontrar usuarios comunes entre dos bases de datos sin revelar cu√°les son). Microsoft Research activo en esta √°rea. Aplicaci√≥n enterprise 2025: combinaci√≥n de signals de threat intelligence entre organizaciones del mismo sector sin revelar sus incidentes individuales (fraud networks collaboration).                                 |
| Confidential Computing (SGX/TDX)      | No es privacy preservation en el sentido del ML sino un control complementario: el c√≥digo y los datos que procesa se ejecutan en un enclave hardware verificable ‚Äî ni el cloud provider ni el administrador del sistema pueden ver el contenido del enclave. Intel TDX (Trust Domain Extensions), AMD SEV-SNP.                                                                                                           | Disponible en Azure hoy: Azure Confidential VMs con Intel TDX, Azure Confidential Containers. El 'attestation report' permite al tenant verificar que su modelo y sus datos se ejecutan en un enclave genuino sin backdoors del cloud provider. Para el CISO que no conf√≠a completamente en el cloud provider para datos ultra-sensibles: Confidential Computing es la opci√≥n m√°s pr√°ctica y madura hoy (vs. HE que a√∫n tiene overhead prohibitivo para LLMs).                                                             |

<br>

## 15.3 ANONIMIZACI√ìN, PSEUDONIMIZACI√ìN Y SYNTHETIC DATA PARA IA

<br>

Las tres t√©cnicas de esta secci√≥n abordan el mismo problema desde distintos √°ngulos: c√≥mo usar datos que contienen informaci√≥n personal para entrenar modelos de IA sin comprometer la privacidad de los individuos en esos datos. La distinci√≥n entre ellas tiene implicaciones legales directas bajo GDPR y la Ley 25.326: solo la anonimizaci√≥n exitosa saca los datos del scope del derecho de privacidad ‚Äî la pseudonimizaci√≥n y los datos sint√©ticos mal generados siguen siendo datos personales.

<br>

### 15.3.1 El Continuo Anonimizaci√≥n-Pseudonimizaci√≥n-Sint√©tico

| T√âCNICA          | DEFINICI√ìN Y GARANT√çA                                                                                                                                                                                                                                                                                                      | LIMITACIONES PARA IA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | STATUS REGULATORIO GDPR / Ley 25.326                                                                                                                                                                                                                                                                                                                                                                               |
| ---------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Anonimizaci√≥n    | Proceso irreversible de transformaci√≥n de datos personales tal que la re-identificaci√≥n del individuo sea imposible para cualquier atacante con cualquier informaci√≥n auxiliar. T√©cnicas: k-anonymity (cada individuo es indistinguible de al menos k-1 otros), l-diversity, t-closeness.                                  | Para IA: la anonimizaci√≥n agresiva destruye el valor estad√≠stico del dataset ‚Äî especialmente para variables correlacionadas. Los modelos entrenados con datos fuertemente anonimizados muestran degradaci√≥n severa. El problema de quasi-identifiers: variables aparentemente inocuas (c√≥digo postal + g√©nero + fecha de nacimiento) permiten re-identificar al 87% de la poblaci√≥n de EE.UU. (Sweeney 1997). En el contexto de LLMs, los embeddings de texto pueden actuar como quasi-identifiers. | FUERA DEL SCOPE: datos genuinamente an√≥nimos no son datos personales bajo GDPR ni Ley 25.326. El CISO no necesita base legal para procesarlos. El reto: probar que la anonimizaci√≥n es genuina ‚Äî los supervisores de protecci√≥n de datos exigen evidencia t√©cnica, no solo declaraci√≥n.                                                                                                                            |
| Pseudonimizaci√≥n | Sustituci√≥n de identificadores directos (nombre, DNI, email) por pseud√≥nimos (tokens, IDs sint√©ticos) manteniendo el resto de los atributos. El proceso es reversible con la clave de pseudonimizaci√≥n.                                                                                                                    | Para IA: es √∫til para reducir el riesgo de re-identificaci√≥n pero no lo elimina ‚Äî los quasi-identifiers siguen presentes. Un LLM puede re-identificar a un individuo a partir de la combinaci√≥n de atributos incluso sin los identificadores directos. El 2025 study on enterprise prompt data encontr√≥ que 8.5% de prompts en Copilot/ChatGPT expon√≠an datos sensibles incluyendo employee PII (26.8% del total de exposiciones).                                                                  | DENTRO DEL SCOPE: es un dato personal bajo GDPR y Ley 25.326. Reduce el riesgo (GDPR Art. 89 permite excepciones para investigaci√≥n con datos pseudonimizados) pero no elimina las obligaciones. El CISO debe mantener la tabla de correspondencia pseud√≥nimo-identidad con controles estrictos.                                                                                                                   |
| Datos Sint√©ticos | Datos artificialmente generados que replican las propiedades estad√≠sticas del dataset real sin contener registros reales de individuos. Generados con GANs (Generative Adversarial Networks), VAEs, o LLMs. Los datos sint√©ticos preservan las distribuciones estad√≠sticas, correlaciones y patrones del dataset original. | Para IA: calidad variable seg√∫n el m√©todo de generaci√≥n. El riesgo de memorizaci√≥n: un generativo (GAN, LLM) puede memorizar registros del training dataset y reproducirlos. KAIST 2025: m√©todo que usa representaciones sint√©ticas en lugar de datos reales para FL, permitiendo colaboraci√≥n entre hospitales y bancos sin sharing de datos reales.                                                                                                                                               | STATUS COMPLEJO: depende del m√©todo de generaci√≥n. Si el generativo fue entrenado con DP, los datos sint√©ticos heredan la garant√≠a DP. Si no: los datos sint√©ticos pueden ser datos personales (si permiten re-identificaci√≥n o si el generativo memoriz√≥ datos reales). El art√≠culo 29 Working Party (WP29) no ha dado una posici√≥n definitiva. Tratar como datos personales hasta que se demuestre lo contrario. |

<br>

### 15.3.2 Machine Unlearning ‚Äî El Derecho al Olvido en Modelos de IA

El derecho al olvido (GDPR Art. 17, Ley 25.326 Art. 16) establece que un individuo puede solicitar la eliminaci√≥n de sus datos. En sistemas de informaci√≥n tradicionales, esto significa borrar el registro de una base de datos. En un modelo de ML ya entrenado, el equivalente es t√©cnicamente dif√≠cil: el modelo 'record√≥' el dato durante el entrenamiento y ese aprendizaje est√° distribuido en millones de par√°metros. Machine Unlearning es la disciplina que busca eliminar la influencia de un dato espec√≠fico de un modelo ya entrenado sin reentrenarlo desde cero.

| ENFOQUE                                    | DESCRIPCI√ìN T√âCNICA                                                                                                                                                                                                                                                                                                                                                                                                                      | VIABILIDAD 2025-2026                                                                                                                                                                                                                                                                                                                                                                                                           |
| ------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Reentrenamiento Exacto(Exact Unlearning)   | Reentrenar el modelo completo desde cero excluyendo los datos del individuo que solicita el olvido. Garant√≠a perfecta: el nuevo modelo nunca vio esos datos.                                                                                                                                                                                                                                                                             | Solo viable cuando el dataset de training est√° controlado, el modelo tiene un costo de training razonable, y las solicitudes de olvido son infrecuentes. Para modelos fine-tuned sobre Azure ML: viable si el dataset de fine-tuning est√° documentado (AIBOM). Para modelos base (GPT-4o, Claude): imposible ‚Äî el CISO no tiene acceso al training del proveedor.                                                              |
| Approximate Unlearning(Newton Step)        | Actualizar los par√°metros del modelo con un gradiente dise√±ado para deshacer el aprendizaje de los datos espec√≠ficos, sin reentrenar desde cero. T√©cnicas: gradient ascent sobre los datos a olvidar, influence function-based unlearning.                                                                                                                                                                                               | Estado del arte 2025: funciona razonablemente bien para modelos peque√±os y datos claramente memoritzados. Para LLMs de escala masiva: la investigaci√≥n est√° activa (Apple, Google, Microsoft Research) pero no hay producci√≥n estable a√∫n. El CISO debe documentar que para modelos base de proveedores, el derecho al olvido t√©cnico no es actualmente realizable ‚Äî esto tiene implicaciones para la base legal del training. |
| S√≠ntesis de Pol√≠tica(Practical Compliance) | En ausencia de unlearning t√©cnico viable, pol√≠tica de compliance: (1) No usar datos personales identificables para training sin base legal s√≥lida. (2) Usar datos an√≥nimos o sint√©ticos para training. (3) Para fine-tuning con datos propios: mantener AIBOM, dise√±ar el pipeline para permitir reentrenamiento selectivo. (4) Transparencia con los titulares: si el unlearning exacto no es posible, informar y ofrecer alternativas. | Recomendado como postura pr√°ctica para el CISO hoy (2026). La pol√≠tica combina: base legal s√≥lida para training (leg√≠timo inter√©s o consentimiento), minimizaci√≥n de datos personales en training (preferir datos sint√©ticos), AIBOM que documenta qu√© datos se usaron, y proceso de evaluaci√≥n de solicitudes de olvido caso a caso.                                                                                          |

<br>

üîë Herramienta pr√°ctica: Microsoft Presidio: Microsoft Presidio es una herramienta open-source (Apache 2.0) para detecci√≥n y anonimizaci√≥n de PII en texto e im√°genes. Usa NLP (spaCy) para reconocimiento de entidades (nombres, DNI, emails, n√∫meros de tarjeta, coordenadas GPS, etc.) y permite elegir el tipo de sustituci√≥n: redacci√≥n, tokenizaci√≥n, enmascaramiento, cifrado, o s√≠ntesis de valores plausibles. Integrable en pipelines de Azure ML para pre-procesar datos antes de indexaci√≥n en RAG o antes de fine-tuning. Disponible en PyPI: pip install presidio-analyzer presidio-anonymizer. Soporta espa√±ol, ingl√©s, alem√°n, italiano, portugu√©s ‚Äî relevante para corpus en Argentina y LATAM.

<br>

## 15.4 DATA MINIMIZATION EN CONTEXTOS LLM Y RAG

<br>

Data Minimization (GDPR Art. 5(1)(c)) establece que los datos personales deben ser adecuados, pertinentes y limitados a lo necesario para los fines para los que son procesados. En el contexto de LLMs y sistemas RAG, este principio tiene implicaciones t√©cnicas concretas que van m√°s all√° de las pol√≠ticas: determinan qu√© entra en el contexto del modelo, cu√°nto tiempo se retiene, y qu√© se puede inferir.

<br>

| DIMENSI√ìN DE MINIMIZACI√ìN         | RIESGO SIN MINIMIZACI√ìN                                                                                                                                                                                                                                                                                                                                                                                                            | CONTROL T√âCNICO DE MINIMIZACI√ìN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Contexto del LLM (Context Window) | El contexto del LLM incluye el historial completo de la conversaci√≥n, documentos relevantes recuperados por RAG, informaci√≥n de perfil del usuario. Sin minimizaci√≥n: el contexto puede incluir emails de terceros, datos salariales de compa√±eros, conversaciones pasadas irrelevantes para la consulta actual. La combinaci√≥n de datos en el contexto permite al LLM hacer inferencias que ninguna fuente individual permitir√≠a. | (1) RAG con re-ranking sem√°ntico: incluir solo los chunks con similarity score mayor al threshold configurado. (2) Purging autom√°tico del historial de conversaci√≥n tras un n√∫mero configurable de turnos o tras el cierre de sesi√≥n. (3) Filtros de contexto que eliminen PII del historial antes de incluirlo en el prompt del LLM. (4) Separaci√≥n por clasificaci√≥n: el contexto de un usuario de Nivel de Acceso 3 no incluye autom√°ticamente documentos de Nivel 4 aunque est√© en el contexto de la sesi√≥n. |
| Corpus RAG ‚Äî Indexaci√≥n           | El corpus RAG indexa todo lo que se le indica: si SharePoint tiene 10 a√±os de emails corporativos, el RAG puede recuperar emails de una persona que ya sali√≥ de la empresa, datos de proyectos cancelados, informaci√≥n de clientes de contratos ya terminados.                                                                                                                                                                     | (1) Pol√≠ticas de retenci√≥n para el corpus RAG que repliquen las pol√≠ticas de retenci√≥n del dato original (Purview Data Lifecycle Management). (2) Exclusi√≥n de categor√≠as de datos del corpus: datos de RRHH, datos de clientes de contratos cerrados, informaci√≥n personal de emails personales. (3) Revisi√≥n de permisos de SharePoint antes de indexaci√≥n: el RAG no debe indexar contenido al que el usuario no deber√≠a acceder directamente.                                                                |
| Logs de Interacciones con el LLM  | Los logs de conversaciones con Copilot/LLM contienen el contenido completo de prompts y respuestas ‚Äî incluida la PII que el usuario ingres√≥ o recibi√≥. Sin pol√≠tica de retenci√≥n, se acumulan indefinidamente. El log es una fuente de PII con alto riesgo de re-identificaci√≥n.                                                                                                                                                   | (1) Retenci√≥n de logs de interacciones IA: m√°ximo 90 d√≠as para logs completos (contenido), indefinido para metadatos (timestamp, user, tokens usados). (2) Anonimizaci√≥n de logs antes de transferir a analytics: sustituci√≥n de nombres y datos identificables con tokens. (3) Separaci√≥n entre logs de seguridad (auditables, con contenido) y logs de uso/analytics (anonimizados). (4) Purview Data Lifecycle Management: pol√≠ticas autom√°ticas de borrado de logs de Copilot seg√∫n configuraci√≥n.           |
| Respuestas del LLM ‚Äî Output PII   | El LLM puede incluir PII de terceros en sus respuestas: nombres de empleados en documentos que resume, datos salariales en tablas que analiza, informaci√≥n de clientes en emails que procesa.                                                                                                                                                                                                                                      | (1) Purview DLP en outputs de Copilot: pol√≠tica que detecta PII en respuestas del LLM y la redacta antes de mostrar al usuario. (2) Sensitivity labels: si el documento fuente tiene label Confidential, la respuesta del LLM que incluye su contenido hereda la clasificaci√≥n. (3) Guardrails de output espec√≠ficos para PII: Microsoft Azure AI Content Safety con categor√≠as de PII configuradas.                                                                                                             |
| Embeddings en Vector Database     | Los embeddings de texto preservan informaci√≥n sem√°ntica del texto original. Ataques de embedding inversion pueden reconstruir texto aproximado del original desde el embedding ‚Äî especialmente para textos cortos como nombres, emails, o datos de tarjetas.                                                                                                                                                                       | (1) Access control en la vector database (Azure AI Search): solo los roles con acceso al documento original pueden recuperar sus embeddings. (2) No indexar datos de alta sensibilidad (credenciales, datos m√©dicos, datos financieros personales) directamente como texto en el RAG ‚Äî usar referencias a sistemas seguros en su lugar. (3) Encriptaci√≥n at-rest de los embeddings en Azure AI Search.                                                                                                           |

<br>

## 15.5 MICROSOFT PURVIEW: EL STACK DE PRIVACIDAD PARA IA EN M365

<br>

Microsoft Purview es la plataforma de seguridad, gobernanza y compliance de datos de Microsoft ‚Äî y desde 2024-2025 se ha transformado en la herramienta central para gestionar la privacidad de datos en el contexto de Copilot, agentes y aplicaciones de IA enterprise. Para el CISO con 20,000+ licencias M365 E5, Purview no es una herramienta adicional: est√° incluida en la licencia y es el mecanismo primario de control de privacidad de IA disponible hoy.

<br>

### 15.5.1 Mapa de Capacidades Purview para Privacidad de IA

| CAPACIDAD PURVIEW                                     | QU√â HACE                                                                                                                                                                                                                                         | RELEVANCIA PARA PRIVACIDAD DE IA                                                                                                                                                                                                                                     | DISPONIBILIDAD                                                                 |
| ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| Data Security Posture Management for AI (DSPM for AI) | Visibility central sobre qu√© aplicaciones de IA usa la organizaci√≥n (M365 Copilot, ChatGPT, Gemini, etc.), qu√© datos sensibles fluyen hacia ellas, y cu√°les son los riesgos de oversharing. Dashboard con recomendaciones y pol√≠ticas one-click. | El 'front door' de la privacidad de IA: sin visibilidad no hay gesti√≥n. Identifica sitios SharePoint con datos sobre-compartidos que el Copilot puede amplificar. Detecta prompts que incluyen PII siendo enviados a apps de IA no aprobadas.                        | GA. Disponible en Purview portal. Incluido con M365 E5.                        |
| Sensitivity Labels (Microsoft Information Protection) | Etiquetas de clasificaci√≥n aplicadas a documentos, emails, y contenido M365 que controlan qu√© puede hacer el LLM con el contenido etiquetado. Permite configurar qu√© labels impiden que Copilot resuma o procese el contenido.                   | El mecanismo central de Data Minimization para RAG: documentos con labels de alta sensibilidad no son indexados por RAG o Copilot sin autorizaci√≥n expl√≠cita. Los labels fluyen con el dato: un documento Confidential mantiene ese label aunque Copilot lo procese. | GA. Requiere configuraci√≥n de labels y pol√≠ticas.                              |
| DLP para Copilot (Data Loss Prevention)               | Pol√≠ticas DLP aplicadas espec√≠ficamente a prompts y respuestas de Copilot: detectan PII, credenciales, datos financieros en los prompts enviados por los usuarios y en las respuestas generadas por el LLM.                                      | Protecci√≥n del output del LLM: si el LLM genera una respuesta con PII de un empleado, la pol√≠tica DLP puede redactarla antes de mostrarla al usuario. Prevenci√≥n de exfiltraci√≥n: bloquea prompts que intentan sacar datos confidenciales v√≠a Copilot.               | GA para M365 Copilot. En preview para algunos AI apps de terceros.             |
| Purview Audit (Audit Log de Interacciones IA)         | Registro auditado de todas las interacciones de usuarios con Copilot: qu√© prompts enviaron, qu√© respuestas recibieron, qu√© documentos proces√≥ el LLM. El audit log es inmutable y searchable.                                                    | Accountability y derecho de acceso GDPR: el titular puede preguntar qu√© datos suyos proces√≥ el LLM. La organizaci√≥n puede auditar qu√© empleados est√°n enviando PII a Copilot. Evidencia forense en caso de incidente de privacidad involucrando IA.                  | GA. Requiere habilitaci√≥n del audit log de Copilot.                            |
| Insider Risk Management for AI                        | Detecta patrones de comportamiento de usuarios con Copilot que indican posible riesgo de insider threat: b√∫squedas masivas antes de salir de la empresa, consultas sobre documentos de proyectos no asignados, extracci√≥n de datos financieros.  | Complementa la Data Minimization: identifica cu√°ndo un usuario est√° usando Copilot para acceder a m√°s datos de los que su funci√≥n justifica. Genera alertas para investigaci√≥n, no acciones autom√°ticas.                                                             | GA. Requiere licencia M365 E5 Compliance.                                      |
| Compliance Manager ‚Äî Templates de Regulaci√≥n IA       | Templates pre-configurados para evaluar el estado de compliance con GDPR, EU AI Act, ISO 42001, NIST 2 AI, y otras regulaciones. Genera assessment autom√°tico y acciones recomendadas.                                                           | Para el CISO que necesita documentar compliance GDPR Art. 25 (PbD) y Art. 35 (DPIA) en relaci√≥n a sistemas de IA: Compliance Manager provee el framework y evidencia de controles implementados.                                                                     | GA. Templates de EU AI Act, GDPR disponibles.                                  |
| Data Lifecycle Management                             | Pol√≠ticas autom√°ticas de retenci√≥n y eliminaci√≥n de contenido M365, incluyendo logs de Copilot y documentos del corpus RAG. Define cu√°nto tiempo se retienen los prompts y respuestas de Copilot.                                                | Implementaci√≥n t√©cnica de la Data Minimization temporal: los logs de Copilot se eliminan autom√°ticamente seg√∫n la pol√≠tica configurada. Derecho al olvido: la eliminaci√≥n de contenido en SharePoint propaga al corpus RAG en el siguiente ciclo de indexaci√≥n.      | GA. Requiere configuraci√≥n de pol√≠ticas de retenci√≥n espec√≠ficas para AI logs. |
| DSPM for AI ‚Äî Data Risk Assessments                   | Escaneo semanal autom√°tico de los 100 SharePoint sites m√°s usados para identificar datos sobre-compartidos que podr√≠an ser amplificados por Copilot: documentos sin sensitivity label accesibles a toda la organizaci√≥n.                         | Prevenci√≥n proactiva del Copilot Oversharing Problem: identifica antes de que el CISO lo descubra por un incidente cu√°les son los documentos que Copilot est√° usando y que no deber√≠an estar disponibles tan ampliamente.                                            | GA. Escaneo autom√°tico + assessments manuales configurables.                   |

<br>

‚ö†Ô∏è El Copilot Oversharing Problem: Un estudio 2025 de enterprise prompt data encontr√≥ que el 8.5% de los prompts enviados a Copilot y ChatGPT riesgaban exponer datos sensibles: customer info (45.8%), employee PII (26.8%), legal and financial data (14.9%), security details (6.9%). Microsoft Purview DSPM for AI detecta oversharing en SharePoint ‚Äî pero no controla lo que el LLM infiere combinando datos de m√∫ltiples fuentes a las que el usuario tiene acceso leg√≠timo individualmente. Esto es el l√≠mite de Purview: controla acceso, no inferencia sem√°ntica. Herramientas complementarias como Knostic o guardrails de output son necesarias para el gap de inferencia.

<br>

## 15.6 DPIA PARA SISTEMAS DE IA: METODOLOG√çA Y CASOS PR√ÅCTICOS

<br>

El DPIA (Data Protection Impact Assessment) o Evaluaci√≥n de Impacto en la Protecci√≥n de Datos es obligatorio bajo GDPR Art. 35 cuando el tratamiento de datos 'puede suponer un alto riesgo para los derechos y libertades de las personas f√≠sicas'. Los sistemas de IA son candidatos naturales al DPIA: combinan procesamiento automatizado de datos a escala, decisiones que afectan a las personas, y riesgos de sesgos y discriminaci√≥n que no existen en sistemas convencionales. La Reforma 2025 de la Ley 25.326 argentina introduce un requisito equivalente para tratamientos automatizados.

<br>

### 15.6.1 Cu√°ndo es Obligatorio el DPIA para Sistemas de IA

| CRITERIO DE OBLIGATORIEDAD (GDPR Art. 35)                       | SISTEMA DE IA QUE LO ACTIVA                                                                                                         | EJEMPLO EN CONTEXTO M365                                                                                                                                                               |
| --------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Evaluaci√≥n o puntuaci√≥n sistem√°tica de personas (profiling)     | Cualquier sistema de IA que score, clasifique, o perfile individuos bas√°ndose en sus datos.                                         | Un agente de Copilot Studio que analiza el rendimiento de empleados bas√°ndose en datos de Microsoft Viva Insights (reuniones, emails, actividad Teams) para apoyar decisiones de RRHH. |
| Decisiones automatizadas con efectos jur√≠dicos o significativos | Sistemas donde la IA toma o apoya decisiones que afectan material o legalmente a personas.                                          | Un sistema que usa ML para scoring crediticio de clientes (si la empresa es financiera), o para priorizaci√≥n de candidatos en procesos de selecci√≥n.                                   |
| Monitoreo sistem√°tico                                           | Sistemas de vigilancia o monitoreo sistem√°tico de personas en √°reas accesibles al p√∫blico o en el workplace.                        | Microsoft Viva Insights con an√°lisis de comportamiento de empleados. Insider Risk Management que monitorea actividad de usuarios en M365.                                              |
| Datos sensibles a escala                                        | Procesamiento a gran escala de categor√≠as especiales de datos: salud, biometr√≠a, afiliaci√≥n pol√≠tica, religi√≥n, orientaci√≥n sexual. | Un LLM que procesa registros m√©dicos de empleados (aseguradora de salud), o datos de afiliaci√≥n sindical.                                                                              |
| Datos de personas vulnerables                                   | Tratamiento de datos de menores, pacientes, personas bajo custodia.                                                                 | Cualquier sistema de IA que procesa datos de menores en contextos educativos o de salud.                                                                                               |
| Transferencias internacionales de datos de alto riesgo          | Datos que se transfieren fuera del EEE (o Argentina) a pa√≠ses sin nivel adecuado de protecci√≥n para training de modelos.            | Uso de APIs de LLMs con servidores fuera de la UE/Argentina para procesar datos de empleados o clientes locales, sin Standard Contractual Clauses adecuadas.                           |

<br>

### 15.6.2 El Proceso DPIA para Sistemas de IA ‚Äî 8 Pasos

| # | PASO                                                      | CONTENIDO ESPEC√çFICO PARA IA                                                                                                                                                                                                                                                                                                                                                                                                                                        | OUTPUT Y EVIDENCIA                                                                                                                                                    |
| - | --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1 | Descripci√≥n del sistema de IA y del tratamiento           | Describir: arquitectura del sistema (LLM base, RAG, agentes), fuentes de datos usadas (SharePoint, Exchange, Dataverse), capacidad de inferencia (qu√© puede deducir el sistema adem√°s de lo que se le da expl√≠citamente), destinatarios de los outputs, retenci√≥n de datos y logs.                                                                                                                                                                                  | Ficha t√©cnica del sistema de IA. Flowchart de datos. Inventario de fuentes de datos y su clasificaci√≥n.                                                               |
| 2 | Evaluaci√≥n de necesidad y proporcionalidad                | ¬øEs la IA necesaria para el fin declarado? ¬øPodr√≠a el fin lograrse con menos datos o con t√©cnicas menos intrusivas? ¬øLos datos usados son los m√≠nimos necesarios? Para sistemas de RRHH: ¬øel an√°lisis de comportamiento de empleados es proporcional a la necesidad de gesti√≥n de rendimiento?                                                                                                                                                                      | Justificaci√≥n documentada de la base legal (inter√©s leg√≠timo, consentimiento, contrato). An√°lisis de alternativas consideradas. Decisi√≥n de proporcionalidad firmada. |
| 3 | Identificaci√≥n de riesgos de privacidad espec√≠ficos de IA | Riesgos espec√≠ficos de IA que no existen en sistemas convencionales: (1) Memorizaci√≥n de datos personales en el modelo, (2) Inferencias no intencionadas (el modelo deduce orientaci√≥n sexual de patrones de comportamiento), (3) Sesgos algor√≠tmicos que discriminan grupos protegidos, (4) Reidentificaci√≥n de datos pseudonimizados, (5) Exfiltraci√≥n de datos v√≠a prompts adversariales, (6) Derecho al olvido t√©cnicamente inviable en el modelo ya entrenado. | Risk register espec√≠fico para IA. Probabilidad e impacto de cada riesgo. Riesgos residuales tras controles.                                                           |
| 4 | Evaluaci√≥n de controles existentes                        | Documentar los controles implementados y su efectividad: Purview DLP en Copilot (¬øbloquea efectivamente PII en outputs?), Sensitivity Labels (¬øtodos los documentos relevantes est√°n etiquetados?), Audit Log (¬øes completo y buscable?), Data Minimization en RAG (¬øel corpus est√° limitado a datos necesarios?), controles de acceso.                                                                                                                             | Mapa de controles √ó riesgos. Gaps identificados entre riesgos y controles. Rating de efectividad de cada control.                                                     |
| 5 | Plan de mitigaci√≥n de riesgos residuales                  | Para cada riesgo sin cobertura de control adecuada: dise√±ar e implementar controles adicionales. Para el riesgo de memorizaci√≥n en modelos propietarios: plan de uso de DP en fine-tuning. Para sesgo: auditor√≠a con Azure Responsible AI Dashboard y Fairlearn antes del deployment. Para Machine Unlearning: proceso de evaluaci√≥n caso a caso.                                                                                                                   | Plan de mitigaci√≥n con acciones, responsables y fechas. Compromiso de implementaci√≥n previo al deployment o en plazo definido.                                        |
| 6 | Consulta con DPO (Data Protection Officer)                | El DPO debe revisar el DPIA antes de que el sistema entre en producci√≥n. Si los riesgos residuales son altos, puede ser necesaria consulta previa con la autoridad supervisora (AEPD en Espa√±a, AGPD en Argentina para Ley 25.326). El DPO tiene derecho a disentir y el responsable debe documentar por qu√© no sigui√≥ su consejo.                                                                                                                                  | Opinion del DPO firmada. Si hay discrepancia: documentaci√≥n de la justificaci√≥n del responsable. Si aplica: evidencia de consulta previa con autoridad supervisora.   |
| 7 | Revisi√≥n continua ‚Äî El DPIA es un documento vivo          | El DPIA debe revisarse cuando: el sistema cambia significativamente (nuevo modelo base, nuevas fuentes de datos, nuevas capacidades de agente), cuando ocurre un incidente de privacidad relacionado con el sistema, y peri√≥dicamente (recomendado: anualmente para sistemas de alto riesgo).                                                                                                                                                                       | Calendario de revisi√≥n peri√≥dica. Log de cambios al sistema con indicaci√≥n de si requieren revisi√≥n del DPIA. Proceso de re-DPIA para cambios significativos.         |
| 8 | Documentaci√≥n y registro                                  | El DPIA debe estar documentado y disponible para la autoridad supervisora cuando lo requiera. No es obligatorio publicarlo, pero s√≠ tenerlo disponible. Purview Compliance Manager provee templates de DPIA y gesti√≥n del ciclo de vida del documento.                                                                                                                                                                                                              | DPIA firmado y versionado. Accesible en el registro de DPIAs de la organizaci√≥n. Historial de revisiones. Disponible para auditor√≠a regulatoria.                      |

<br>

## 15.7 PLAN DE PRIVACIDAD DE IA PARA CISO CON M365/COPILOT (2026-2027)

<br>

El plan que sigue traduce los conceptos de las secciones anteriores en acciones concretas, secuenciadas y con criterios de √©xito medibles para el CISO que gestiona 20,000+ licencias M365 E5 con Copilot activo. Se estructura en tres horizontes: los primeros 90 d√≠as (baseline y controles urgentes), el primer a√±o (programa maduro), y 2027 (madurez avanzada).

<br>

### 15.7.1 Los Primeros 90 D√≠as ‚Äî Baseline y Controles Urgentes

| <p><br></p> | ACCI√ìN                                                         | DETALLE DE EJECUCI√ìN                                                                                                                                                                                                                                                                                                                                                                          | CRITERIO DE √âXITO                                                                                                                                                                                  |
| ----------- | -------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1           | Activar DSPM for AI y ejecutar Data Risk Assessment inicial    | Habilitar Microsoft Purview DSPM for AI en el portal de Purview. Ejecutar el assessment de los 100 SharePoint sites m√°s accedidos para identificar datos sobre-compartidos que Copilot puede amplificar. Revisar el AI Activity report: qu√© tipos de datos sensibles est√°n fluyendo hacia Copilot y en qu√© volumen.                                                                           | Mapa de riesgos de oversharing. Lista de sitios SharePoint con datos sobre-compartidos priorizados por volumen y sensibilidad. Baseline de PII en prompts de Copilot.                              |
| 2           | Completar el etiquetado MIP de documentos de alta sensibilidad | Auditar el estado de los Sensitivity Labels en M365: qu√© % de los documentos clasificados como Confidencial/Alta Confidencialidad tienen el label aplicado. Implementar auto-labeling para documentos sin label que contienen patrones de datos sensibles (Purview auto-classification). Los labels son el mecanismo central que impide que Copilot procese contenido que no debe.            | Target: >90% de documentos con clasificaci√≥n Confidential o superior tienen label aplicado. Cobertura de auto-labeling habilitada para patrones de PII principales (DNI, CUIL, datos financieros). |
| 3           | Implementar DLP en prompts y respuestas de Copilot             | Configurar pol√≠ticas DLP en Microsoft Purview para M365 Copilot: (1) Detectar y bloquear/alertar prompts que incluyen n√∫meros de tarjeta, DNI, CUIL, contrase√±as. (2) Detectar y redactar PII de empleados en respuestas de Copilot antes de mostrarlas. (3) Pol√≠tica para documentos con labels Highly Confidential: Copilot no puede resumirlos ni incluir su contenido en respuestas.      | Pol√≠ticas DLP activas para Copilot. Reporte mensual: N¬∞ de prompts bloqueados/alertados, tipos de PII detectados, N¬∞ de redacciones de output.                                                     |
| 4           | Ejecutar DPIA para los casos de uso Copilot de mayor riesgo    | Identificar los 3-5 casos de uso de Copilot con mayor riesgo de privacidad: casos de RRHH (an√°lisis de rendimiento, n√≥mina), casos financieros (acceso a P\&L, presupuesto), casos legales (contratos con PII de clientes). Ejecutar el proceso DPIA de 8 pasos para cada uno usando el template de Compliance Manager.                                                                       | DPIAs completados para todos los casos de uso de alto riesgo. DPO ha revisado y firmado. Hallazgos documentados con plan de mitigaci√≥n.                                                            |
| 5           | Configurar retenci√≥n de logs de Copilot                        | Habilitar el Audit Log de Copilot en Purview (si no est√° habilitado). Configurar pol√≠tica de retenci√≥n: logs completos (contenido de prompts/respuestas) retenidos m√°ximo 90 d√≠as. Logs de metadatos (user, timestamp, tokens) retenidos hasta 1 a√±o para analytics. Asegurarse de que la pol√≠tica de retenci√≥n est√° documentada en el Registro de Actividades de Tratamiento (GDPR Art. 30). | Pol√≠tica de retenci√≥n de logs de Copilot configurada y verificada. Documentaci√≥n en el RAT (Registro de Actividades de Tratamiento). Comunicaci√≥n al DPO de la pol√≠tica.                           |
| 6           | Inventario de sistemas de IA y evaluaci√≥n de DPIA pendientes   | Levantar el inventario completo de sistemas de IA en la organizaci√≥n: M365 Copilot, agentes de Copilot Studio, apps sobre Azure AI Foundry, herramientas de IA de terceros (ChatGPT Enterprise si existe, GitHub Copilot, etc.). Para cada sistema: ¬ørequiere DPIA? ¬øfue realizado? ¬øcu√°ndo fue la √∫ltima revisi√≥n?                                                                           | Inventario de sistemas de IA con status de DPIA para cada uno. Backlog de DPIAs pendientes con priorizaci√≥n y fechas target.                                                                       |

<br>

### 15.7.2 Programa de Privacidad de IA Maduro ‚Äî A√±o 1 y 2027

| INICIATIVA                                                        | DESCRIPCI√ìN Y ALCANCE                                                                                                                                                                                                                                                                                                                                                                                     | HERRAMIENTAS Y M√âTRICAS                                                                                                                                                                                                                                                                                                                       |
| ----------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Privacy-by-Design como gate de deployment de IA                   | Ning√∫n sistema de IA (agente de Copilot Studio, app Azure AI Foundry, plugin Copilot) puede salir a producci√≥n sin un checklist de privacy review aprobado. El checklist incluye: DPIA completado si corresponde, Sensitivity Labels configurados, DLP policies configuradas, corpus RAG auditado, pol√≠tica de retenci√≥n de logs definida.                                                                | Gate formal en el proceso de deployment de IA. Registro de aprobaciones. Responsable: DPO y CISO co-firman los deployments de alto riesgo. KPI: 100% de nuevos sistemas de IA con privacy review completado.                                                                                                                                  |
| Auditor√≠a anual de privacidad de IA                               | Revisi√≥n anual del estado del programa: re-ejecutar el Data Risk Assessment de DSPM for AI, revisar todos los DPIAs existentes, auditar el estado de los controles Purview, evaluar cambios regulatorios desde la √∫ltima revisi√≥n (EU AI Act, reforma Ley 25.326, nuevas gu√≠as de autoridades supervisoras).                                                                                              | Purview DSPM for AI reports. Compliance Manager compliance score. Herramienta externa de auditor√≠a si el volumen lo justifica. Output: Privacy Posture Report anual presentable al Board.                                                                                                                                                     |
| Capacitaci√≥n en privacidad de IA para usuarios de Copilot         | Los 20,000+ usuarios de Copilot no entienden instintivamente qu√© datos env√≠an al LLM ni las implicaciones. Programa de capacitaci√≥n: qu√© no poner en prompts de Copilot (PII de terceros, credenciales, datos de clientes), c√≥mo usar la clasificaci√≥n de documentos, qu√© hacer si Copilot genera output con datos de otra persona.                                                                       | Microsoft Viva Learning para distribuci√≥n de contenido. M√≥dulo de 30 minutos con quiz. KPI: >80% de usuarios Copilot con capacitaci√≥n completada en 12 meses. Reducci√≥n medible de prompts con PII detectados por DLP.                                                                                                                        |
| Implementaci√≥n de Presidio para corpus RAG (2027)                 | Para organizaciones que desarrollan aplicaciones propias sobre Azure AI Foundry con corpus RAG propietario: implementar Microsoft Presidio en el pipeline de indexaci√≥n para anonimizar PII antes de que el texto ingrese al vector database. Configurar para espa√±ol y los tipos de PII relevantes: DNI, CUIL, CBU, nombres de personas identificables.                                                  | Azure ML pipeline con Presidio. KPI: 0% de PII identificable en embeddings del vector database. Auditor√≠a trimestral de muestras del corpus RAG.                                                                                                                                                                                              |
| Proceso de ejercicio de derechos GDPR/25.326 para sistemas de IA  | Definir c√≥mo responde la organizaci√≥n cuando un individuo ejerce derechos sobre datos que pueden estar en sistemas de IA: derecho de acceso (qu√© datos del individuo est√°n en el corpus RAG), derecho de rectificaci√≥n (corregir un dato en el RAG), derecho al olvido (eliminar datos del corpus RAG y, donde sea posible, del modelo).                                                                  | Proceso documentado con SLAs: acceso en 30 d√≠as, olvido en 30 d√≠as donde t√©cnicamente posible (eliminaci√≥n del corpus RAG es viable; Machine Unlearning del modelo base no lo es actualmente). Registro de solicitudes y respuestas. Escalar al DPO y asesor√≠a legal los casos donde el Machine Unlearning exacto no es t√©cnicamente posible. |
| Privacy Enhancing Technologies (PETs) para modelos propios (2027) | Para organizaciones que desarrollan o fine-tunean modelos propios: evaluar la implementaci√≥n de Differential Privacy en el pipeline de fine-tuning usando Opacus (PyTorch) o TensorFlow Privacy. Target: epsilon calibrado a los requisitos de compliance (Œµ=8 para alineaci√≥n con ISO/IEC 27559 y NIST SP 800-226). Evaluar Synthetic Data como alternativa para datasets que contienen PII de clientes. | Azure ML + Opacus/TF Privacy. M√©tricas: epsilon, degradaci√≥n de accuracy vs. baseline, confirmaci√≥n de compliance DP. KPI: fine-tuning de modelos propios con DP implementado. Synthetic dataset generator configurado para casos de uso identificados.                                                                                       |

<br>

## 15.8 REFERENCIAS (R391‚ÄìR420)

<br>

Privacidad de IA: Marcos Regulatorios y Principios:

R391. GDPR, Art. 25 (Privacy-by-Design y Privacy-by-Default), Art. 35 (Data Protection Impact Assessment), Art. 17 (Derecho al Olvido), Art. 5(1)(c) (Data Minimization). eur-lex.europa.eu. Marco legal primario que obliga el privacy engineering en sistemas de IA que procesan datos de ciudadanos de la UE.

R392. EU AI Act, Art. 10 (Data and Data Governance). Sistemas de IA de alto riesgo deben garantizar pr√°cticas de gobernanza de datos adecuadas que incluyan medidas de privacidad apropiadas en los datos de entrenamiento, validaci√≥n y testing.

R393. Cavoukian, Ann, 'Privacy by Design: The 7 Foundational Principles', Information & Privacy Commissioner of Ontario, 2009. Los 7 principios fundacionales que se convierten en requisito legal bajo GDPR Art. 25 y que esta Fase adapta al contexto de sistemas de IA.

R394. Secure Privacy, 'Compliance Challenges at the Intersection between AI & GDPR in 2025', 2025. GDPR penalties reaching 4% global revenue or EUR 20M. DPIA obligatorio para sistemas de IA de alto riesgo. Privacy Shield y EU-U.S. Data Privacy Framework para transferencias internacionales. Data minimization y purpose limitation aplicados a IA.

R395. NIST SP 800-226, 'Guidelines for Evaluating Differential Privacy Guarantees', 2023. Gu√≠a t√©cnica para evaluaci√≥n de implementaciones de Differential Privacy. Referencia para epsilon values en contextos de compliance regulatorio.

R396. ISO/IEC 27559:2022, 'Privacy Enhancing Data De-identification Framework'. Marco de referencia para t√©cnicas de de-identificaci√≥n y privacidad mejorada. Referencia para compliance de DP con Œµ en rango 5.74-14.13.

<br>

Differential Privacy ‚Äî Teor√≠a y Aplicaci√≥n Enterprise:

R397. Dwork, Cynthia (Microsoft Research), 'Differential Privacy', ICALP 2006. El art√≠culo fundacional que define Differential Privacy y la garant√≠a matem√°tica de privacidad. Creado por la investigadora que luego co-lider√≥ el programa de privacidad de Microsoft.

R398. Alation, 'Privacy-Preserving Machine Learning: Minimizing PII and PHI', agosto 2025. T√©cnicas comparadas: Differential Privacy, Homomorphic Encryption, Federated Learning, SMPC. TensorFlow Privacy, PySyft, FATE como frameworks enterprise. Entity recognition y automated PII detection a escala.

R399. Nature Scientific Reports, 'Privacy-preserving federated credit risk models: evaluating differential privacy and homomorphic encryption techniques', enero 2026. DP-FL con Œµ=8.65: alineado con GDPR Privacy-by-Design, ISO/IEC 27559, NIST SP 800-226. Degradaci√≥n de accuracy <3%. CKKS-based HE-FL: confidencialidad criptogr√°fica pero overhead significativo.

R400. Opacus (Meta/PyTorch), 'Differential Privacy for Deep Learning', github.com/pytorch/opacus. Librer√≠a est√°ndar para DP-SGD en PyTorch. Integrable en pipelines Azure ML. Soporte para epsilon accounting con R√©nyi Differential Privacy.

R401. TensorFlow Privacy (Google), 'TF Privacy: Machine Learning with Differential Privacy', 2025. Extensiones DP para TensorFlow. Desarrollado en colaboraci√≥n parcial con Microsoft Research. Compatible con Azure ML.

<br>

Federated Learning ‚Äî Arquitectura y Casos de Uso:

R402. Introl.io, 'Federated Learning Infrastructure: Privacy-Preserving Enterprise AI Guide 2025', diciembre 2025. Mercado FL: $0.1B en 2025 ‚Üí $1.6B en 2035 (27.3% CAGR). Large enterprises: 63.7% del mercado. Solo 5.2% de investigaci√≥n en producci√≥n real. NVIDIA FLARE (enterprise), Flower (investigaci√≥n), PySyft (privacidad m√°xima). Healthcare: 34% cuota de mercado FL.

R403. Dialzara, 'Federated Edge AI: The Complete 2025 Guide to Privacy-Preserving Distributed Intelligence', 2025. Zurich Insurance + Orange Telecom: 30% mejora en predicciones con FL sin sharing de datos. Vertical FL para credit scoring con datos telco. FedBPT para adaptar LLMs grandes con FL.

R404. World Journal of Advanced Research and Reviews, 'Federated learning for privacy-preserving data analytics', 2025 (26(01), 1220-1232). FL en mobile environments: GDPR/CCPA compliance. Ataques contra FL: model inversion, membership inference, Byzantine attacks. Byzantine-robust FL frameworks, blockchain-based verification.

R405. KAIST Research, 'Federated Learning with Synthetic Data Representations', 2025. M√©todo que usa synthetic data representations en lugar de datos reales para FL. Hospitales y bancos entrenan modelos colaborativos sin compartir datos personales. Referencia citada en contexto de privacy-preserving AI por m√∫ltiples fuentes 2025.

R406. Refontelearning, 'Federated Learning for Privacy-Preserving AI: Building Trust in a Decentralized World', 2025. Secure aggregation + DP como defensa combinada. Governance, scalability y compliance: marcos legales para FL multi-organizaci√≥n. Roles emergentes: federated learning engineer, privacy engineer para FL.

<br>

Anonimizaci√≥n, Synthetic Data y Machine Unlearning:

R407. Sweeney, Latanya, 'Simple Demographics Often Identify People Uniquely', Carnegie Mellon University, 1997. El art√≠culo cl√°sico: c√≥digo postal + g√©nero + fecha de nacimiento identifica al 87% de poblaci√≥n de EE.UU. La base emp√≠rica para el concepto de quasi-identifiers y la dificultad de la anonimizaci√≥n real.

R408. Microsoft Presidio, 'PII Anonymization Library', github.com/microsoft/presidio. Open source (Apache 2.0). Detecci√≥n de PII con NLP (spaCy) + anonymization. Soporta espa√±ol, ingl√©s, alem√°n, portugu√©s. Integrable en Azure ML pipelines para pre-procesamiento de datos para RAG o fine-tuning.

R409. Article 29 Working Party (WP29), 'Opinion 05/2014 on Anonymisation Techniques', 2014. La gu√≠a oficial de los supervisores de protecci√≥n de datos de la UE sobre anonimizaci√≥n: qu√© cuenta como genuinamente an√≥nimo, cu√°ndo la pseudonimizaci√≥n es insuficiente. La referencia legal que el CISO debe conocer para declarar que un dataset est√° fuera del scope de GDPR.

R410. Cao, Yinzhi et al., 'Machine Unlearning', IEEE S\&P 2015. El art√≠culo fundacional de Machine Unlearning. El estado del arte 2025: unlearning exacto (reentrenamiento) viable solo para modelos peque√±os y datasets controlados. Approximate unlearning para LLMs: investigaci√≥n activa en Apple, Google, Microsoft Research ‚Äî sin producci√≥n estable disponible.

<br>

Microsoft Purview ‚Äî Documentaci√≥n Oficial y An√°lisis:

R411. Microsoft Learn, 'Microsoft Purview data security and compliance protections for Microsoft 365 Copilot and other generative AI apps', 2025. DSPM for AI, Sensitivity Labels para Copilot, DLP en prompts y respuestas, Audit Log de interacciones Copilot, Compliance Manager con templates EU AI Act, GDPR.

R412. Microsoft Learn, 'Data Security Posture Management for AI (classic)', 2025. learn.microsoft.com/en-us/purview/dspm-for-ai. Dashboard central DSPM: insights AI activity, ready-to-use policies, data risk assessments, compliance controls. Escaneo semanal autom√°tico de top 100 SharePoint sites.

R413. Microsoft Security Blog, 'New Microsoft Purview features help protect and govern your data in the era of AI', diciembre 2025. DSPM for AI, Data Security Investigations (AI-powered para investigar riesgos), Adaptive Protection, templates Compliance Manager para EU AI Act, ISO 42001, NIST 2 AI.

R414. BizTech Magazine, 'Microsoft Ignite 2025: How Microsoft Purview Drives Data Security in the AI Era', noviembre 2025. Data Security Investigations: primera soluci√≥n AI-powered para investigar riesgos de datos/IA. Microsoft Agent 365: governance para AI agents (1.3 billion proyectados para 2028). Three dimensions: informaci√≥n, DLP, Insider Risk Management.

R415. Knostic, 'Why Microsoft Purview Needs Help Preventing Oversharing', junio 2025. 8.5% de prompts enterprise exponen datos sensibles: customer info (45.8%), employee PII (26.8%), legal/financial (14.9%), security (6.9%). Purview limita acceso pero no inferencia sem√°ntica. El gap de inferencia: el LLM puede combinar fuentes individuales inofensivas en outputs sensibles.

<br>

Privacy Engineering Operacional:

R416. Microsoft Learn, 'Use Microsoft Purview to manage data security and compliance for Microsoft Agent 365', 2025. DSPM for AI preview con AI observability page para agentes. Audit de interacciones agent-to-human, human-to-agent, agent-to-tools, agent-to-agent. Sensitivity labels y DLP para Agent 365.

R417. Microsoft Learn, 'Learn about Microsoft Purview', 2025. Purview como plataforma unificada: data security + governance + compliance. Data Security Posture Management, Information Protection, DLP, Insider Risk Management, Compliance Manager, Data Lifecycle Management. Pricing: Purview Suite (E3+) y pay-as-you-go para data estate.

R418. Microsoft Security Blog, 'Unifying Data Security and Governance for the AI Era: Microsoft Purview Innovations for Fabric Data', septiembre 2025. Purview Information Protection policies para Fabric items (GA). DLP para OneLake estructurado (GA). Integrated Information Protection + DLP + Insider Risk Management + DSPM para AI.

R419. Microsoft SDK Developer Blog, 'Data Security and Compliance for GenAI ‚Äî Purview Developer SDK', 2025. DSPM for AI components: insights analytics, ready-to-use policies, data risk assessments, compliance controls. Data Classification, DLP, Lifecycle Management integrados. APIs para desarrolladores que construyen apps compliance-aware sobre Azure AI Foundry.

R420. Gobierno de Argentina, 'Proyecto de Reforma de la Ley 25.326 de Protecci√≥n de Datos Personales', 2025 (en proceso legislativo). Introduce evaluaciones de impacto equivalentes al DPIA para tratamientos automatizados de datos personales incluyendo sistemas de IA. Fortalece derechos de acceso, rectificaci√≥n y eliminaci√≥n. Referencia para el CISO con operaciones en Argentina.

<br>



