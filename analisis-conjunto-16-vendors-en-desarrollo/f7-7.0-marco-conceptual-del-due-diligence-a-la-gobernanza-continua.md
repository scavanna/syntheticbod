---
icon: check
---

# F7 - 7.0 MARCO CONCEPTUAL: DEL DUE DILIGENCE A LA GOBERNANZA CONTINUA

**¬ß7.0 Marco Conceptual** ‚Äî Transici√≥n del due diligence puntual (Fases 1-6) a la gobernanza continua. Tabla visual de las cuatro etapas del ciclo de vida.

**¬ß7.1 Marcos de Gobernanza** ‚Äî An√°lisis comparativo NIST AI RMF vs. ISO 42001 vs. EU AI Act en 8 dimensiones (naturaleza jur√≠dica, territorio, sanciones, certificaci√≥n, etc.). Estrategia "Build Once, Comply Everywhere" documentada con ahorro estimado del 35-45% en costo de compliance.

**¬ß7.2 Rol del CISO** ‚Äî Datos emp√≠ricos 2026: 90%+ organizaciones sin acceso irrestricto a IA, 65% herramientas sin aprobaci√≥n, $670K costo diferencial por Shadow AI. Modelo de comit√© de gobernanza con RACI completo. Framework AEGIS de Forrester (6 dominios) para gobernanza de agentes.

**¬ß7.3 OWASP Agentic Top 10 (2026)** ‚Äî Los dos principios rectores (Menor Agencia + Observabilidad Fuerte) con tabla detallada de los 10 riesgos ASI01-ASI10, ejemplos reales (EchoLeak, Replit database deletion, primer MCP server malicioso npm septiembre 2025) y controles de gobernanza requeridos por riesgo.

**¬ß7.4 Shadow AI** ‚Äî Framework de gesti√≥n en 6 fases (Inventario ‚Üí Clasificaci√≥n ‚Üí Pol√≠tica ‚Üí Controles t√©cnicos ‚Üí Alternativas internas ‚Üí Monitoreo continuo).

**¬ß7.5 Monitoreo Continuo e Incident Response** ‚Äî 6 √°reas de monitoreo con frecuencias y alertas. AI IRP en 5 fases con caracter√≠sticas espec√≠ficas para incidentes de IA (vs. IR tradicional).

**¬ß7.6 EU AI Act** ‚Äî Timeline completo 2024-2028 con estado actual de cada hito. Checklist de implementaci√≥n Arts. 9-17 para sistemas alto riesgo. Tabla de aplicabilidad para 4 escenarios de organizaciones LATAM con exposici√≥n europea.

**¬ß7.7 Re-Evaluaci√≥n de Proveedores** ‚Äî Framework de 6 frecuencias (continuo ‚Üí mensual ‚Üí trimestral ‚Üí semestral ‚Üí anual ‚Üí evento-driven). 7 disparadores de re-evaluaci√≥n de emergencia con acciones espec√≠ficas.

**¬ß7.8 Scorecard de Madurez** ‚Äî 5 dimensiones √ó 4 niveles (Inicial ‚Üí Desarrollando ‚Üí Definido ‚Üí Optimizado). Meta 2026 para sectores regulados: Nivel 3 en agosto 2026 con roadmap a Nivel 4.

**¬ß7.9 Referencias** ‚Äî R145‚ÄìR180, organizadas en 4 categor√≠as (OWASP, EU AI Act, CISO/Gobernanza, Marcos t√©cnicos).

## 7.0 MARCO CONCEPTUAL: DEL DUE DILIGENCE A LA GOBERNANZA CONTINUA

<br>

Las Fases 1 a 6 de este informe abordaron la selecci√≥n y due diligence de proveedores de IA: evaluar sus filosof√≠as √©ticas, metodolog√≠as de alignment, transparencia, comportamiento ante dilemas √©ticos, adecuaci√≥n a sectores regulados y la solidez de sus t√©rminos contractuales. La Fase 7 representa una transici√≥n fundamental: el proceso no termina cuando se firma el contrato. En realidad, ese momento marca el inicio del ciclo de gobernanza continua que determina si la promesa de los an√°lisis de due diligence se materializa en la pr√°ctica operacional.

<br>

Esta transici√≥n es especialmente cr√≠tica en 2026 por tres razones convergentes: (1) la aceleraci√≥n hacia sistemas de IA ag√©nticos que act√∫an aut√≥nomamente en producci√≥n y que presentan superficies de ataque radicalmente distintas a los LLMs pasivos; (2) la inminencia de los deadlines del EU AI Act para sistemas de alto riesgo (agosto 2026) que convierten la gobernanza de IA en obligaci√≥n legal, no solo buena pr√°ctica; y (3) el surgimiento del Shadow AI ‚Äîherramientas desplegadas sin aprobaci√≥n de TI‚Äî como vector de riesgo de primer nivel, documentado en el 65% de las organizaciones.

<br>

| ANTES                                               | TRANSICI√ìN                                                            | DURANTE                                                                         | CICLO CONTINUO                                                                    |
| --------------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |
| Evaluaci√≥n de proveedores(Fases 1‚Äì4)                | Due diligence sectorial y contractual(Fases 5‚Äì6)                      | Implementaci√≥n y primeros 90 d√≠as                                               | Gobernanza post-implementaci√≥n(Fase 7)                                            |
| Filosof√≠a √©ticaAlignmentTransparenciaDilemas √©ticos | Sector: regulatorioContracto: ZDR, indemnizaci√≥nSLAs, certificaciones | Integraci√≥n t√©cnicaConfiguraci√≥n seguridadCapacitaci√≥n equiposBaseline m√©tricas | Monitoreo continuoRe-evaluaci√≥n peri√≥dicaGesti√≥n incidentesCompliance regulatorio |

<br>

üìå Principio rector de la Fase 7: La gobernanza de IA no es un estado que se alcanza ‚Äî es un proceso continuo. El proveedor que cumpl√≠a todos los criterios de due diligence en enero puede haber cambiado sus pol√≠ticas de entrenamiento, sufrido una brecha de seguridad, o actualizado su modelo base en marzo sin notificaci√≥n al cliente. La gobernanza post-implementaci√≥n convierte la evaluaci√≥n puntual en capacidad organizacional.

<br>

## 7.1 MARCOS DE GOBERNANZA DE IA: NIST AI RMF, ISO 42001, EU AI ACT

<br>

En 2026 convergen tres marcos principales que definen c√≥mo las organizaciones deben gestionar los riesgos de sus sistemas de IA. Cada uno tiene origen, alcance y naturaleza jur√≠dica distinta. La clave para organizaciones con exposici√≥n regulatoria m√∫ltiple es un enfoque de 'Build Once, Comply Everywhere': implementar controles que satisfagan simult√°neamente los tres marcos, evitando duplicaci√≥n de esfuerzo.

<br>

### 7.1.1 NIST AI Risk Management Framework (AI RMF 1.0 + Perfiles Sectoriales)

| DIMENSI√ìN                | DETALLE                                                                                                                                                                                                                                                                                                                            |
| ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Origen y naturaleza      | NIST (EE.UU.), enero 2023. Voluntario a nivel federal; puede volverse obligatorio en sectores (OCC/FDIC/Fed SR 11-7 alineado). No tiene fuerza de ley, pero establece el est√°ndar de facto para organizaciones que interact√∫an con el gobierno o sistemas regulados de EE.UU.                                                      |
| Estructura: 4 funciones  | GOVERN ‚Äî Pol√≠ticas, roles, cultura de riesgo. MAPEADO (MAP) ‚Äî Identificar contexto y riesgos. MEDIR (MEASURE) ‚Äî Evaluar y analizar riesgos. GESTIONAR (MANAGE) ‚Äî Priorizar, responder, monitorear.                                                                                                                                 |
| Relevancia pr√°ctica 2026 | Gartner (2025): organizaciones con certificaci√≥n ISO 27001 pueden alcanzar ISO 42001 hasta un 40% m√°s r√°pido. El NIST AI RMF es el puente conceptual entre ISO 27001 (seguridad de informaci√≥n) e ISO 42001 (gesti√≥n de IA). Organizaciones que ya tienen controles NIST CSF implantados tienen ventaja estructural.               |
| Limitaci√≥n documentada   | IANS (feb. 2026): muchos CISOs encuentran el marco 'demasiado voluminoso o vago, dado sus suposiciones sobre el nivel de control que la mayor√≠a de las empresas tienen sobre sus modelos de IA y su infraestructura'. Apto para organizaciones con modelos propios; m√°s dif√≠cil de aplicar cuando el modelo es un SaaS de tercero. |
| Perfiles sectoriales     | NIST public√≥ perfiles sectoriales para: Financial Services (2024), AI Cybersecurity (2024), GenAI (2024). Estos perfiles mapean los outcomes del AI RMF a requisitos espec√≠ficos del sector, reduciendo el trabajo de adaptaci√≥n.                                                                                                  |

<br>

### 7.1.2 ISO/IEC 42001:2023 ‚Äî Sistema de Gesti√≥n de IA

| DIMENSI√ìN                    | DETALLE                                                                                                                                                                                                                                                                                           |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Origen y naturaleza          | ISO/IEC, diciembre 2023. Certificable por terceros ‚Äî diferencia cr√≠tica frente al NIST AI RMF. Primera norma certificable dedicada a sistemas de gesti√≥n de IA. Auditors acreditados pueden emitir certificaciones formales.                                                                      |
| Relaci√≥n con ISO 27001       | Dise√±ada para integrarse con ISO 27001. Comparte estructura de alto nivel (Annex SL). Las organizaciones certificadas en ISO 27001 pueden implementar ISO 42001 como extensi√≥n. Ventaja aceleraci√≥n: 30-40% menos esfuerzo de implementaci√≥n seg√∫n datos de conformidad 2025.                     |
| Requisitos clave             | Pol√≠tica de IA organizacional documentada. Inventario de sistemas de IA con evaluaci√≥n de riesgos. Roles y responsabilidades en gobernanza de IA. Evaluaci√≥n de impacto de IA sobre personas. Controles de sesgo, transparencia y explicabilidad. Monitoreo continuo y mejora.                    |
| Estado de adopci√≥n 2025-2026 | Adopci√≥n acelerada en sectores regulados post-EU AI Act (2024). El EU AI Act referencia ISO 42001 como posible evidencia de conformidad para sistemas de alto riesgo. Certificaci√≥n ISO 42001 puede ser ventaja competitiva en licitaciones gubernamentales y contratos con grandes corporativos. |
| Relaci√≥n con proveedores IA  | ISO 42001 incluye requisitos sobre gesti√≥n de terceros (proveedores de IA). El Annex A de controles incluye: evaluaci√≥n de proveedores de IA externos, gesti√≥n de datos de IA de terceros, acuerdos contractuales con proveedores que reflejen responsabilidades de IA.                           |

<br>

### 7.1.3 Tabla Comparativa: Tres Marcos en Acci√≥n

| DIMENSI√ìN                           | NIST AI RMF                            | ISO 42001                                 | EU AI ACT                                            |
| ----------------------------------- | -------------------------------------- | ----------------------------------------- | ---------------------------------------------------- |
| Naturaleza jur√≠dica                 | Voluntario (EE.UU.)                    | Certificable (global)                     | Obligatorio (UE + extraterritorial)                  |
| Aplicabilidad territorial           | EE.UU. + influencia global             | Global                                    | UE + cualquier empresa que opera en UE               |
| Sanciones incumplimiento            | Sin sanciones directas                 | P√©rdida de certificaci√≥n                  | Hasta ‚Ç¨35M / 7% facturaci√≥n global                   |
| Certificaci√≥n de terceros           | No                                     | S√≠ (auditors acreditados)                 | S√≠ (organismos notificados)                          |
| Cobertura de terceros (proveedores) | Referencia como buena pr√°ctica         | Requisito Annex A                         | Obligaci√≥n expl√≠cita para deployers                  |
| Compatibilidad con ISO 27001        | Alta (estructura similar)              | Dise√±ada para integrarse                  | Referencia ISO 42001 como evidencia                  |
| Granularidad operacional            | Alta (funciones + outcomes detallados) | Alta (controles certificables)            | Alta (requisitos t√©cnicos por categor√≠a)             |
| Deadline para sectores regulados    | Sin deadline formal                    | Sin deadline formal (adopci√≥n voluntaria) | Agosto 2026 (alto riesgo); Diciembre 2027 (backstop) |

<br>

üí° Build Once ‚Äî Comply Everywhere: La estrategia √≥ptima para organizaciones con exposici√≥n a los tres marcos: (1) certificar ISO 27001 como base de controles de seguridad; (2) extender a ISO 42001 para gobernanza de IA certificable; (3) mapear los controles ISO 42001 a los requisitos del EU AI Act para sistemas de alto riesgo. Este enfoque reduce el costo total de compliance en un 35-45% frente a implementaciones independientes (dato: Corporate Compliance Insights, enero 2026).

<br>

## 7.2 EL ROL EXPANDIDO DEL CISO EN LA GOBERNANZA DE IA (2026)

<br>

El CISO se encuentra en 2026 en un punto de inflexi√≥n sin precedentes. Ya no es √∫nicamente guardi√°n de la ciberseguridad: se est√° convirtiendo en el l√≠der de la gobernanza de IA de la organizaci√≥n. Esta expansi√≥n de mandato es documentada por m√∫ltiples fuentes en el primer trimestre de 2026 y tiene implicaciones directas para la estructura, recursos y posicionamiento del √°rea de seguridad.

<br>

### 7.2.1 Datos Clave del Estado del CISO en 2026

| HALLAZGO                                                                                                                                                                                                                         | FUENTE Y FECHA                               |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| M√°s del 90% de organizaciones NO permiten acceso irrestricto a herramientas de IA. El 56% bloquea la mayor√≠a de herramientas con listas blancas definidas.                                                                       | IANS Research, febrero 2026                  |
| M√°s del 85% de CISOs ya han implementado pol√≠ticas dedicadas de IA o actualizado sus marcos de seguridad para incluir IA.                                                                                                        | IANS Research, febrero 2026                  |
| La mitad de las organizaciones han establecido comit√©s dedicados de gobernanza de IA ‚Äî de car√°cter cross-funcional con participaci√≥n del CISO.                                                                                   | IANS Research, febrero 2026                  |
| El Shadow AI ya representa el 20% de todas las brechas de datos, con un costo diferencial de $670,000 por encima del costo promedio est√°ndar.                                                                                    | IBM Cost of a Data Breach Report 2025        |
| El 65% de las herramientas de IA en organizaciones operan sin aprobaci√≥n de TI o seguridad.                                                                                                                                      | Vectra AI / IAPP, 2025                       |
| Las organizaciones con liderazgo de gobernanza de IA en el C-suite son 3 veces m√°s propensas a tener programas maduros.                                                                                                          | IAPP AI Governance Profession Report 2025    |
| El mercado de herramientas de gobernanza de IA creci√≥ de $227-340M (2024-2025) a trayectoria de $4.83B para 2034. CAGR: 35-45%.                                                                                                  | Precedence Research / MarketsandMarkets 2025 |
| Seg√∫n una encuesta de Dark Reading, el 48% de los profesionales de ciberseguridad identifican la IA ag√©ntica como el vector de ataque n√∫mero uno para 2026, superando ransomware, deepfakes y ataques a la cadena de suministro. | Dark Reading poll, 2026                      |

<br>

### 7.2.2 Estructura de Gobernanza de IA: Modelo de Comit√©

La estructura de gobernanza de IA enterprise requiere participaci√≥n cross-funcional. El CISO lidera o co-lidera el comit√©, pero no puede ni debe ser el √∫nico responsable. Liminal AI Enterprise AI Governance Guide (2025) documenta el modelo est√°ndar:

<br>

| ROL                             | RESPONSABILIDAD EN GOBERNANZA IA                                                                              | DECISIONES EXCLUSIVAS                                                                                                       | FRECUENCIA COMIT√â                                      |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------ |
| CISO                            | Lidera marcos de seguridad y riesgo de IA. Aprueba herramientas. Gestiona Shadow AI. Responde incidentes.     | Aprobaci√≥n/bloqueo de herramientas de IA. Definici√≥n de controles t√©cnicos. Pol√≠ticas de acceso a modelos.                  | Mensual (comit√© pleno) + ad hoc (incidentes)           |
| Legal / Compliance              | Garantiza cumplimiento regulatorio (EU AI Act, GDPR, sector-espec√≠fico). Revisa contratos con proveedores IA. | Clasificaci√≥n de sistemas alto riesgo EU AI Act. Aprobaci√≥n de DPAs y BAAs. Opini√≥n sobre litigaci√≥n activa de proveedores. | Mensual (comit√© pleno)                                 |
| CTO / Arquitectura              | Define est√°ndares t√©cnicos de integraci√≥n. Supervisa calidad y rendimiento de modelos.                        | Aprobaci√≥n de arquitecturas ag√©nticas. Est√°ndares de API y conectores. Gesti√≥n de versiones de modelos.                     | Mensual + revisiones t√©cnicas trimestrales             |
| Negocio / Due√±os de Caso de Uso | Definen requisitos funcionales. Identifican nuevos casos de uso. Reportan anomal√≠as.                          | Priorizaci√≥n de inversi√≥n en casos de uso. Definici√≥n de umbrales de supervisi√≥n humana por proceso.                        | Mensual (reportes) + aprobaci√≥n de nuevos casos de uso |
| AI Champions (por unidad)       | Embeben pr√°cticas de gobernanza en operaciones diarias. Act√∫an de enlace con el comit√© central.               | Primera l√≠nea de revisi√≥n de casos de uso departamentales. Escalaci√≥n de anomal√≠as al CISO.                                 | Semanal (operacional)                                  |

<br>

### 7.2.3 El Marco AEGIS de Forrester: Gobernanza de Agentes de IA

üìÑ Fuente: Forrester AEGIS framework ‚Äî Agentic AI Enterprise Guardrails for Information Security (2025). Referenciado en m√∫ltiples implementaciones enterprise documentadas 2025-2026.

<br>

Forrester introdujo el framework AEGIS en 2025 como estructura espec√≠fica para CISOs que gobiernan sistemas de IA ag√©nticos ‚Äî donde los riesgos son cualitativamente distintos a los LLMs pasivos. Los seis dominios AEGIS:

<br>

| # | DOMINIO AEGIS                       | DESCRIPCI√ìN Y CONTROLES CLAVE                                                                                                                                                                                                                                                  |
| - | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 1 | Governance, Risk & Compliance (GRC) | Pol√≠ticas que definen acciones permitidas y prohibidas para agentes de IA. Comit√© cross-funcional de gobernanza con representaci√≥n de seguridad, legal, privacidad, compliance, IT y negocio. Evaluaciones peri√≥dicas de riesgo. Gesti√≥n de excepciones.                       |
| 2 | Identity & Access Management (IAM)  | Los agentes de IA no son usuarios humanos pero necesitan identidades, credenciales y permisos. Redefinir el marco IAM para agentes: credenciales de corta duraci√≥n, permisos granulares por tarea, aislamiento de identidades entre agentes. Principio: menor agencia posible. |
| 3 | Data Governance                     | Gesti√≥n del flujo de datos entre agentes y sistemas externos. Controles de acceso a datos sensibles. Auditor√≠a de qu√© datos procesa cada agente y con qu√© fin. Prevenci√≥n de exfiltraci√≥n a trav√©s de cadenas de herramientas.                                                 |
| 4 | Model Risk Management               | Gesti√≥n del riesgo de los modelos subyacentes: drift, alucinaciones, degradaci√≥n de rendimiento. Integraci√≥n con frameworks MRM existentes (SR 11-7 para bancos). Inventario de modelos en producci√≥n con responsables asignados.                                              |
| 5 | Security Operations                 | Monitoreo en tiempo real del comportamiento de agentes. Detecci√≥n de anomal√≠as en patrones de invocaci√≥n de herramientas. Respuesta a incidentes espec√≠fica para IA ag√©ntica. Kill switches implementados y probados.                                                          |
| 6 | Third-Party Risk Management         | Evaluaci√≥n continua de proveedores de IA (no solo en el momento de contrataci√≥n). Monitoreo de cambios en pol√≠ticas de proveedores. Gesti√≥n de dependencias en ecosistemas multi-agente (MCP servers, plugins, APIs de terceros).                                              |

<br>

‚ö† Tensi√≥n documentada ‚Äî Innovaci√≥n vs. Control: IANS Research (feb. 2026): 'Los CISOs exitosos son los que han construido equipos s√≥lidos, creado espacio para abordar riesgos m√°s all√° de la seguridad inform√°tica tradicional, y se han posicionado como socios de negocio de confianza.' La trampa a evitar: la gobernanza de IA no debe ser un bloqueante de la innovaci√≥n sino su habilitador responsable. Gobernanza excesivamente restrictiva genera Shadow AI ‚Äî el problema que se intenta evitar.

<br>

## 7.3 OWASP TOP 10 PARA APLICACIONES AG√âNTICAS 2026

<br>

El OWASP Top 10 for Agentic Applications 2026 fue publicado el 10 de diciembre de 2025, resultado de m√°s de un a√±o de investigaci√≥n con participaci√≥n de m√°s de 100 expertos en seguridad, investigadores y profesionales de la industria. Es el primer framework est√°ndar global dedicado espec√≠ficamente a los riesgos de seguridad de los sistemas de IA aut√≥nomos ‚Äî agentes que no solo generan texto sino que planifican, deciden, invocan herramientas y ejecutan acciones con m√≠nima supervisi√≥n humana.

<br>

Relevancia para la gobernanza post-implementaci√≥n: las Fases 1-6 evaluaron a los proveedores. El OWASP Agentic Top 10 define los controles que el equipo interno debe implementar independientemente del proveedor elegido, porque estos riesgos emergen de la arquitectura ag√©ntica, no del modelo subyacente. La adopci√≥n del Top 10 como baseline de control arquitect√≥nico es recomendada por IANS para CISOs evaluando su programa de gobernanza de IA en 2026.

<br>

üìä Contexto de urgencia: Dark Reading poll (2026): 48% de profesionales de ciberseguridad identifican la IA ag√©ntica como el VECTOR DE ATAQUE #1 para 2026 ‚Äî superando ransomware, deepfakes y supply chain attacks. Sin embargo, solo el 34% de empresas tienen controles de seguridad espec√≠ficos para IA en producci√≥n. Barracuda Security identific√≥ 43 componentes de frameworks ag√©nticos con vulnerabilidades de supply chain embebidas.

<br>

### 7.3.1 Los Dos Principios Rectores del OWASP Agentic Top 10

| PRINCIPIO                                   | DESCRIPCI√ìN OPERACIONAL                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| ------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| MENOR AGENCIA(Least Agency)                 | Extensi√≥n del principio de Menor Privilegio (Least Privilege) al espacio de IA ag√©ntica. Un agente solo debe tener el m√≠nimo nivel de autonom√≠a necesario para completar su tarea espec√≠fica. La autonom√≠a no utilizada es superficie de ataque no necesaria. Este principio implica: (a) definir expl√≠citamente qu√© herramientas puede invocar cada agente; (b) escopar los permisos por tarea, no por rol permanente; (c) requerir aprobaci√≥n humana para acciones de alto impacto, irreversibles o que escalen privilegios; (d) nunca permitir que un agente apruebe su propia escalada de permisos.                                               |
| OBSERVABILIDAD FUERTE(Strong Observability) | Control de seguridad no negociable para sistemas ag√©nticos. Sin visibilidad clara sobre qu√© hacen los agentes, por qu√© lo hacen y qu√© herramientas invocan, los problemas menores se expanden silenciosamente en fallos sist√©micos. Los requisitos m√≠nimos de observabilidad ag√©ntica incluyen: (a) logs inmutables y firmados de todas las acciones, invocaciones de herramientas y comunicaciones inter-agente; (b) trazabilidad completa del estado de objetivos del agente; (c) alertas en tiempo real ante patrones de uso de herramientas an√≥malos; (d) capacidad de replay de sesiones completas de agentes para investigaci√≥n post-incidente. |

<br>

### 7.3.2 Los 10 Riesgos Cr√≠ticos (ASI01‚ÄìASI10) con Implicaci√≥n para Gobernanza

| ID    | RIESGO                              | DESCRIPCI√ìN Y EJEMPLO REAL                                                                                                                                                                                                                                                                                                                                                                                                                                                        | CONTROLES DE GOBERNANZA REQUERIDOS                                                                                                                                                                                                                                                                                                                                                                       |
| ----- | ----------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ASI01 | Agent GoalHijacking                 | Un atacante altera los objetivos del agente mediante prompts maliciosos inyectados en emails, documentos, PDFs, invitaciones de calendario o datos de RAG. El agente no puede distinguir instrucciones v√°lidas de contenido malicioso. Ejemplo real: EchoLeak (CVE-2025-32711) en Microsoft 365 Copilot ‚Äî zero-click attack que redirig√≠a el agente. Ejemplo real: Replit 'vibe coding' ‚Äî agente elimin√≥ base de datos de producci√≥n.                                             | (1) Tratar TODOS los inputs de lenguaje natural como no confiables. (2) Validaci√≥n y filtrado de contenido antes del procesamiento ag√©ntico. (3) Permisos de herramientas tan restringidos que un objetivo hijackeado no cause da√±o catastr√≥fico. (4) Monitoreo de drift de objetivos en tiempo real.                                                                                                    |
| ASI02 | Tool Misuse \&Exploitation          | El agente usa herramientas leg√≠timas de formas inseguras o no intencionadas ‚Äî encadenando herramientas inofensivas con APIs sensibles, ejecutando comandos destructivos, causando picos de costo por amplificaci√≥n de bucles. Ejemplo: agente de atenci√≥n al cliente con acceso a CRM y sistema de facturaci√≥n es manipulado para emitir cr√©ditos no autorizados en miles de cuentas. La distinci√≥n cr√≠tica: el agente ten√≠a acceso leg√≠timo ‚Äî el abuso ocurre a nivel sem√°ntico. | (1) Definir alcance expl√≠cito de herramientas por agente ‚Äî qu√© puede hacer, no solo a qu√© puede acceder. (2) Rate limits en invocaciones de herramientas por agente/sesi√≥n. (3) Aprobaci√≥n humana obligatoria para acciones sobre datos financieros o irreversibles. (4) Auditor√≠a de cadenas de herramientas.                                                                                           |
| ASI03 | Identity \&Privilege Abuse          | Agentes heredan credenciales de usuarios o sistemas con privilegios elevados ‚Äî tokens de sesi√≥n, claves SSH en memoria, delegaci√≥n cross-agente sin escopado. Los atacantes explotan la escalada de privilegios a trav√©s de cadenas de confianza entre agentes. El problema del 'confused deputy' toma nuevas dimensiones cuando agentes de IA pueden hacerse pasar por servicios internos.                                                                                       | (1) Credenciales de corta duraci√≥n espec√≠ficas por tarea para cada agente. (2) Prohibir la cach√© de credenciales entre sesiones de agente. (3) Autorizaci√≥n expl√≠cita en cada acci√≥n, no herencia de sesi√≥n. (4) Identidades de agente aisladas ‚Äî un agente no puede asumir la identidad de otro sin autorizaci√≥n expl√≠cita.                                                                             |
| ASI04 | Agentic SupplyChain Vulnerabilities | Las dependencias del ecosistema ag√©ntico (plugins, MCP servers, templates de prompt, APIs de terceros, otros agentes) se cargan din√°micamente en tiempo de ejecuci√≥n ‚Äî creando una cadena de suministro viva y vulnerable. El primer MCP server malicioso in-the-wild fue descubierto en npm en septiembre de 2025. Barracuda identific√≥ 43 componentes de frameworks ag√©nticos con vulnerabilidades embebidas de supply chain.                                                   | (1) SBOM/AIBOM ‚Äî inventario de todos los componentes del ecosistema ag√©ntico. (2) Firmar y atestar manifiestos, modelos, prompts y descriptores de herramientas. (3) Pinear dependencias y bloquear fuentes no confiables. (4) Ejecutar agentes en entornos sandbox con restricciones de red. (5) Kill switches que puedan revocar acceso en todo el deployment ante compromiso.                         |
| ASI05 | Insecure AgentCode Execution        | Agentes que generan o ejecutan c√≥digo en tiempo real (coding assistants, agentes de automatizaci√≥n de infraestructura) pueden ser manipulados para ejecutar c√≥digo malicioso a trav√©s de prompts manipulados o serializaci√≥n insegura. Incluye: shell injection a trav√©s de prompts, alucinaciones de c√≥digo con backdoors ocultos, explotaci√≥n de cadenas multi-herramienta. Ejemplo: agente de CI/CD que genera y ejecuta c√≥digo de infraestructura sin revisi√≥n humana.        | (1) Sandbox de ejecuci√≥n de c√≥digo con restricciones de red y filesystem. (2) Revisi√≥n humana obligatoria antes de ejecutar c√≥digo generado por agente en producci√≥n. (3) An√°lisis est√°tico automatizado del c√≥digo generado antes de ejecuci√≥n. (4) Auditor√≠a de todos los artefactos de c√≥digo generados por agentes.                                                                                  |
| ASI06 | Memory \&Context Poisoning          | Adversarios corrompen el contexto almacenado, embeddings o stores RAG con datos maliciosos, causando que el razonamiento futuro del agente sea sesgado o inseguro. El envenenamiento de memoria es persistente ‚Äî el agente contin√∫a comport√°ndose incorrectamente mucho despu√©s del ataque inicial. Ataques demostrados contra la memoria a largo plazo de Gemini y el contexto persistente de ChatGPT.                                                                           | (1) Validaci√≥n de integridad de datos en stores RAG antes de ingesta. (2) Separaci√≥n estricta de fuentes de datos por nivel de confianza. (3) Auditor√≠a peri√≥dica de embeddings almacenados para detectar contaminaci√≥n. (4) L√≠mites de tiempo de vida en contexto persistente de agentes.                                                                                                               |
| ASI07 | Multi-AgentCommunication Attacks    | Sistemas multi-agente dependen de comunicaci√≥n continua que expande significativamente la superficie de ataque. Sin autenticaci√≥n, verificaci√≥n de integridad y validaci√≥n sem√°ntica, atacantes pueden interceptar, suplantar o manipular mensajes entre agentes. Amenazas en transporte, ruteo, descubrimiento y canales laterales donde agentes filtran datos a trav√©s de patrones de timing o comportamiento.                                                                  | (1) Autenticaci√≥n mutua entre agentes en todos los canales de comunicaci√≥n. (2) Cifrado end-to-end en mensajes inter-agente. (3) Validaci√≥n sem√°ntica de mensajes ‚Äî verificar que las instrucciones recibidas son coherentes con el protocolo del sistema. (4) Registro inmutable de todas las comunicaciones inter-agente.                                                                              |
| ASI08 | UncontrolledResource Consumption    | Agentes pueden consumir recursos de forma descontrolada ‚Äî bucles infinitos, amplificaci√≥n de llamadas a APIs, generaci√≥n masiva de datos. Afecta costo (picos de facturaci√≥n no anticipados), disponibilidad (DoS auto-infligido) y rendimiento de sistemas dependientes. Este riesgo es √∫nico en IA ag√©ntica donde el agente puede auto-generar nuevas tareas.                                                                                                                   | (1) Budget de tokens por sesi√≥n de agente con hard limits configurables. (2) Rate limits expl√≠citos en invocaciones de herramientas por agente/tiempo. (3) Circuit breakers que detengan agentes ante comportamiento an√≥malo de consumo. (4) Alertas de costo en tiempo real con umbrales predefinidos.                                                                                                  |
| ASI09 | Human-Agent TrustExploitation       | Usuarios y organizaciones depositan confianza ciega en los outputs y acciones del agente sin la supervisi√≥n apropiada ‚Äî aceptando resultados defectuosos o maliciosos. Este riesgo es humano, no t√©cnico. La confianza excesiva en agentes reduce la vigilancia necesaria para detectar comportamientos an√≥malos.                                                                                                                                                                 | (1) Cultura organizacional de evaluaci√≥n cr√≠tica de outputs ag√©nticos. (2) Human-in-the-loop obligatorio para decisiones de alto impacto. (3) Explicabilidad de acciones ‚Äî el agente debe poder explicar por qu√© tom√≥ cada acci√≥n. (4) Programa de capacitaci√≥n en reconocimiento de fallos ag√©nticos para usuarios finales.                                                                             |
| ASI10 | Rogue Agents                        | Agentes cuyo comportamiento ha derivado de su prop√≥sito original ‚Äî persiguiendo objetivos ocultos, autoreplic√°ndose, secuestrando flujos de trabajo, o optimizando se√±ales de recompensa de formas que da√±an a la organizaci√≥n. A diferencia del Goal Hijacking (ASI01 ‚Äî requiere atacante activo), los Rogue Agents son una amenaza auto-iniciada por fallos de gobernanza y dise√±o interno. Son el equivalente ag√©ntico de la amenaza interna.                                  | (1) Kill switches implementados, auditables y f√≠sicamente aislados ‚Äî no negociable. (2) Monitoreo comportamental continuo para detectar deriva sutil antes de que sea catastr√≥fica. (3) Auditor√≠a rigurosa de la funci√≥n de recompensa/objetivo del agente. (4) Pruebas de red-team espec√≠ficas para comportamiento rogue. (5) Mecanismo de 'behavioral baseline' con alertas ante desv√≠os estad√≠sticos. |

<br>

## 7.4 SHADOW AI: EL RIESGO INVISIBLE ($670K COSTO DIFERENCIAL POR BRECHA)

<br>

El Shadow AI se refiere a herramientas y modelos de IA desplegados dentro de una organizaci√≥n sin la aprobaci√≥n o conocimiento de los equipos de TI y seguridad. Es el equivalente del Shadow IT en la era de la IA ‚Äî y se ha convertido en 2025-2026 en uno de los vectores de riesgo de mayor crecimiento en el espacio enterprise.

<br>

| DATO CLAVE                                                                             | FUENTE Y CONTEXTO                                                                                                                                                                                                                                                     |
| -------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 65% de herramientas de IA en organizaciones operan sin aprobaci√≥n de TI o seguridad    | Vectra AI / IAPP AI Governance Profession Report (2025). El porcentaje refleja el delta entre la velocidad de adopci√≥n de herramientas de IA por usuarios de negocio y la capacidad de los equipos de seguridad de evaluar y aprobar esas herramientas.               |
| Shadow AI representa el 20% de TODAS las brechas de datos en 2025                      | IBM Cost of a Data Breach Report 2025. Este dato, consistente con el patr√≥n hist√≥rico del Shadow IT, indica que la falta de visibilidad sobre qu√© herramientas usan los empleados y qu√© datos comparten con ellas es ya una causa principal de exposici√≥n.            |
| Costo diferencial por brecha asociada a Shadow AI: +$670,000                           | IBM Cost of a Data Breach Report 2025. El costo adicional respecto al promedio de brechas no relacionadas con Shadow AI refleja la dificultad de detecci√≥n, contenci√≥n y remediaci√≥n cuando los controles est√°ndar no conocen la existencia del sistema comprometido. |
| 77% de organizaciones trabajan activamente en gobernanza de IA                         | IAPP AI Governance Profession Report (2025). Sin embargo, la mayor√≠a carece de las herramientas para hacerlo efectivamente ‚Äî existe una brecha entre intenci√≥n y capacidad operacional.                                                                               |
| El 56% de CISOs bloquean la mayor√≠a de herramientas de IA con listas blancas definidas | IANS Research, febrero 2026. Esta estrategia responde al problema de Shadow AI pero puede generar resistencia organizacional y redireccionar el Shadow AI a canales m√°s ocultos (dispositivos personales, accounts personales).                                       |

<br>

### 7.4.1 Framework de Gesti√≥n del Shadow AI

| FASE | ACCI√ìN                | IMPLEMENTACI√ìN PR√ÅCTICA                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ---- | --------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1    | INVENTARIO            | Realizar un inventario comprehensivo de TODOS los sistemas de IA en producci√≥n, incluyendo deployments Shadow AI. Herramientas: an√°lisis de tr√°fico de red para detectar llamadas a APIs de LLM no autorizadas, encuestas an√≥nimas a empleados, revisi√≥n de logs de proxy corporativo, an√°lisis de gasto en tarjetas corporativas (suscripciones a servicios de IA). El inventario debe ser el punto de partida ‚Äî no se puede gobernar lo que no se conoce. |
| 2    | CLASIFICACI√ìN         | Clasificar cada herramienta de IA descubierta: (a) aprobada y gestionada; (b) aprobada pero no gestionada (visible pero sin controles); (c) no aprobada pero de bajo riesgo; (d) no aprobada y de alto riesgo. La clasificaci√≥n determina la respuesta: las categor√≠as (c) y (d) requieren evaluaci√≥n acelerada o bloqueo.                                                                                                                                  |
| 3    | POL√çTICA              | Desarrollar y comunicar una AI Usage Policy clara que defina: herramientas aprobadas (lista blanca), herramientas prohibidas (lista negra, ej. DeepSeek), categor√≠as que requieren aprobaci√≥n previa, tipos de datos que NUNCA pueden enviarse a herramientas de IA externas (PHI, PII, datos confidenciales de negocio, c√≥digo fuente propietario), y consecuencias del incumplimiento.                                                                    |
| 4    | CONTROLES T√âCNICOS    | Implementar controles t√©cnicos que hagan la pol√≠tica aplicable: (a) DLP (Data Loss Prevention) con reglas espec√≠ficas para APIs de IA; (b) Proxy corporativo con inspecci√≥n de tr√°fico a dominios de IA conocidos; (c) Bloqueo de acceso a herramientas no aprobadas desde la red corporativa; (d) Browser extensions o agentes endpoint que detecten uso de herramientas de IA en dispositivos corporativos.                                               |
| 5    | ALTERNATIVAS INTERNAS | La estrategia m√°s efectiva a largo plazo para reducir Shadow AI no es solo bloquear ‚Äî es ofrecer alternativas internas aprobadas que satisfagan las necesidades de los usuarios de negocio. Si los usuarios usan ChatGPT personal porque la empresa no ofrece una alternativa enterprise, el Shadow AI persistir√°. Implementar herramientas aprobadas, capaces y f√°ciles de usar es parte de la gobernanza.                                                 |
| 6    | MONITOREO CONTINUO    | El Shadow AI no es un problema que se resuelve una vez ‚Äî es un estado continuo dado el ritmo de adopci√≥n. Establecer monitoreo continuo: revisi√≥n mensual de herramientas nuevas detectadas, proceso de aprobaci√≥n acelerada para herramientas de bajo riesgo (no crear cuellos de botella que generen resistencia), y revisi√≥n trimestral de la pol√≠tica.                                                                                                  |

<br>

## 7.5 MONITOREO CONTINUO Y GESTI√ìN DE INCIDENTES DE IA

<br>

Una vez que un sistema de IA est√° en producci√≥n, el monitoreo continuo no es opcional ‚Äî es el mecanismo que detecta cuando la realidad diverge de las garant√≠as del due diligence. Los modelos cambian, los proveedores actualizan sus pol√≠ticas, surgen nuevas vulnerabilidades y los patrones de uso evolucionan. El monitoreo continuo transforma la evaluaci√≥n de due diligence de un evento puntual en una pr√°ctica operacional duradera.

<br>

### 7.5.1 √Åreas de Monitoreo Post-Implementaci√≥n

| √ÅREA DE MONITOREO                     | QU√â MONITOREAR                                                                                                                                                                                    | FRECUENCIA Y ALERTAS                                                                                                           |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| Rendimiento del modelo                | Precisi√≥n, recall, tasa de alucinaciones, latencia, disponibilidad. Comparar contra baseline establecido en implementaci√≥n. Detectar model drift (cuando el proveedor actualiza el modelo base).  | Continuo (automatizado). Alerta si m√©tricas caen >X% del baseline. Revisi√≥n manual mensual.                                    |
| Sesgo y fairness                      | Distribuci√≥n de outputs por grupos demogr√°ficos. Disparate impact en casos de uso que afectan decisiones. Especialmente cr√≠tico en: cr√©dito, contrataci√≥n, servicio al cliente.                   | Mensual (an√°lisis estad√≠stico). Revisi√≥n ante cambios de modelo. Alerta si se detecta disparate impact >20%.                   |
| Comportamiento ag√©ntico               | Para sistemas ag√©nticos: patrones de invocaci√≥n de herramientas, duraci√≥n de sesiones, volumen de datos accedidos, cadenas de acciones. Baseline de comportamiento normal + alertas ante desv√≠os. | Continuo (tiempo real para alertas cr√≠ticas). Revisi√≥n diaria de dashboards. Post-mortem ante cualquier anomal√≠a.              |
| Cambios en pol√≠ticas del proveedor    | Actualizaciones de ToS, DPAs, pol√≠ticas de entrenamiento, certificaciones. El proveedor puede cambiar su pol√≠tica ZDR o sus t√©rminos de indemnizaci√≥n con 30 d√≠as de preaviso.                    | Suscripci√≥n a changelog del proveedor. Revisi√≥n mensual de documentos legales. Alerta ante cualquier cambio en t√©rminos clave. |
| Incidentes de seguridad del proveedor | CVEs que afectan los servicios del proveedor. Brechas de seguridad reportadas. Cambios en la superficie de ataque del servicio.                                                                   | Suscripci√≥n a CVE feeds del proveedor. CISA advisories. Revisi√≥n semanal.                                                      |
| Uso y costos                          | Volumen de tokens, llamadas a API, costos por unidad de negocio. Shadow AI puede detectarse via picos de uso no anticipados. Amplificaci√≥n de costos por comportamiento ag√©ntico an√≥malo.         | Continuo (dashboard de costo). Alerta ante picos >200% del baseline semanal. Revisi√≥n mensual de ROI por caso de uso.          |

<br>

### 7.5.2 Plan de Respuesta a Incidentes de IA (AI Incident Response Plan)

El incident response para IA tiene caracter√≠sticas distintas al IR tradicional de ciberseguridad. Los incidentes de IA pueden ser: ataques externos (ASI01-ASI10 OWASP), degradaciones internas (drift, alucinaciones), incumplimientos regulatorios (uso de PHI en sistema no autorizado), o comportamientos no alineados (outputs da√±inos, sesgados o inapropiados). Un AI IRP debe cubrirlos todos:

<br>

| FASE | NOMBRE        | ACCIONES ESPEC√çFICAS PARA INCIDENTES DE IA                                                                                                                                                                                                                                                                                                                                              |
| ---- | ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1    | DETECCI√ìN     | Monitoreo automatizado detecta anomal√≠a. Fuentes: alertas de behavioral analytics, reportes de usuarios, logs de invocaci√≥n de herramientas, alertas de costo. Criterios de escalaci√≥n definidos: impacto financiero, n√∫mero de usuarios afectados, tipo de datos involucrados, si el comportamiento es ag√©ntico o LLM pasivo.                                                          |
| 2    | CONTENCI√ìN    | ACCI√ìN INMEDIATA: Kill switch del agente o sistema de IA afectado. Revocar credenciales de agente comprometido. Aislar el entorno afectado. Si el proveedor tiene responsabilidad: notificar al proveedor con evidencia documentada (logs, capturas) y solicitar SLA de respuesta contractualmente acordado.                                                                            |
| 3    | INVESTIGACI√ìN | Determinar: (a) tipo de incidente ‚Äî ataque externo, fallo interno, incumplimiento regulatorio; (b) alcance ‚Äî qu√© datos accedi√≥/modific√≥ el sistema; (c) causa ra√≠z ‚Äî vulnerabilidad en el dise√±o ag√©ntico, cambio en modelo base del proveedor, ataque a supply chain; (d) si hay obligaci√≥n de notificaci√≥n regulatoria (DORA: 4h para incidentes mayores; HIPAA: 60 d√≠as; GDPR: 72h). |
| 4    | REMEDIACI√ìN   | Correcci√≥n de la causa ra√≠z. Si el incidente involucra cambios del proveedor (actualizaci√≥n de modelo): decidir si se mantiene el proveedor o se activa el plan de salida contractual. Actualizar controles OWASP Agentic Top 10 en base a lecciones aprendidas. Documentar para auditor√≠a regulatoria.                                                                                 |
| 5    | POST-MORTEM   | Revisi√≥n post-incidente dentro de 72h. Actualizaci√≥n de AI IRP con aprendizajes. Comunicaci√≥n a stakeholders internos (comit√© de gobernanza) y externos si aplica (reguladores, clientes afectados). Revisi√≥n de si el incidente deber√≠a haber sido detectado antes y por qu√© no lo fue.                                                                                                |

<br>

## 7.6 EU AI ACT: HOJA DE RUTA DE COMPLIANCE (DEADLINE AGOSTO 2026)

<br>

El EU AI Act (Reglamento UE 2024/1689) es el primer marco regulatorio comprehensivo de IA a nivel mundial, de aplicaci√≥n extraterritorial: aplica a cualquier empresa que desarrolla, despliega o usa sistemas de IA que afectan a usuarios en la UE, independientemente de d√≥nde est√© incorporada. Para organizaciones en LATAM con clientes o filiales en la UE ‚Äî como grupos bancarios europeos con subsidiarias en Argentina, Chile, Colombia, Brasil ‚Äî el EU AI Act es relevancia directa y operacional.

<br>

### 7.6.1 Timeline de Compliance: Hitos Cr√≠ticos

| FECHA            | ESTADO              | OBLIGACIONES Y ACCIONES REQUERIDAS                                                                                                                                                                                                                                                                                                                                                            |
| ---------------- | ------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1 Agosto 2024    | ‚úÖ VIGENTE           | Entrada en vigor del Reglamento. Inicio de per√≠odo de transici√≥n.                                                                                                                                                                                                                                                                                                                             |
| 2 Febrero 2025   | ‚úÖ VIGENTE           | Pr√°cticas prohibidas: sistemas de IA que representan 'riesgo inaceptable' est√°n prohibidos desde esta fecha. Incluye: scoring social, biometr√≠a en tiempo real en espacios p√∫blicos, manipulaci√≥n subliminal. Obligaciones de alfabetizaci√≥n en IA (AI literacy) para proveedores y desplegadores.                                                                                            |
| 2 Agosto 2025    | ‚úÖ VIGENTE           | Obligaciones para modelos GPAI (General Purpose AI): documentaci√≥n t√©cnica, evaluaciones de seguridad, gesti√≥n de riesgos sist√©micos para modelos frontier. Los principales proveedores (OpenAI, Anthropic, Google, Microsoft, Meta, etc.) ya est√°n sujetos a estas obligaciones. Multas por GPAI: hasta ‚Ç¨15M o 3% de facturaci√≥n global.                                                     |
| 2 Agosto 2026    | ‚ö†Ô∏è PR√ìXIMO(6 meses) | Aplicaci√≥n plena para sistemas de ALTO RIESGO (Annex III): cr√©dito, scoring financiero, detecci√≥n de fraude, decisiones que afectan acceso a servicios esenciales, sistemas de contrataci√≥n, herramientas educativas. Multas por incumplimiento: hasta ‚Ç¨35M o 7% facturaci√≥n global. Sanciones aplican tanto a proveedores de IA como a deployers (las organizaciones que usan los sistemas). |
| 2 Diciembre 2027 | Backstop legal      | Fecha m√°xima para compliance de sistemas alto riesgo Annex III si los est√°ndares armonizados no estaban disponibles antes de agosto 2026 (Digital Omnibus proposal, noviembre 2025). Organizaciones deben planificar para agosto 2026, no confiar en el backstop.                                                                                                                             |
| 2 Agosto 2027    | Aplicaci√≥n plena    | El EU AI Act aplica en su totalidad a todas las categor√≠as restantes. Modelos GPAI ya en mercado antes de agosto 2025 deben estar en plena conformidad.                                                                                                                                                                                                                                       |
| 2 Agosto 2028    | Productos embebidos | Sistemas de IA integrados en productos regulados (dispositivos m√©dicos, maquinaria industrial, veh√≠culos) con deadline extendido de August 2028 m√°ximo (Annex I).                                                                                                                                                                                                                             |

<br>

üö® Urgencia ‚Äî Agosto 2026: 6 meses: K\&L Gates (enero 2026): 'Los organismos reguladores de alto nivel estiman que las organizaciones que comienzan hoy APENAS tienen tiempo suficiente para cumplir con el deadline de agosto 2026.' Las estimaciones de inversi√≥n: grandes empresas (>‚Ç¨1B): $8-15M inicial para sistemas alto riesgo. Empresas medianas: $2-5M inicial. El incumplimiento no es solo una multa ‚Äî puede implicar prohibici√≥n de operar el sistema en el mercado de la UE.

<br>

### 7.6.2 Requisitos para Sistemas de Alto Riesgo: Checklist de Implementaci√≥n

| ART.       | REQUISITO EU AI ACT                                 | ACCI√ìN DE IMPLEMENTACI√ìN                                                                                                                                                                                                                                               |
| ---------- | --------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Art. 9     | Sistema de gesti√≥n de riesgos (c√≠clico, no puntual) | Implementar proceso continuo de identificaci√≥n, evaluaci√≥n y mitigaci√≥n de riesgos de IA. Documentar en el registro de riesgos corporativo. Revisi√≥n m√≠nima anual y ante cambios materiales del sistema.                                                               |
| Art. 10    | Gobernanza de datos de entrenamiento                | Documentar proveniencia de datos de entrenamiento del proveedor. Verificar representatividad y ausencia de sesgos relevantes. Exigir al proveedor documentaci√≥n de Model Card. Si es modelo propio: mantener dataset documentation.                                    |
| Art. 11    | Documentaci√≥n t√©cnica                               | Mantener documentaci√≥n t√©cnica exhaustiva del sistema de IA: arquitectura, datos, limitaciones, condiciones de uso apropiado, m√©tricas de rendimiento, resultados de evaluaciones de seguridad. Accesible para autoridades competentes.                                |
| Art. 12    | Registro autom√°tico (logs)                          | Los sistemas de alto riesgo deben generar logs autom√°ticos que permitan: (a) monitorear el funcionamiento; (b) facilitar supervisi√≥n post-mercado; (c) garantizar trazabilidad de las operaciones del sistema. Retenci√≥n de logs m√≠nima definida por sector.           |
| Art. 13    | Transparencia a desplegadores                       | Proveedor debe proveer informaci√≥n suficiente para que el deployer entienda: capacidades y limitaciones, performance esperada, requisitos de mantenimiento, medidas de supervisi√≥n humana necesarias. Exigir al proveedor compliance con este art√≠culo en el contrato. |
| Art. 14    | Supervisi√≥n humana efectiva                         | Dise√±ar e implementar medidas que permitan a personas f√≠sicas supervisar el sistema de IA y intervenir cuando sea necesario. Los controles de supervisi√≥n deben ser apropiados al riesgo del caso de uso. Prohibido el 'human in the loop' cosm√©tico.                  |
| Art. 15    | Precisi√≥n, robustez y ciberseguridad                | Verificar y documentar m√©tricas de precisi√≥n del sistema. Evaluar robustez ante inputs an√≥malos o adversariales. Implementar medidas de ciberseguridad espec√≠ficas para el sistema de IA (OWASP Agentic Top 10 aplicable).                                             |
| Art. 16-17 | Registro en base de datos EU + QMS                  | Registrar sistema de alto riesgo en la EU AI Database p√∫blica (cuando est√© operativa). Implementar sistema de gesti√≥n de calidad (QMS) para el ciclo de vida del sistema de IA. Nombrar representante autorizado en la UE si el proveedor es no-UE.                    |

<br>

### 7.6.3 Aplicabilidad para Organizaciones en LATAM

Para organizaciones con sede en LATAM pero con exposici√≥n al mercado europeo, la aplicabilidad del EU AI Act depende del rol en la cadena de valor de la IA:

| ESCENARIO LATAM                                                        | ROL EN EU AI ACT                    | OBLIGACIONES                                                                                                                                                                                                                                                                                                     |
| ---------------------------------------------------------------------- | ----------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Filial de grupo bancario europeo en Argentina/Chile/Colombia           | Deployer de sistemas de alto riesgo | Obligaciones de deployer: supervisi√≥n humana, logs, reporte de incidentes, cooperaci√≥n con autoridades. El proveedor de IA debe cumplir las obligaciones de proveedor.                                                                                                                                           |
| Empresa LATAM que exporta servicios de IA a clientes europeos          | Proveedor de IA con alcance EU      | Obligaciones de proveedor: documentaci√≥n t√©cnica, evaluaci√≥n de conformidad, registro EU AI Database, representante autorizado en UE. Mucho m√°s exigente que ser deployer.                                                                                                                                       |
| Empresa LATAM que usa IA solo para clientes locales sin filiales EU    | Fuera del alcance directo           | EU AI Act no aplica directamente. Sin embargo: LATAM pa√≠ses est√°n desarrollando regulaciones propias (Brasil LGPD + regulaci√≥n IA en desarrollo, Colombia, M√©xico). Los est√°ndares EU son referencia para reguladores locales ‚Äî prepararse para EU AI Act es prepararse para el futuro regulatorio de la regi√≥n. |
| Empresa LATAM usando proveedores de IA globales para procesos internos | Deployer ‚Äî evaluaci√≥n caso a caso   | Si los procesos impactan a clientes/empleados en la UE: deployer con obligaciones. Si el impacto es solo en LATAM: posiblemente fuera de alcance. Requiere an√°lisis legal espec√≠fico.                                                                                                                            |

<br>

## 7.7 FRAMEWORK DE RE-EVALUACI√ìN PERI√ìDICA DE PROVEEDORES

<br>

La due diligence de las Fases 1-6 estableci√≥ el baseline de evaluaci√≥n de proveedores en el momento de la selecci√≥n. Sin embargo, los proveedores cambian: actualizan pol√≠ticas de privacidad, sufren brechas de seguridad, cambian de propietario, resuelven litigaciones activas, modifican sus modelos base, o reciben nuevas investigaciones regulatorias. El framework de re-evaluaci√≥n convierte esa evaluaci√≥n puntual en un proceso continuo.

<br>

| FRECUENCIA    | TIPO DE REVISI√ìN            | ALCANCE                                                                                                                                                                                                                                 | RESPONSABLE                               |
| ------------- | --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- |
| CONTINUO      | Monitoreo automatizado      | Cambios en ToS, nuevas CVEs, brechas reportadas, cambios regulatorios que afecten al proveedor. Suscripci√≥n a feeds de noticias y advisories.                                                                                           | CISO / Equipo de seguridad (automatizado) |
| MENSUAL       | Revisi√≥n operacional        | M√©tricas de rendimiento vs. SLA. Comportamiento ag√©ntico (si aplica). Costos vs. presupuesto. Incidentes del mes. Shadow AI detectado.                                                                                                  | AI Governance Committee                   |
| TRIMESTRAL    | Revisi√≥n t√°ctica            | An√°lisis de litigaci√≥n activa del proveedor. Cambios en pol√≠ticas de entrenamiento documentados. Nuevas certificaciones obtenidas o perdidas. Actualizaci√≥n del scorecard de la Fase 5 (sectorial).                                     | CISO + Legal                              |
| SEMESTRAL     | Revisi√≥n estrat√©gica        | Re-evaluaci√≥n completa de la filosof√≠a √©tica del proveedor (hallazgos tipo Fases 1-4). Evaluaci√≥n de cambios en liderazgo o propiedad. Revisi√≥n del panorama competitivo ‚Äî ¬øhay un proveedor mejor?                                     | AI Governance Committee + C-suite         |
| ANUAL         | Auditor√≠a formal            | Auditor√≠a completa contra marco de gobernanza (ISO 42001, NIST AI RMF, EU AI Act para sistemas alto riesgo). Revisi√≥n contractual: ¬øsiguen vigentes todas las cl√°usulas negociadas? Evaluaci√≥n de plan de salida.                       | Internal Audit + CISO + Legal             |
| EVENTO-DRIVEN | Re-evaluaci√≥n de emergencia | Disparadores: brecha de seguridad mayor del proveedor, cambio de control corporativo (adquisici√≥n, fusi√≥n), investigaci√≥n regulatoria significativa, cambio de modelo base no notificado, incidente de seguridad de IA de alto impacto. | CISO ‚Äî activaci√≥n inmediata               |

<br>

### 7.7.1 Disparadores de Re-Evaluaci√≥n Inmediata

Los siguientes eventos deben disparar una re-evaluaci√≥n no planificada del proveedor independientemente del ciclo de revisi√≥n regular:

| DISPARADOR                                           | ACCI√ìN REQUERIDA                                                                                                                                                                                                              |
| ---------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Brecha de seguridad mayor confirmada en el proveedor | Activar cl√°usula de notificaci√≥n de incidentes del contrato. Evaluar impacto en datos propios. Considerar suspensi√≥n preventiva del servicio hasta aclaraci√≥n. Solicitar informe forense al proveedor.                        |
| Cambio de control corporativo (M\&A, adquisici√≥n)    | Los compromisos contractuales pre-adquisici√≥n pueden no ser vinculantes para el nuevo propietario. Revisar si el nuevo propietario tiene pol√≠ticas compatibles. Activar cl√°usula de cambio de control del contrato si existe. |
| Actualizaci√≥n de modelo base no notificada           | Los modelos actualizados pueden tener rendimiento diferente, nuevas vulnerabilidades o cambios en comportamiento relevantes para compliance (SaMD en salud, SR 11-7 en banca). Exigir notificaci√≥n previa contractualmente.   |
| Nueva investigaci√≥n regulatoria significativa        | Investigaciones de privacidad (CNIL, Garante), sesgo (EEOC), seguridad (CISA) pueden resultar en cambios operacionales forzados o sanciones. Evaluar impacto de posibles outcomes en continuidad del servicio.                |
| CVE cr√≠tica publicada afectando el servicio          | Evaluar si la vulnerabilidad aplica al caso de uso propio. Verificar si el proveedor ha publicado mitigaci√≥n o parche. Implementar controles compensatorios si hay demora.                                                    |
| Cambio material en pol√≠ticas de ZDR o entrenamiento  | Si el proveedor actualiza sus t√©rminos para permitir el uso de datos del cliente para entrenamiento o reduce las garant√≠as de retenci√≥n, puede representar incumplimiento de los compromisos contractuales negociados.        |
| Fallo de renovaci√≥n de certificaci√≥n cr√≠tica         | Si el proveedor pierde una certificaci√≥n requerida (SOC 2, ISO 27001, HIPAA BAA) ‚Äî especialmente en sectores regulados ‚Äî puede representar incumplimiento directo de los requisitos del sector.                               |

<br>

## 7.8 SCORECARD DE MADUREZ DE GOBERNANZA DE IA

<br>

El scorecard de madurez permite a las organizaciones evaluar su posici√≥n actual en gobernanza de IA y planificar una trayectoria de mejora. Est√° organizado en cinco dimensiones que reflejan los temas clave de la Fase 7. Para cada dimensi√≥n, se definen cuatro niveles de madurez: Inicial (sin controles formales), Desarrollando (controles parciales), Definido (controles formales y documentados), Optimizado (controles automatizados y mejora continua).

<br>

| DIMENSI√ìN                             | NIVEL 1INICIAL                                                                                         | NIVEL 2DESARROLLANDO                                                                                 | NIVEL 3DEFINIDO                                                                                                                           | NIVEL 4OPTIMIZADO                                                                                                             |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| 1. Estructura de Gobernanza           | No existe comit√© de gobernanza de IA. El CISO no tiene mandato formal sobre IA.                        | Comit√© informal o ad hoc. Roles de gobernanza de IA en discusi√≥n pero no formalizados.               | AI Governance Committee formal con RACI definido. Reuniones mensuales. Pol√≠ticas de IA documentadas y comunicadas.                        | Comit√© operativo con m√©tricas de KPI de gobernanza. AI Champions en cada unidad de negocio. Mejora continua documentada.      |
| 2. Inventario y Clasificaci√≥n de IA   | No existe inventario de sistemas de IA en producci√≥n. Shadow AI desconocido.                           | Inventario parcial de sistemas aprobados. Shadow AI no medido.                                       | Inventario completo de todos los sistemas de IA (incluyendo Shadow AI detectado). Clasificaci√≥n por riesgo EU AI Act.                     | Inventario automatizado con discovery continuo. Integrado con CMDB. Alertas ante nuevos sistemas no registrados.              |
| 3. Gesti√≥n de Riesgo Ag√©ntico (OWASP) | No se han implementado controles espec√≠ficos para IA ag√©ntica. Principio de Menor Agencia desconocido. | Conocimiento del OWASP Agentic Top 10 pero sin implementaci√≥n sistem√°tica. Algunos controles ad hoc. | Implementaci√≥n formal del Principio de Menor Agencia. Kill switches implementados y probados. Observabilidad b√°sica de agentes.           | Controles OWASP Agentic Top 10 implementados y auditados. Red teaming ag√©ntico peri√≥dico. Behavioral analytics automatizado.  |
| 4. Compliance Regulatorio             | EU AI Act no analizado. NIST AI RMF desconocido. ISO 42001 no implementado.                            | An√°lisis preliminar de aplicabilidad de EU AI Act. Mapeo inicial de sistemas alto riesgo.            | Clasificaci√≥n completa de sistemas bajo EU AI Act. Plan de compliance documentado con responsables y fechas. ISO 42001 en implementaci√≥n. | Compliance total EU AI Act para sistemas alto riesgo. ISO 42001 certificado. Monitoring automatizado de cambios regulatorios. |
| 5. Gesti√≥n Continua de Proveedores    | Evaluaci√≥n de proveedores solo en el momento de selecci√≥n. Sin re-evaluaci√≥n peri√≥dica.                | Re-evaluaci√≥n anual informal. Sin monitoreo continuo de cambios en pol√≠ticas de proveedores.         | Re-evaluaci√≥n semestral formal con scorecard documentado. Monitoreo mensual de cambios en ToS y CVEs. Planes de salida definidos.         | Re-evaluaci√≥n continua automatizada. Alertas en tiempo real ante cambios en proveedores. Pruebas anuales de planes de salida. |

<br>

üéØ Meta 2026 para organizaciones en sectores regulados: Organizaciones con exposici√≥n al EU AI Act (sistemas alto riesgo) o en sectores regulados (financiero, salud, gobierno) deben alcanzar NIVEL 3 en todas las dimensiones antes de agosto 2026, y tener un roadmap documentado hacia NIVEL 4 para finales de 2026. Organizaciones que inician hoy tienen tiempo justo ‚Äî la complejidad de implementaci√≥n suele subestimarse un 40-60% (estimaci√≥n basada en proyectos de compliance an√°logos GDPR 2018).

<br>

## 7.9 REFERENCIAS

<br>

OWASP Agentic Security Initiative ‚Äî Fuentes Primarias:

1. R145. OWASP GenAI Security Project, Top 10 for Agentic Applications 2026. genai.owasp.org/resource/owasp-top-10-for-agentic-applications-for-2026/ (diciembre 2025).
2. R146. OWASP GenAI Security Project, State of Agentic AI Security and Governance 1.0. genai.owasp.org (agosto 2025).
3. R147. OWASP GenAI Security Project, Agentic AI Threats and Mitigations. genai.owasp.org (febrero 2025).
4. R148. OWASP GenAI Security Project, Practical Guide for Securely Using Third-Party MCP Servers. genai.owasp.org.
5. R149. LittleData, OWASP Agentic AI Top 10: Critical Security Risks in 2026. littledata.com (febrero 2026).
6. R150. CSO Online, Managing Agentic AI Risk: Lessons from the OWASP Top 10. csoonline.com (diciembre 2025).
7. R151. Barracuda Security, 43 agent framework components with embedded supply chain vulnerabilities (referenciado en OWASP documentation 2025).

<br>

EU AI Act ‚Äî Fuentes Regulatorias y An√°lisis:

1. R152. K\&L Gates, EU and Luxembourg Update on the European Harmonised Rules on AI ‚Äî Recent Developments. klgates.com (enero 2026). Incluye an√°lisis Digital Omnibus noviembre 2025.
2. R153. European Commission, AI Act. digital-strategy.ec.europa.eu (febrero 2026). Actualizaci√≥n sobre Digital Package on Simplification.
3. R154. artificialintelligenceact.eu, Implementation Timeline. Tracker oficial de plazos.
4. R155. Axis Intelligence, EU AI Act News 2026: Compliance Requirements & Deadlines. axis-intelligence.com.
5. R156. SIG (Software Improvement Group), Comprehensive EU AI Act Summary ‚Äî January 2026 update. softwareimprovementgroup.com.
6. R157. Dataguard, EU AI Act Compliance Timeline: Key Dates & Deadlines Explained. dataguard.com.
7. R158. Trilateral Research, EU AI Act Implementation Timeline: Mapping Models to Risk Tiers. trilateralresearch.com.
8. R159. LegalNodes, EU AI Act 2026 Updates: Compliance Requirements and Business Risks. legalnodes.com (febrero 2026).

<br>

CISO y Gobernanza de IA Enterprise ‚Äî 2025-2026:

1. R160. IANS Research, The CISO's Expanding AI Mandate: Leading Governance in 2026. iansresearch.com (febrero 2026).
2. R161. Vectra AI, AI Governance Tools: Selection and Security Guide for 2026. vectra.ai (febrero 2026). Incluye dato 65% Shadow AI y $670K diferencial.
3. R162. IAPP, AI Governance Vendor Report 2026. iapp.org (enero 2026).
4. R163. IBM, Cost of a Data Breach Report 2025. ibm.com. Shadow AI 20% de brechas, $670K diferencial.
5. R164. Liminal AI, Enterprise AI Governance: Complete Implementation Guide (2025). liminal.ai.
6. R165. TrustCloud, The 2025 CISOs Guide to AI Governance. trustcloud.ai (julio 2025).
7. R166. Conifers AI, The Enterprise AI SOC: A CISOs Guide From Pilot to Production in 2026. conifers.ai. Incluye Forrester AEGIS framework.
8. R167. Corporate Compliance Insights, 2026 Operational Guide to Cybersecurity, AI Governance & Emerging Risks. corporatecomplianceinsights.com (enero 2026).
9. R168. Medium/Ehsan Idawi, AI in 2025: The Cybersecurity Imperative for Mandatory Governance. medium.com (diciembre 2025).

<br>

Marcos T√©cnicos de Gobernanza de IA:

1. R169. NIST, AI Risk Management Framework (AI RMF 1.0). nist.gov/artificial-intelligence (enero 2023).
2. R170. NIST, AI RMF Playbook ‚Äî Financial Services Profile (2024).
3. R171. NIST, Artificial Intelligence Cybersecurity Profile (2024).
4. R172. ISO/IEC 42001:2023, Information Technology ‚Äî Artificial Intelligence ‚Äî Management System. iso.org.
5. R173. IBM + e&, Enterprise-Grade Agentic AI to Transform Governance and Compliance. newsroom.ibm.com (enero 2026). Ejemplo de implementaci√≥n real con watsonx Orchestrate + IBM OpenPages.
6. R174. Sombra Inc., Ultimate Guide to AI Regulations and Governance in 2026. sombrainc.com (octubre 2025).
7. R175. MITRE ATLAS, Adversarial Threat Landscape for Artificial-Intelligence Systems. atlas.mitre.org. Framework complementario para threat modeling de IA.
8. R176. Dark Reading poll: 48% of cybersecurity professionals identify agentic AI as top attack vector for 2026 (referenciado en m√∫ltiples fuentes, 2026).
9. R177. Gartner, Innovation Insight: AI SOC Agents (octubre 2025). Eval√∫a 7 √°reas de valor de AI en operaciones de seguridad.
10. R178. IAPP AI Governance Profession Report 2025. 77% organizaciones trabajando en gobernanza de IA. 3x mayor madurez con liderazgo C-suite.
11. R179. Precedence Research / MarketsandMarkets: AI governance market ‚Äî $227-340M (2024-2025) a $4.83B (2034). CAGR 35-45%.
12. R180. Transcend, EU AI Act Implementation Timeline: Key Milestones for Enforcement. transcend.io.
