---
icon: check
---

# F16 - 16.0 INTRODUCCI√ìN: EL FIN DEL BLACK BOX COMO NORMA ACEPTABLE





16.0  Introducci√≥n: El Fin del Black Box como Norma Aceptable

16.1  Taxonom√≠a de la Explainabilidad: Tipos, Dimensiones y Audiencias

16.2  T√©cnicas XAI: SHAP, LIME, Mechanistic Interpretability y el Estado del Arte 2025-2026

16.3  Model Cards, System Cards y AI FactSheets: Documentaci√≥n de Accountability

16.4  Fairness Algor√≠tmica: M√©tricas, Trade-offs y Mitigaci√≥n

16.5  Microsoft Responsible AI Dashboard: El Stack de XAI para Azure ML

16.6  EU AI Act y Explainabilidad: Obligaciones, Plazos y Penalidades

16.7  Plan de XAI para el CISO con M365/Copilot/Azure AI Foundry

16.8  Referencias (R421‚ÄìR450)

## 16.0 INTRODUCCI√ìN: EL FIN DEL BLACK BOX COMO NORMA ACEPTABLE

<br>

Durante d√©cadas, el principio impl√≠cito del machine learning en producci√≥n fue: si el modelo funciona ‚Äîsi minimiza el error en el test set, maximiza el AUC, produce retornos en backtesting‚Äî no es necesario entender por qu√© funciona. El black box era un costo aceptable de la performance. Enero 2026 marca el fin formal de esa era: el EU AI Act ‚Äîcon penalidades de hasta 6% del revenue global‚Äî y el GDPR ‚Äî'derecho a explicaci√≥n' bajo Art. 22‚Äî han hecho que la opacidad algor√≠tmica sea no solo √©ticamente cuestionable sino jur√≠dicamente costosa.

En este contexto, Explainable AI (XAI) ha pasado de disciplina acad√©mica de nicho a requisito central del stack de IA enterprise. La evidencia es cuantitativa: organizaciones con XAI comprensivo reportan 31% de reducci√≥n en ciclos de debugging, 24% de reducci√≥n en incidentes de sesgo, 18% de mejora en trust metrics. Un banco espa√±ol que implement√≥ SHAP para explicar rechazos de cr√©dito redujo disputas en un 30%. Goldman Sachs usa SHAP para credit scoring satisfaciendo simult√°neamente GDPR Art. 22 y la Equal Credit Opportunity Act de EE.UU.

<br>

| DIMENSI√ìN                              | ERA BLACK BOX (pre-2024)                                                                         | ERA XAI OBLIGATORIA (2025-2026)                                                                                                                                                    |
| -------------------------------------- | ------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Criterio de √©xito del modelo           | Accuracy, AUC, RMSE en test set. El 'why' es secundario o irrelevante.                           | Accuracy + Explainability + Fairness como criterios co-primarios. Un modelo preciso pero opaco puede ser rechazado en producci√≥n o sancionado por el regulador.                    |
| Responsabilidad cuando el modelo falla | Difusa: 'el modelo decidi√≥'. Proveedor, data scientist y decisor final comparten la opacidad.    | Clara y legal: EU AI Act asigna responsabilidad expl√≠cita al 'deployer'. El CISO necesita poder explicar qu√© decidi√≥ el modelo y por qu√©.                                          |
| Audiencia de las explicaciones         | T√©cnica: data scientists que debuggean el modelo.                                                | M√∫ltiple: reguladores (conformity assessment), usuarios afectados (derecho a explicaci√≥n GDPR Art. 22), Board (governance), DPO (DPIA).                                            |
| Timing de la explicaci√≥n               | Post-hoc: capa de explicaci√≥n a√±adida despu√©s de que el modelo est√° en producci√≥n.               | By-design: explainabilidad dise√±ada desde el inicio del pipeline. 'Explainability by design' como nuevo est√°ndar de excelencia.                                                    |
| Trade-off con performance              | Aceptado como inevitable: m√°s explainabilidad = menos accuracy.                                  | Cuestionado: modelos glass-box (EBM, GAMs) compiten en accuracy con black-boxes en muchos dominios. Mechanistic Interpretability mapea circuitos en LLMs sin sacrificar capacidad. |
| Postura regulatoria                    | Ninguna espec√≠fica para IA. Reguladores financieros ped√≠an explicaciones informales caso a caso. | EU AI Act Art. 13-17: transparencia y documentaci√≥n obligatorias para sistemas de alto riesgo. Penalty: hasta 6% revenue global (50% mayor que GDPR).                              |

<br>

üìà Impacto empresarial cuantificado 2025: XAI comprensivo: 31% m√°s r√°pido en debugging, 24% reducci√≥n incidentes de sesgo, 18% mejora trust metrics (EthicalXAI Platform, 2025). Banco espa√±ol + SHAP para rechazos de cr√©dito: 30% reducci√≥n en disputas. Goldman Sachs SHAP: credit scoring + GDPR Art. 22 + ECOA compliance simult√°neos. EU AI Act penalty m√°xima: 6% revenue global ‚Äî 50% m√°s que GDPR (4%).

<br>

## 16.1 TAXONOM√çA DE LA EXPLAINABILIDAD: TIPOS, DIMENSIONES Y AUDIENCIAS

<br>

La explainabilidad no es un concepto monol√≠tico sino un espacio multidimensional. La misma t√©cnica puede ser apropiada para un data scientist e in√∫til para un cliente que recibi√≥ un rechazo crediticio. El CISO que dise√±a una estrategia XAI debe navegar este espacio: diferentes tipos, alcances, audiencias y requerimientos regulatorios.

<br>

| DIMENSI√ìN                                         | CATEGOR√çAS                                                                                                                                                                                                                                                                                                                                                | IMPLICACI√ìN PARA EL CISO                                                                                                                                                                                                                                                                                                             |
| ------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| TIPO ‚Äî ¬øCu√°ndo se genera la explicaci√≥n?          | ANTE-HOC (Intrinsic): el modelo es inherentemente interpretable ‚Äî no necesita capa adicional. √Årboles de decisi√≥n, regresi√≥n log√≠stica, GAMs, EBM. POST-HOC: modelo black-box con capa de explicaci√≥n a√±adida post-training. SHAP, LIME, Integrated Gradients.                                                                                            | Para modelos propios de bajo riesgo: evaluar si un EBM alcanza accuracy suficiente ‚Äî si s√≠, preferirlo. Para alto riesgo: post-hoc con SHAP o InterpretML. Para LLMs propietarios (Copilot, GPT-4o): solo post-hoc es posible ‚Äî y a nivel de fuentes, no de razonamiento interno.                                                    |
| ALCANCE ‚Äî ¬øQu√© explica la explicaci√≥n?            | GLOBAL: por qu√© el modelo toma sus decisiones en general ‚Äî cu√°les features son m√°s importantes en el conjunto total. Feature importance SHAP global, partial dependence plots. LOCAL: por qu√© el modelo tom√≥ esta decisi√≥n espec√≠fica sobre este individuo en este momento. SHAP waterfall plot, LIME para una predicci√≥n.                                | Para compliance regulatorio (EU AI Act Art. 13, GDPR Art. 22): las explicaciones locales son las obligatorias ‚Äî el individuo tiene derecho a saber por qu√© se tom√≥ una decisi√≥n sobre √©l espec√≠ficamente. Las globales son √∫tiles para auditores y Board. Ambas se necesitan.                                                        |
| AUDIENCIA ‚Äî ¬øPara qui√©n es la explicaci√≥n?        | T√âCNICA: data scientists, auditores. Pueden consumir SHAP values, feature importance plots, error cohorts. EJECUTIVA: CISO, DPO, Board. Necesitan scorecards y res√∫menes de riesgo. USUARIO FINAL: el individuo afectado. Necesita lenguaje natural, simple, accionable. REGULADORA: autoridad supervisora. Necesita documentaci√≥n t√©cnica y Model Cards. | El error m√°s com√∫n en XAI enterprise: generar explicaciones t√©cnicas excelentes para el data scientist y olvidar al usuario final y al regulador. El CISO debe asegurar que el pipeline cubre las 4 audiencias ‚Äî con formatos diferentes para cada una.                                                                              |
| FIDELIDAD ‚Äî ¬øQu√© tan bien refleja el modelo real? | ALTA: la explicaci√≥n refleja con precisi√≥n el razonamiento real del modelo. Dif√≠cil en redes neuronales profundas y transformers. BAJA: aproximaci√≥n simplificada que puede ser misleading. El riesgo: una explicaci√≥n de baja fidelidad genera confianza falsa (automation bias).                                                                        | El EDPS (European Data Protection Supervisor) advierte: explicaciones post-hoc de baja fidelidad pueden generar 'automation bias' ‚Äî usuarios aceptan la decisi√≥n del modelo sin cuestionamiento cr√≠tico porque recibieron una explicaci√≥n aparentemente razonada. La evaluaci√≥n de fidelidad es parte del proceso de validaci√≥n XAI. |

<br>

## 16.2 T√âCNICAS XAI: SHAP, LIME, MECHANISTIC INTERPRETABILITY Y EL ESTADO DEL ARTE 2025-2026

<br>

El ecosistema de t√©cnicas XAI ha madurado significativamente. SHAP y LIME dominan la adopci√≥n enterprise. La frontera del campo est√° siendo redefinida por Mechanistic Interpretability ‚Äî que busca entender los 'circuitos' internos de las redes neuronales. Un breakthrough en 2025 con Sparse Autoencoders (SAEs) permiti√≥ mapear con un detalle sin precedentes qu√© features activan qu√© neuronas en LLMs, con implicaciones directas para la seguridad.

<br>

| T√âCNICA                                                | FUNDAMENTO                                                                                                                                                                                                             | FORTALEZAS / LIMITACIONES                                                                                                                                                                                                                                                                             | MEJOR PARA                                                                                                                                                  | ADOPCI√ìN 2025                                                                                                                                                              |
| ------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| SHAP (SHapley Additive exPlanations)                   | Shapley values de teor√≠a de juegos cooperativos. La contribuci√≥n marginal promedio de cada feature, promediada sobre todas las coaliciones posibles. Garant√≠as: eficiencia, simetr√≠a, dummy, aditividad.               | + Fundamento matem√°tico riguroso. + Global Y local desde el mismo framework. + Consistencia garantizada. ‚Äì Costoso computacionalmente para modelos grandes (mitigado con TreeSHAP). ‚Äì Solo aproximaciones para LLMs masivos.                                                                          | Modelos estructurados (XGBoost, LightGBM, Random Forest). Sectores financieros con exigencia de auditor√≠a. Credit scoring + GDPR Art. 22.                   | Goldman Sachs credit scoring. JPMorgan ECOA compliance. Est√°ndar de facto para ML estructurado. pip install shap. Integrado en InterpretML + Azure ML.                     |
| LIME (Local Interpretable Model-agnostic Explanations) | Perturbaci√≥n local: genera variantes sint√©ticas del input, observa cambios en output, ajusta modelo lineal local ponderado por proximidad. El modelo lineal local es la explicaci√≥n.                                   | + Model-agnostic (cualquier black-box). + Explicaciones locales intuitivas (60% influido por X, 25% por Y). ‚Äì Inestabilidad: inputs similares pueden generar explicaciones diferentes si hay fronteras de decisi√≥n complejas. ‚Äì Sin garant√≠a matem√°tica de fidelidad.                                 | Explicaciones individuales para usuarios finales (rechazos de cr√©dito). Modelos de texto e im√°genes. Prototyping r√°pido.                                    | PayPal fraud detection. BBVA Mercury library. Interfaz de explicaci√≥n customer-facing. pip install lime. Integrado en InterpretML.                                         |
| Integrated Gradients                                   | Para modelos diferenciables (redes neuronales): integra los gradientes del output respecto al input desde un baseline (vac√≠o) hasta el input real. Resultado: atribuci√≥n por feature o token.                          | + Exacto matem√°ticamente para modelos diferenciables. + Satisface completeness (suma = output - baseline). + Excelente para texto e im√°genes. ‚Äì Requiere acceso a gradientes (white-box). ‚Äì Para GPT-4o/Copilot: no disponible (API sin gradientes).                                                  | LLMs propios en Azure AI Foundry con acceso a gradientes. Computer vision (Google DeepMind eye disease: saliency maps sobre retinal imaging).               | PyTorch Captum. torch.autograd. Relevante para modelos propios Azure AI Foundry con acceso completo.                                                                       |
| Mechanistic Interpretability (SAEs)                    | Sparse Autoencoders aprenden representaciones dispersas de las activaciones neuronales. Identifican qu√© 'features' conceptuales activan qu√© neuronas. Breakthrough 2025: mapping de miles de features en transformers. | + √önica t√©cnica que busca entender el razonamiento real del modelo, no aproximarlo. + Implicaciones de seguridad: identificar circuitos que amplifican sesgos o responden a prompts adversariales. ‚Äì Investigaci√≥n, no producci√≥n estable para la mayor√≠a. ‚Äì Requiere acceso a activaciones internas. | Organizaciones con LLMs propios en Azure AI Foundry. Safety research. Comprensi√≥n de comportamientos emergentes.                                            | Anthropic: SAEs para Claude 3 Sonnet ('Scaling Monosemanticity', 2024). Google Gemma Scope (open release 2025). Microsoft Research: investigaci√≥n activa.                  |
| EBM (Explainable Boosting Machine)                     | Modelo glass-box: GAMs con interacciones de pares detectadas autom√°ticamente. Cada feature tiene una shape function visualizable directamente. No requiere post-hoc.                                                   | + Accuracy comparable a gradient boosting en muchos datasets estructurados. + Completamente interpretable sin capa adicional. + Auditable directamente por el regulador. ‚Äì No √∫til para texto/im√°genes sin feature engineering. ‚Äì Alta dimensionalidad penaliza m√°s.                                  | Casos donde hay opci√≥n de elegir el modelo. Sectores altamente regulados donde el regulador audita la funci√≥n directamente. Healthcare, credit, HR scoring. | Microsoft InterpretML: librer√≠a flagship para EBMs. pip install interpret. Integrado en Responsible AI Dashboard (Azure ML). Recomendado en MS Responsible AI Standard v2. |

<br>

üî¨ Mechanistic Interpretability ‚Äî El Salto Cualitativo de 2025: Las t√©cnicas post-hoc (SHAP, LIME) explican los inputs y outputs de los modelos. Mechanistic Interpretability busca entender los circuitos internos. Breakthrough 2025 con SAEs: Anthropic mape√≥ miles de features conceptuales en Claude 3 Sonnet ‚Äî neuronas que responden a conceptos como 'c√≥digo malicioso', 'razonamiento legal', 'tono emocional'. Google public√≥ Gemma Scope (open release) para investigaci√≥n externa. Para el CISO: si podemos identificar los circuitos que amplifican sesgos o que responden a prompts adversariales, podemos dise√±ar guardrails m√°s precisos ‚Äî conectando XAI con el red teaming de la Fase 13.

<br>

## 16.3 MODEL CARDS, SYSTEM CARDS Y AI FACTSHEETS: DOCUMENTACI√ìN DE ACCOUNTABILITY

<br>

Los Model Cards son documentos estructurados que describen la funci√≥n, capacidades, limitaciones y consideraciones √©ticas de un sistema de IA. Propuestos por Google en 2019, son el est√°ndar de facto de documentaci√≥n de accountability. El EU AI Act los hace obligatorios bajo Technical Documentation (Art. 11 y 17) para sistemas de alto riesgo. Para LLMs de uso general: forman parte de las obligaciones de transparencia GPAI bajo el EU AI Act.

<br>

| TIPO DE DOCUMENTO                                          | CONTENIDO Y PROP√ìSITO                                                                                                                                                                                                                                                                                                                                                                                                                                                            | OBLIGATORIEDAD Y ADOPCI√ìN 2025-2026                                                                                                                                                                                                                                                                            |
| ---------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Model Card (Google, 2019)                                  | Para modelos individuales de ML. Secciones est√°ndar: (1) Model Details (arquitectura, training data, par√°metros), (2) Intended Use (usos previstos y usos inapropiados expl√≠citamente documentados), (3) Factors (grupos demogr√°ficos, condiciones ambientales), (4) Metrics (m√©tricas y su justificaci√≥n), (5) Evaluation Data, (6) Training Data, (7) Quantitative Analyses (resultados desagregados por grupos), (8) Ethical Considerations, (9) Caveats and Recommendations. | EU AI Act Art. 11/17: Technical Documentation incluye todos los elementos del Model Card m√°s gesti√≥n de riesgos. GPAI Model Cards: OpenAI, Anthropic, Google, Microsoft los publican obligatoriamente. C√≥digo: pip install model-card-toolkit.                                                                 |
| System Card (OpenAI, 2023)                                 | Para sistemas completos, no solo el modelo base. Documenta el sistema tal como se deploya, incluyendo guardrails y filtros de seguridad a√±adidos. Secciones: Background, Deployment context, Safety measures (RLHF, Constitutional AI), Risk evaluations (red teaming results ‚Äî conecta con Fase 13), Limitations, Usage policies. OpenAI publica System Cards para GPT-4o, GPT-4 Vision, Sora.                                                                                  | No es requisito legal formal pero es expectativa del mercado enterprise. Para el CISO que eval√∫a proveedores: el System Card es el documento de accountability del proveedor. Los 16 proveedores del proyecto fueron evaluados en Fase 3 ‚Äî el System Card es evidencia directa de su postura de transparencia. |
| AI FactSheet (IBM, 2019)                                   | Alternativa de IBM al Model Card. Mayor √©nfasis en la supply chain del sistema de IA: qui√©n entren√≥ el modelo, con qu√© datos, en qu√© infraestructura, qu√© certificaciones tiene. Comparable al AIBOM (AI Bill of Materials) de la Fase 15. Incluye informaci√≥n sobre el proveedor m√°s all√° del modelo t√©cnico.                                                                                                                                                                   | IBM watsonx.governance: AI FactSheets integradas nativamente en el ciclo de vida de modelos. Para el CISO que eval√∫a sistemas de proveedores: el AI FactSheet es la herramienta de due diligence t√©cnica (complementa el an√°lisis contractual y √©tico de las Fases 5-6).                                       |
| Responsible AI Scorecard (Azure ML)                        | Herramienta integrada en Azure ML que genera autom√°ticamente un scorecard de evaluaci√≥n responsable. Incluye: Error Analysis (cohorts donde el modelo falla), Fairness Assessment (Fairlearn), Interpretability (InterpretML/SHAP), Counterfactual Analysis (DiCE), Causal Analysis (EconML). Exportable como PDF para stakeholders t√©cnicos y no t√©cnicos.                                                                                                                      | GA en Azure ML (2025). Genera autom√°ticamente la documentaci√≥n de accountability para modelos propios. El CISO con modelos en Azure AI Foundry puede usar el Scorecard como evidencia para la Technical Documentation del EU AI Act.                                                                           |
| Conformity Assessment Documentation (EU AI Act Art. 43-46) | Para sistemas de alto riesgo: documentaci√≥n formal que demuestra cumplimiento con el Reglamento. Incluye: descripci√≥n del sistema, sistema de gesti√≥n de riesgos (Art. 9), datos de entrenamiento (Art. 10), documentaci√≥n t√©cnica (Art. 11), logging (Art. 12), transparencia (Art. 13), supervisi√≥n humana (Art. 14), accuracy (Art. 15). Self-assessment para la mayor√≠a; third-party assessment obligatorio para los m√°s cr√≠ticos.                                           | Aplicable desde agosto 2026 para sistemas de alto riesgo ya desplegados. El CISO debe identificar cu√°les sistemas internos caen en categor√≠as de alto riesgo y preparar la documentaci√≥n. El Model Card y el Responsible AI Scorecard son punto de partida.                                                    |

<br>

## 16.4 FAIRNESS ALGOR√çTMICA: M√âTRICAS, TRADE-OFFS Y MITIGACI√ìN

<br>

Fairness algor√≠tmica es el complemento √©tico indispensable de la explainabilidad. Una explicaci√≥n puede ser perfectamente clara y al mismo tiempo revelar que el modelo discrimina sistem√°ticamente a un grupo protegido. El CISO debe entender tanto las m√©tricas como el trade-off fundamental: no existe una m√©trica de fairness √∫nica que sea satisfecha simult√°neamente cuando las tasas base entre grupos difieren (Teorema de Chouldechova/Kleinberg, 2017). Elegir qu√© definici√≥n aplicar es una decisi√≥n √©tica y legal, no t√©cnica.

<br>

| M√âTRICA                 | DEFINICI√ìN                                                                                                                                                                                          | CU√ÅNDO APLICAR ‚Äî IMPLICACI√ìN LEGAL                                                                                                                                                                                                                                                                                              |
| ----------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Demographic Parity      | La tasa de decisiones positivas es igual para todos los grupos: P(≈∂=1 \| Grupo A) = P(≈∂=1 \| Grupo B). El modelo aprueba el mismo porcentaje de solicitudes independientemente del grupo.           | Apropiada cuando el objetivo es corregir inequidades hist√≥ricas (affirmative action). Obligatoria en RRHH donde hay discriminaci√≥n sist√©mica documentada. Limitaci√≥n: puede aprobar solicitudes de menor calificaci√≥n de un grupo ‚Äî tensi√≥n con eficiencia. No compatible con Equal Opportunity cuando las tasas base difieren. |
| Equal Opportunity       | La tasa de verdaderos positivos es igual entre grupos: P(≈∂=1 \| Y=1, Grupo A) = P(≈∂=1 \| Y=1, Grupo B). El modelo no rechaza a los candidatos calificados de ning√∫n grupo con mayor frecuencia.     | Apropiada para allocation de beneficios: el modelo no debe rechazar a prestatarios solventes de un grupo en mayor proporci√≥n. Relevante para cr√©dito y contrataci√≥n. Fairlearn Exponentiated Gradient puede optimizar para esta m√©trica.                                                                                        |
| Equalized Odds          | Generalizaci√≥n: tanto TPR como FPR son iguales entre grupos. P(≈∂=1\|Y=1,G) igual para todos G, Y P(≈∂=1\|Y=0,G) igual para todos G. La m√©trica m√°s fuerte de fairness.                               | La m√©trica m√°s exigente: requiere que el modelo se comporte igualmente en casos positivos y negativos para todos los grupos. Imposible de satisfacer simult√°neamente con Demographic Parity cuando las tasas base difieren (Teorema de Chouldechova).                                                                           |
| Counterfactual Fairness | La decisi√≥n ser√≠a la misma en un contrafactual donde el individuo pertenece a un grupo diferente, manteniendo todo lo dem√°s constante. Implementado v√≠a DiCE.                                       | La m√©trica m√°s intuitiva para explicar al individuo: 'si usted fuera de otro g√©nero/etnia, con los mismos ingresos, la decisi√≥n habr√≠a sido la misma'. √ötil para cumplir con GDPR Art. 22 de forma que el individuo pueda entender y contestar.                                                                                 |
| Calibration             | El modelo est√° calibrado si la probabilidad predicha refleja la frecuencia real: P(Y=1 \| ≈∂=p, Grupo A) = P(Y=1 \| ≈∂=p, Grupo B) = p. Los scores tienen el mismo significado para todos los grupos. | Esencial en sistemas donde el n√∫mero predicho se usa directamente para decisiones cuantitativas: scoring de riesgo, scoring de salud, scoring de cr√©dito. Sin calibraci√≥n por grupos, un score de 0.7 significa cosas distintas para distintos grupos.                                                                          |

<br>

‚ö†Ô∏è Teorema de Imposibilidad de Fairness: Chouldechova (2017) y Kleinberg et al. (2017): cuando las tasas base de la variable de outcome difieren entre grupos ‚Äîcaso casi universal en aplicaciones reales‚Äî es matem√°ticamente imposible satisfacer simult√°neamente Calibration, Demographic Parity y Equal Opportunity. El CISO y el equipo legal deben elegir expl√≠citamente qu√© m√©trica de fairness aplicar y documentar la justificaci√≥n. Elegir impl√≠citamente (sin documentar) tiene riesgo regulatorio. Microsoft Responsible AI Standard v2 requiere documentar esta elecci√≥n.

<br>

### 16.4.2 Fairlearn ‚Äî La Herramienta de Mitigaci√≥n de Sesgos de Microsoft

| COMPONENTE FAIRLEARN   | DESCRIPCI√ìN T√âCNICA                                                                                                                                                                                                                                                                     | CASO DE USO EN AZURE ML                                                                                                                                                                                                                      |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Metrics Dashboard      | Visualizaci√≥n interactiva: selection rates por grupo, TPR, FPR, AUC desagregado. Selecci√≥n de sensitive feature (g√©nero, etnia, edad) y m√©trica de fairness. Comparaci√≥n de m√∫ltiples modelos en scatter plot performance vs. fairness.                                                 | Para modelos propios en Azure ML: ejecutar post-training para verificar ausencia de disparidades antes del deployment. El output puede incluirse en el Responsible AI Scorecard como evidencia para el EU AI Act.                            |
| Exponentiated Gradient | Algoritmo de reducci√≥n: reformula el problema de fairness como serie de clasificaciones ponderadas. En cada iteraci√≥n, aumenta el peso de los ejemplos de grupos tratados injustamente. Resultado: clasificador que minimiza el error total sujeto a la constraint de fairness elegida. | Para modelos que muestran disparidad significativa: re-entrena incorporando la constraint de fairness. Compatible con scikit-learn, LightGBM, PyTorch. El CISO audita el trade-off: cu√°nta accuracy se sacrifica para qu√© nivel de fairness. |
| Threshold Optimizer    | Enfoque post-processing: modifica el threshold de decisi√≥n de forma diferenciada por grupo sin re-entrenar el modelo. M√°s r√°pido pero menos preciso que Exponentiated Gradient.                                                                                                         | Para modelos legacy en producci√≥n que no pueden re-entrenarse: permite a√±adir fairness sin cambiar el modelo subyacente. √ötil para sistemas de scoring de clientes ya desplegados que muestran disparidad en auditor√≠a.                      |
| GridSearch             | Entrena una familia de modelos con diferentes niveles de penalizaci√≥n de fairness. Permite comparar el Pareto frontier de performance vs. fairness. El equipo elige expl√≠citamente qu√© punto del trade-off seleccionar y documenta la justificaci√≥n.                                    | El scatter plot de GridSearch (performance vs. fairness disparity) es una herramienta visual poderosa para explicar el trade-off al AI Governance Committee y documentar la decisi√≥n de selecci√≥n de modelo.                                 |

<br>

## 16.5 MICROSOFT RESPONSIBLE AI DASHBOARD: EL STACK DE XAI PARA AZURE ML

<br>

El Microsoft Responsible AI Dashboard es el 'single pane of glass' que consolida XAI, fairness, an√°lisis de errores y an√°lisis causal en una interfaz integrada. Disponible en Azure ML Studio, es el resultado de integrar cuatro toolkits open-source: Fairlearn, InterpretML, DiCE, y EconML.

<br>

| COMPONENTE              | LIBRER√çA SUBYACENTE                                                                                                                          | PREGUNTA QUE RESPONDE                                                                                       | OUTPUT                                                                                                                                 |
| ----------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| Error Analysis          | microsoft/responsible-ai-toolbox. √Årbol de decisi√≥n que identifica los cohorts del dataset donde el modelo tiene mayor tasa de error.        | ¬øD√≥nde falla el modelo? ¬øHay grupos espec√≠ficos para los que el modelo es significativamente menos preciso? | Heatmap de errores por cohort. √Årbol de decisi√≥n de fallos. Input para el Model Card y para el fairness assessment.                    |
| Interpretability        | microsoft/interpret ‚Üí InterpretML. Integra SHAP, LIME, EBM. Soporta local (por predicci√≥n) y global (por modelo).                            | ¬øPor qu√© el modelo toma sus decisiones? ¬øPor qu√© este cliente espec√≠fico recibi√≥ esta predicci√≥n?           | Feature importance plot (global). SHAP waterfall plot (local). EBM shape functions. Exportable al Responsible AI Scorecard.            |
| Fairness Assessment     | microsoft/fairlearn. M√©tricas: selection rate, TPR, FPR, AUC por grupo. Mitigaci√≥n: Exponentiated Gradient, Threshold Optimizer, GridSearch. | ¬øEs el modelo justo con todos los grupos sensibles? ¬øCu√°l es el trade-off entre fairness y performance?     | Dashboard interactivo de m√©tricas desagregadas. Scatter plot performance vs. fairness. Evidencia para conformity assessment EU AI Act. |
| Counterfactual Analysis | microsoft/DiCE. Genera los m√≠nimos cambios en features no protegidas que cambiar√≠an la decisi√≥n del modelo.                                  | ¬øQu√© deber√≠a cambiar para obtener un resultado diferente? ¬øPuede el usuario tomar acciones concretas?       | 'Si su ingreso fuera $X m√°s, la solicitud ser√≠a aprobada.' Implementa GDPR Art. 22. Esencial para sistemas de cr√©dito y RRHH.          |
| Causal Analysis         | microsoft/EconML. Estima efectos causales ‚Äîno solo correlaciones‚Äî de features sobre outcomes.                                                | ¬øCu√°l es el impacto causal real de una intervenci√≥n? ¬øQu√© factores tienen causalidad vs. solo correlaci√≥n?  | 'Reducir la tasa causalmente aumenta la tasa de repago en X%.' Evita decisiones basadas en correlaciones espurias.                     |

<br>

üîß C√≥mo activar el Responsible AI Dashboard en Azure ML: 1. Registrar el modelo en Azure ML Registry. 2. Azure ML Studio ‚Üí Seleccionar modelo ‚Üí 'Create Responsible AI Dashboard'. 3. Configurar dataset de evaluaci√≥n con sensitive features (g√©nero, etnia, edad). 4. Seleccionar componentes (Error Analysis, InterpretML, Fairlearn, DiCE, EconML). 5. El dashboard se genera como asset adjunto al modelo. 6. Exportar como 'Responsible AI Scorecard' (PDF): m√©tricas clave en formato compartible con stakeholders no t√©cnicos. Disponibilidad: GA en Azure ML. Requiere: Azure ML workspace, modelo registrado, dataset de evaluaci√≥n etiquetado.

<br>

## 16.6 EU AI ACT Y EXPLAINABILIDAD: OBLIGACIONES, PLAZOS Y PENALIDADES

<br>

El EU AI Act no usa expl√≠citamente el t√©rmino 'Explainable AI' ‚Äî pero sus requisitos de transparencia, documentaci√≥n t√©cnica, supervisi√≥n humana y logging hacen que la explainabilidad sea una necesidad pr√°ctica para cualquier sistema de alto riesgo. La penalidad m√°xima ‚Äî 6% del revenue global ‚Äî supera al GDPR.

<br>

| Art. | T√çTULO                          | IMPLICACI√ìN XAI                                                                                                                                                                                                                               | HERRAMIENTA T√âCNICA                                                                                                                                                                     |
| ---- | ------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 9    | Sistema de Gesti√≥n de Riesgos   | Proceso continuo de identificaci√≥n, an√°lisis y mitigaci√≥n de riesgos a lo largo de todo el ciclo de vida. Debe documentar c√≥mo se identificaron y mitigaron los riesgos ‚Äî incluyendo los derivados de sesgos y comportamientos no explicados. | DPIA de IA. Fairlearn bias assessment. Red Teaming (Fase 13). Registro de riesgos del sistema.                                                                                          |
| 10   | Datos de Entrenamiento          | Los sistemas de alto riesgo deben usar datos relevantes, representativos, libres de errores. Los conjuntos de datos deben examinarse para detectar posibles sesgos. La detecci√≥n de sesgos implica herramientas de fairness assessment.       | Fairlearn metrics sobre datos de training. Error Analysis para detectar cohorts subrepresentados. AIBOM.                                                                                |
| 11   | Documentaci√≥n T√©cnica           | Antes del deployment: documentaci√≥n t√©cnica completa incluyendo descripci√≥n del sistema, capacidades, limitaciones, proceso de validaci√≥n y testing.                                                                                          | Model Card completo. Responsible AI Scorecard de Azure ML. AIBOM para modelos fine-tuned.                                                                                               |
| 12   | Registro (Logging)              | Los sistemas de alto riesgo deben tener capacidades de logging para reconstruir las circunstancias que llevaron a resultados identificados como riesgosos.                                                                                    | Azure ML Model Monitoring. Purview Audit Log para sistemas M365. Custom logging en Azure AI Foundry.                                                                                    |
| 13   | Transparencia al Usuario        | Los usuarios deben poder entender las capacidades y limitaciones del sistema para usarlo apropiadamente. Informaci√≥n clara sobre prop√≥sito, accuracy, limitaciones, errores posibles.                                                         | Componente de Transparencia del Model Card. Documentation visible en la interfaz de usuario. Formaci√≥n de usuarios documentada.                                                         |
| 14   | Supervisi√≥n Humana              | Los sistemas de alto riesgo deben dise√±arse para que humanos puedan supervisar efectivamente: parar el sistema, invalidar decisiones, monitorear comportamiento. Los operadores deben entender suficientemente el sistema.                    | XAI como prerequisito de supervisi√≥n humana efectiva: un operador no puede supervisar lo que no puede entender. Human-in-the-loop para decisiones de alto impacto. Kill switch probado. |
| 17   | Documentaci√≥n T√©cnica Detallada | Para cada versi√≥n del sistema: descripci√≥n del modelo, datos de training, validaci√≥n y testing, resultados de evaluaci√≥n de performance desagregados por grupos relevantes.                                                                   | Model Card con Quantitative Analyses desagregadas. Responsible AI Scorecard. Versionado en Azure ML Registry.                                                                           |
| 65   | Penalidades                     | No compliance con requisitos para sistemas de alto riesgo: hasta 3% revenue global. Uso de sistemas prohibidos (Art. 5): hasta 6% revenue global. Informaci√≥n incorrecta a organismos notificados: hasta 1%.                                  | El compliance con Arts. 9-17 (XAI + fairness + documentaci√≥n) es la defensa. El CISO debe demostrar que los controles fueron implementados, evaluados y documentados.                   |

<br>

### 16.6.1 Timeline EU AI Act ‚Äî Ventana de Compliance para el CISO

| FECHA          | MILESTONE                                                                                           | ACCI√ìN CISO                                                                                                                                               |
| -------------- | --------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1 agosto 2024  | Entrada en vigor del EU AI Act.                                                                     | -                                                                                                                                                         |
| 2 febrero 2025 | Prohibici√≥n de sistemas de IA de riesgo inaceptable (Art. 5).                                       | Verificar que ning√∫n sistema interno cae en categor√≠a prohibida.                                                                                          |
| 2 agosto 2025  | Reglas GPAI y obligaciones de autoridades nacionales. Proveedores GPAI deben publicar Model Cards.  | Verificar que los 16 proveedores del proyecto han publicado su documentaci√≥n GPAI. Actualizar due diligence (Fases 5-6).                                  |
| 2 agosto 2026  | Requisitos para sistemas de alto riesgo (Arts. 9-17) aplicables. Conformity assessment obligatorio. | FECHA CR√çTICA: documentaci√≥n t√©cnica, gesti√≥n de riesgos, logging, XAI y fairness assessment completados para todos los sistemas de alto riesgo internos. |
| 2027           | Sistemas de IA en productos regulados existentes: transici√≥n completa.                              | Compliance total para sistemas en productos m√©dicos, seguridad, etc.                                                                                      |

<br>

## 16.7 PLAN DE XAI PARA EL CISO CON M365/COPILOT/AZURE AI FOUNDRY

<br>

El plan distingue dos contextos: (A) el CISO como usuario de IA de terceros (M365 Copilot, modelos de proveedores) ‚Äî donde el trabajo es evaluar y documentar la explainabilidad que los proveedores ofrecen ‚Äî y (B) el CISO como deployer de modelos propios (Azure ML, Copilot Studio) ‚Äî donde el trabajo es implementar XAI y fairness sobre modelos propios. Las obligaciones regulatorias m√°s intensas aplican al contexto B.

<br>

### 16.7.1 Contexto A ‚Äî M365 Copilot y Proveedores: Evaluar XAI del Proveedor

| PREGUNTA                                                                            | FUENTE DE EVIDENCIA                                                                                                                                                                                                                                                                          | ACCI√ìN DEL CISO                                                                                                                                                                                                                                                                                |
| ----------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ¬øEl proveedor publica Model Card / System Card para los modelos de Copilot?         | Microsoft: Responsible AI Report anual (2025), Model Cards para GPT-4o (Azure OpenAI), familia Phi, Copilot System Card. Disponibles en microsoft.com/ai y Azure AI Foundry documentation.                                                                                                   | Descargar y revisar. Verificar que cubren: intended use, limitations, evaluation results desagregados. Almacenar como evidencia para el AI Governance Committee y auditor√≠a EU AI Act.                                                                                                         |
| ¬øQu√© explainabilidad ofrece Copilot al usuario final?                               | M365 Copilot cita las fuentes de los documentos que usa para generar respuestas ‚Äî indica qu√© archivos de SharePoint / emails de Exchange fueron usados como contexto. No provee SHAP values ni feature importance: la explicaci√≥n es a nivel de fuentes, no de razonamiento interno del LLM. | Comunicar a usuarios el nivel real de explainabilidad disponible. Para casos donde la explicabilidad del reasoning es cr√≠tica (no solo las fuentes): evaluar si Copilot es la herramienta apropiada o si se necesita un sistema con mayor explainabilidad.                                     |
| ¬øLos agentes de Copilot Studio permiten auditar su razonamiento?                    | Copilot Studio registra los pasos de razonamiento y las herramientas llamadas en Azure Monitor / Application Insights. Es posible auditar qu√© herramienta fue llamada con qu√© argumento. El razonamiento interno del LLM sigue siendo opaco.                                                 | Implementar logging de herramientas en todos los agentes (Azure Monitor). Configurar alertas para herramientas llamadas de forma an√≥mala. El trace de herramientas es el 'audit trail' de XAI disponible para agentes.                                                                         |
| ¬øC√≥mo evaluar si Copilot tiene sesgos en el contexto espec√≠fico de la organizaci√≥n? | No hay herramienta de fairness espec√≠fica para M365 Copilot. El assessment debe ser emp√≠rico: dise√±ar escenarios de prueba con inputs equivalentes para distintos grupos y evaluar si los outputs son consistentes.                                                                          | Dise√±ar un 'Copilot Fairness Evaluation' ad hoc: 20-30 escenarios donde el input solo difiere en atributos protegidos (nombre, g√©nero aparente, origen). Evaluar si los outputs difieren sistem√°ticamente. Documentar el resultado. Repetir anualmente o tras actualizaciones del modelo base. |

<br>

### 16.7.2 Contexto B ‚Äî Modelos Propios en Azure ML / AI Foundry: Implementar XAI

| <p><br></p> | ACCI√ìN                                                                   | DETALLE DE EJECUCI√ìN                                                                                                                                                                                                                                                                                                                                                                                                | CRITERIO DE √âXITO                                                                                                                                                                                   |
| ----------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1           | Inventario de modelos propios con clasificaci√≥n EU AI Act                | Levantar el inventario completo de modelos ML propios o fine-tuned. Para cada modelo: clasificar riesgo bajo EU AI Act (Alto Riesgo ‚Üí conformity assessment; Limitado ‚Üí transparencia; Minimal ‚Üí sin obligaciones). Sistemas de alto riesgo relevantes para empresa argentina: RRHH (scoring de empleados, selecci√≥n), scoring de clientes, sistemas de seguridad con decisiones autom√°ticas.                       | Inventario con clasificaci√≥n EU AI Act para cada modelo. Lista de modelos de alto riesgo con target de compliance (agosto 2026).                                                                    |
| 2           | Implementar RAI Dashboard en Azure ML para modelos de alto riesgo        | Para cada modelo de alto riesgo en Azure ML: configurar el Responsible AI Dashboard con los 5 componentes. Definir qu√© grupos sensibles evaluar. Ejecutar el an√°lisis completo. Revisar con el AI Governance Committee. Documentar hallazgos y plan de mitigaci√≥n si hay disparidades.                                                                                                                              | RAI Dashboard ejecutado para 100% de modelos de alto riesgo. Reporte de fairness assessment por grupo sensible. Si hay disparidades: plan de mitigaci√≥n con Fairlearn documentado.                  |
| 3           | Generar Model Card para cada modelo de alto riesgo                       | Usar template de Model Card (model-card-toolkit) adaptado a requisitos de Technical Documentation EU AI Act (Art. 11). Completar todas las secciones. Incluir output del RAI Dashboard como Quantitative Analyses. Review y firma del DPO y AI Governance Committee antes del deployment.                                                                                                                           | Model Card completo y firmado para cada modelo de alto riesgo. Almacenado en Azure ML Registry adjunto al modelo versionado. Actualizado en cada re-entrenamiento significativo.                    |
| 4           | Implementar SHAP en el pipeline de inferencia para decisiones auditables | Para modelos que apoyan decisiones con impacto en personas (RRHH, scoring, selecci√≥n): implementar generaci√≥n de explicaciones SHAP locales en el pipeline de inferencia. Cada decisi√≥n puede acompa√±arse de una explicaci√≥n on-demand. Almacenar top-3 features m√°s importantes por decisi√≥n en el log. Para sistemas customer-facing: implementar DiCE counterfactuals si el usuario pregunta 'qu√© debo cambiar'. | Pipeline de inferencia con explicaciones SHAP locales on-demand. Log de decisiones con features m√°s importantes. Interface de explicaci√≥n para usuario final cuando aplica.                         |
| 5           | Proceso de revisi√≥n anual de fairness y XAI                              | Ciclo anual: (1) Re-ejecutar RAI Dashboard con datos del a√±o en producci√≥n (data drift puede haber cambiado el fairness). (2) Revisar si hay grupos nuevos a evaluar. (3) Actualizar Model Card. (4) Si el modelo fue actualizado: re-ejecutar desde el inicio. (5) Presentar Responsible AI Scorecard al AI Governance Committee como parte del informe anual de postura de IA.                                    | Ciclo anual documentado: RAI Dashboard actualizado, Model Card actualizado, Responsible AI Scorecard presentado al AI Governance Committee. Evidencia de proceso continuo para auditor√≠a EU AI Act. |
| 6           | Capacitaci√≥n del equipo en XAI y fairness enterprise                     | Capacitaci√≥n obligatoria para data scientists: RAI Dashboard de Azure ML, interpretaci√≥n de SHAP values, m√©tricas de fairness y su significado legal, c√≥mo documentar un Model Card. Capacitaci√≥n para CISO y DPO: qu√© significa cada m√©trica del RAI Dashboard, c√≥mo comunicar resultados al Board, cu√°ndo iniciar un proceso de mitigaci√≥n.                                                                       | 100% del equipo de data science con capacitaci√≥n en RAI Dashboard. CISO y DPO con capacitaci√≥n en m√©tricas de fairness. Microsoft Learn provee el learning path de Responsible AI.                  |

<br>

## 16.8 REFERENCIAS (R421‚ÄìR450)

<br>

XAI ‚Äî Fundamentos y Estado del Arte 2025-2026:

R421. Financial Content Markets, 'The End of the Black Box: How Explainable AI is Transforming High-Stakes Decision Making in 2026', enero 2026. XAI como requisito legal y √©tico. Mechanistic Interpretability 2025: SAEs mapean features conceptuales en LLMs. IBM watsonx.governance 'agentic explainability'. Palantir AIP Control Tower. Posible retraso EU Digital Omnibus hasta 2028 para reglas m√°s estrictas en empresas peque√±as.

R422. EthicalXAI Platform, 'SHAP vs LIME: Choosing the Best XAI Method for Your Enterprise Models in 2025', julio 2025. EU AI Act: penalidades hasta 6% revenue global. Organizaciones con XAI comprensivo: 31% m√°s r√°pido en debugging, 24% reducci√≥n incidentes de sesgo, 18% mejora trust metrics. California SB-1001 para algorithmic accountability. Brazil LGPD con provisiones de IA.

R423. Bismart Blog, 'Explainable AI (XAI) in 2025: How to Trust AI', agosto 2025. BBVA Mercury library para explainabilidad de modelos financieros. Banco espa√±ol SHAP: 30% reducci√≥n en disputas de decisiones crediticias. BBVA y Telef√≥nica: auditor√≠as de algoritmos anticipando EU AI Act. XAI como herramienta de trust y adoption.

R424. AI Ireland, 'Explainable AI and the EU AI Act: Unlocking Trust and Compliance Before It's Too Late', abril 2025. EU AI Act: justificaci√≥n en conformity assessments, audit trails obligatorios. Timeline oficial EU AI Act. Post-hoc XAI: SHAP, LIME, Captum. Integrar explainabilidad desde el inicio del modelo lifecycle.

R425. GJETA, 'The Rise of Explainable AI: Enhancing Transparency and Accountability', 2025. Comparativa SHAP vs LIME vs perturbation-based vs self-explainable. Evaluaci√≥n XAI: functionally-grounded, application-grounded, human-grounded. Regulatorio: EU AI Act, GDPR, FDA AI guidance m√©dica.

R426. SwiftTask, 'Explainable AI (XAI) 2024: Algorithmic Transparency and the EU AI Act', 2025. Balance performance-transparencia: modelos simples pierden 8-12% accuracy. Hybrid approaches como compromiso. 'Explainability by design' como nuevo est√°ndar.

<br>

SHAP, LIME, T√©cnicas de Explicaci√≥n:

R427. Lundberg, Scott y Lee, Su-In, 'A Unified Approach to Interpreting Model Predictions' (SHAP), NeurIPS 2017. Shapley values de teor√≠a de juegos cooperativos. Cuatro propiedades: eficiencia, simetr√≠a, dummy, aditividad. pip install shap.

R428. Ribeiro, Marco et al., 'Why Should I Trust You? Explaining the Predictions of Any Classifier' (LIME), KDD 2016. Perturbaci√≥n local + modelo lineal ponderado. Model-agnostic.

R429. Articsledge, 'What is Explainable AI (XAI)? Complete Guide to XAI in 2025', noviembre 2025. Goldman Sachs SHAP + GDPR Art. 22 + ECOA compliance. JPMorgan Chase SHAP + LIME. PayPal fraud detection. Bridgewater: XAI para trading transparency.

R430. CFA Institute, 'Explainable AI in Finance: Addressing the Needs of Diverse Stakeholders', agosto 2025. 6 grupos de stakeholders con necesidades XAI distintas. Overreliance risk: automation bias. Evaluative AI y Neurosymbolic AI como alternativas. No hay benchmark universal de calidad de explicaciones.

R431. EDPS, 'TechDispatch on Explainable Artificial Intelligence', noviembre 2023. Automation bias: las explicaciones pueden aumentar la probabilidad de aceptaci√≥n ciega. GDPR y transparencia del tratamiento. Proxy attributes: XAI puede revelar que el modelo usa proxies de atributos protegidos.

<br>

Model Cards, Fairness y Microsoft Responsible AI:

R432. Mitchell, Margaret et al. (Google), 'Model Cards for Model Reporting', FAccT 2019. Art√≠culo fundacional. Secciones: Model Details, Intended Use, Factors, Metrics, Evaluation Data, Training Data, Quantitative Analyses, Ethical Considerations, Caveats. pip install model-card-toolkit.

R433. Gebru, Timnit et al. (Google), 'Datasheets for Datasets', Communications of the ACM, 2021. Documentaci√≥n de motivaci√≥n, composici√≥n, proceso de recolecci√≥n, preprocesamiento, usos recomendados. Complementario al AIBOM (Fase 15).

R434. Microsoft GitHub, 'Responsible AI Toolbox', github.com/microsoft/responsible-ai-toolbox. Error Analysis, InterpretML (SHAP + LIME + EBM), Fairlearn, DiCE, EconML. Responsible AI Dashboard y Scorecard (PDF). Compatible: scikit-learn, PyTorch, LightGBM.

R435. Medium (Bhavya), 'The Ultimate Guide to Microsoft Responsible AI Toolbox', mayo 2025. Fairlearn para detecci√≥n de sesgos. EBM: glass-box comparable a gradient boosting. Responsible AI Dashboard ideal para executive reviews. DiCE: counterfactuals accionables.

R436. Microsoft Learn, 'What is Responsible AI ‚Äî Azure Machine Learning', 2025. 6 principios: Fairness, Reliability, Privacy, Inclusiveness, Transparency, Accountability. SmartNoise: DP (co-desarrollado con MSR). Counterfit: cyberattack simulation para AI. Responsible AI Scorecard para comunicaci√≥n cross-stakeholder.

R437. Microsoft AI, 'Responsible AI Principles and Approach', 2025. Responsible AI Annual Report 2025. Aether committee. Responsible AI Standard v2. Sensitive use case reviews.

R438. Microsoft Responsible AI Standard v2, 2022-2025. Requisitos F1-F2 para Fairness: Fairlearn, Error Analysis, InterpretML. Documentaci√≥n de trade-offs. Consulta legal para disparidades residuales.

R439. Fairlearn.org Documentation, 2025. M√©tricas: demographic parity, equalized odds, equal opportunity, calibration. Algoritmos: Exponentiated Gradient, Threshold Optimizer, GridSearch. Scatter plot performance vs. fairness.

R440. Medium (Navita Singh), 'Responsible AI in Azure (Part 1)', octubre 2025. Fairlearn Exponentiated Gradient en Azure. Demographic parity vs. equalized odds. Responsible AI Dashboard en Azure ML Studio.

<br>

EU AI Act, Fairness Theory y Accountability Algor√≠tmica:

R441. EU AI Act, Regulation (EU) 2024/1689. Art. 9 (Risk Management), Art. 10 (Data Governance), Art. 11 (Technical Documentation), Art. 12 (Logging), Art. 13 (Transparency), Art. 14 (Human Oversight), Art. 17 (Detailed Technical Documentation), Art. 43-46 (Conformity Assessment), Art. 65 (Penalties).

R442. EU AI Act Timeline oficial. 1 agosto 2024 entrada en vigor. 2 febrero 2025: prohibici√≥n sistemas inaceptables. 2 agosto 2025: GPAI y autoridades nacionales. 2 agosto 2026: sistemas de alto riesgo. 2027: productos regulados legacy.

R443. Medium (Tahir), 'How to Build an Enterprise AI Compliance Program', agosto 2025. Enterprise AI compliance: Governance, Risk Assessment (DPIAs + AIAs), Data Governance, XAI (Model Cards + SHAP/LIME), Human Oversight. Fases de implementaci√≥n.

R444. Chouldechova, Alexandra, 'Fair Prediction with Disparate Impact', Big Data, 2017. Teorema de imposibilidad: cuando las tasas base difieren entre grupos, es matem√°ticamente imposible satisfacer simult√°neamente Calibration, Demographic Parity y Equal Opportunity.

R445. Kleinberg, Jon et al., 'Human Decisions and Machine Predictions', NBER, 2018. Las inconsistencias de fairness en modelos ML reflejan inconsistencias en decisiones humanas. La explainabilidad puede revelar sesgos hist√≥ricos en datos de training.

R446. GoCodeo, 'Top 5 Responsible AI Frameworks in 2025', 2025. Microsoft Azure RAI: 6 pilares, Fairlearn + InterpretML + SHAP + RAI Dashboard. LinkedIn FAIR framework. Credo AI: governance enterprise con policy-as-code.

R447. IBM watsonx.governance Documentation, 2025. 'Agentic explainability'. AI FactSheets integradas. Healthcare suite: XAI step-by-step para recomendaciones cl√≠nicas. 'White box' como selling point enterprise ante liability de decisiones automatizadas.

R448. Anthropic, 'Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet', 2024. SAEs: miles de features conceptuales mapeadas en Claude 3 Sonnet. Implicaciones de seguridad: circuitos que responden a prompts adversariales. Base del programa de Interpretability Safety Research de Anthropic.

R449. Google DeepMind, 'Gemma Scope: Open Sparse Autoencoders Everywhere All At Once', 2025. Open release de SAEs para Gemma 2. Permite que investigadores externos estudien los circuitos internos de un LLM de alta calidad.

R450. NIST AI Risk Management Framework (AI RMF) 1.0, enero 2023. Map, Measure, Manage, Govern. Transparencia y Explicabilidad como funci√≥n core del Measure. Mapping NIST AI RMF ‚Üî EU AI Act para organizaciones con exposici√≥n a ambos marcos.

<br>



